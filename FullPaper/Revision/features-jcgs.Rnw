\documentclass[12pt]{article}
% \usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
% \geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\usepackage{graphicx}
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
% \usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{hyperref}
\graphicspath{{fig/}}
% \usepackage{endfloat} % Figures to the end of the document

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage[colorinlistoftodos]{todonotes}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\done}[2][inline]{\todo[color=SpringGreen, #1]{#2}}  % for todos that have been seen and dealt with
\newcommand{\meh}[2][inline]{\todo[color=White, #1]{#2}}   % for todos that may no longer be relevant 
\newcommand{\comment}[2][inline]{\todo[color=SkyBlue, #1]{#2}} % for comments that may not be "to-do"s
%\newcommand{\mcomment}[1]{\todo[color=SkyBlue]{#1}} % for margin comments
\newcommand{\newtext}[1]{\todo[inline, color=White]{ \color{OliveGreen}{#1}}} % new text - not necessarily something to be done
\newcommand{\newdo}[1]{\todo[inline, color=Lime]{#1}} % new to do item
%
%---------------------------------------------------
%                 Placing Figures

\renewcommand{\topfraction}{0.99}	% max fraction of floats at top
\renewcommand{\bottomfraction}{0.99}	% max fraction of floats at bottom

\renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
\renewcommand{\floatpagefraction}{0.98}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\renewcommand{\dblfloatpagefraction}{0.98}	% require fuller float pages


%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Clusters beat Trend!? \\Testing feature hierarchy in statistical graphics}
  \author{Susan VanderPlas\thanks{
    The authors gratefully acknowledge funding from the National Science Foundation Grant \# DMS 1007697. All data collection has been conducted with approval from the Institutional Review Board IRB 10-347}\hspace{.2cm}\\
    Department of Statistics and Statistical Laboratory, Iowa State University\\
    and \\
    Heike Hofmann\\
    Department of Statistics and Statistical Laboratory, Iowa State University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Clusters beat Trend!? \\Testing feature hierarchy in statistical graphics}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Graphics are very effective for communicating numerical information quickly and efficiently, but many of the design choices we make are based on subjective measures, such as personal taste or conventions of the discipline rather than objective criteria. We briefly introduce perceptual principles such as preattentive features and gestalt heuristics, and then discuss the design and results of a factorial experiment examining the effect of plot aesthetics such as color and trend lines on participants' assessment of ambiguous data displays. The quantitative and qualitative experimental results strongly suggest that plot aesthetics have a significant impact on the perception of important features in data displays. 
\end{abstract}

\noindent%
{\it Keywords:}  Visual inference, Lineup protocol, Preattentive Features, Saliency of Plot Aesthetics, User Study.
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!
<<setup, fig.keep='all', cache=FALSE, echo=FALSE, eval=TRUE, include=FALSE>>=
rm(list=ls())
options(replace.assign=TRUE,width=70)
require(knitr)
opts_chunk$set(fig.path='fig/', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, message=F, warning=F)

library(stringr)
library(lubridate)

library(reshape2)
#library(plyr) ## clashes with dplyr
library(dplyr)
library(magrittr)

library(ggplot2)
library(grid)
suppressMessages(library(gridExtra))
library(RColorBrewer)

library(nullabor)
library(digest)
library(Cairo)

library(lme4)

library(xtable)

source("../../Code/MixtureLineups.R")
source("../../Code/theme_lineup.R")
@
%\tableofcontents
\newpage
\section{Introduction and Background}
Limits on attention span, short term memory, and information storage mechanisms within the human brain make it difficult for us to process numerical information  in raw form effectively. 
(Well-designed) data displays are much better suited for this kind of communication, as  they serve as a form of external cognition \citep{zhang1997nature,scaife1996external}, ordering and visually summarizing data
and thereby invoking our higher-bandwidth visual system. 

One fantastic example of this phenomenon is the Hertzsprung-Russell (HR) diagram, which was described as ``one of the greatest observational syntheses in astronomy and astrophysics" because it allowed astronomers to clearly relate the absolute magnitude of a star to its' spectral classification; facilitating greater understanding of stellar evolution \citep{spence1993remarkable}. 
The data it displayed was previously available in several different tables; but when plotted within the same chart, information that was invisible in a tabular representation became immediately clear \citep{lewandowsky1989perception}. 
Graphical displays more efficiently utilize cognitive resources by reducing the burden of storing, ordering, and summarizing raw data; this frees bandwidth for higher levels of information synthesis, allowing observers to note outliers, understand relationships between variables, and form new hypotheses.

Statistical graphics are powerful because they efficiently and effectively convey numerical information, but there exists  relatively sparse empirical information about how the human perceptual system processes these displays. Our understanding of the perception of statistical graphics is informed by general psychological and psychophysics research as well as more specific research into the perception of data displays \citep{cleveland:1984}. 

One relevant focus of psychological research is preattentive perception, that is, perception which occurs automatically in the first 200 ms of exposure to a visual stimulus \citep{treisman1985preattentive}. 

Research into {\bf preattentive perception} provides us with some information about the temporal hierarchy of graphical feature processing. Color, line orientation, and shape are processed preattentively; that is, within 200 ms, it is possible to identify a single target in a field of distractors, if the target differs with respect to color or shape \citep{goldstein2009encyclopedia}. 
Research by \citet{healey1999large} extends this work, demonstrating that certain features of three-dimensional data displays are also processed preattentively. However, neither target identification nor three-dimensional data processing always translate into faster or more accurate inference about the data displayed, particularly when participants have to integrate several preattentive features to understand the data. 

{\bf Feature detection} at the attentive stage of perception has also been examined in the context of statistical graphics; researchers have evaluated the perceptual implications of utilizing color, fill, shapes, and letters to denote categorical or stratified data in scatterplots. \citet{cleveland:1984} ranked the optimality of these plot aesthetics based on response accuracy, preferring colors, amount of fill, shapes, and finally letters to indicate category membership. \citet{lewandowsky1989discriminating} examined both accuracy and response time, finding that color is faster and more accurately perceived (except by individuals with color deficiency). Shape, fill, and discriminable letters (letters which do not share visual features, such as HQX) were identified as less accurate than color, while confusable letters (such as HEF) result in significantly decreased accuracy. 

{\bf Gestalt psychology} is another area of psychological research, that examines perception as a holistic experience, establishing and evaluating mental heuristics used to transform visual stimuli into useful, coherent information. 
Gestalt rules of perception can be easily applied to statistical graphics, as they describe the way we organize visual input, focusing on the holistic experience rather than the individual perceptual features. 

For example, rather than perceiving four legs, a tail, two eyes, two ears, and a nose, we perceive a dog. This is due to certain perceptual heuristics, which provide a ``top-down" method of understanding visual stimuli by taking into account past experience. 

The rules of perceptual organization relevant to graphical perception in this experiment are:
\begin{itemize}
\item \textbf{Proximity}: two elements which are close together are more likely to belong to a single unit.
\item \textbf{Similarity}: the more of the same or similar aesthetics two elements share two elements are, the more likely they belong to a single unit. 
\item \textbf{Good continuation}: two elements which blend together smoothly likely belong to one unit.
\item \textbf{Common region}: elements contained within a common region likely belong together. 
\end{itemize}
A complete list of the rules of perceptual grouping can be found in \citet{goldstein2009encyclopedia}.

\begin{figure}\centering
<<gestalt1, echo =FALSE, out.width='0.32\\textwidth', fig.width=3, fig.height=3, fig.show='hold'>>=
x1 <- rnorm(25, mean=4, sd=0.5)
x2 <- rnorm(25, mean=0, sd=0.5)
y1 <- rnorm(25, mean=1, sd=0.5)
y2 <- rnorm(25, mean=2, sd=0.5)
qplot(c(x1,x2), c(y1,y2)) + theme_bw() + xlab("x") + ylab("y")

x <- rnorm(60)
y <- rnorm(60)
group <- rep(1:4, length=60)
qplot(x,y, color=factor(group%/%2), shape=factor(group%/%2)) + theme_bw() + xlab("x") + ylab("y") + scale_color_brewer(palette="Set1") + theme(legend.position="none")

x1 <- runif(50,-.75,1.25)
y1 <- x1^2 - x1 + rnorm(50, sd=0.1)
x2 <- runif(25,-.75,1.25)
y2 <- x2 + rnorm(25, sd=0.1)

qplot(c(x1,x2),c(y1,y2)) + theme_bw() + xlab("x") + ylab("y") 
@
\caption[Gestalt principles applied to statistical plots]{\label{fig:gestalt} \emph{Proximity} renders the fifty points of the first scatterplot as two distinct (and equal-sized) groups. Shapes and colors create three groups of points in the middle scatterplot by invoking the Gestalt principle of \emph{Similarity}. \emph{Good Continuation} renders the points in the scatterplot on the right hand side into two groups of points on curves: one a straight line with an upward slope, the other a curve that initially decreases and at the end of the range shows an uptick.}
\end{figure}

The plots in Figure~\ref{fig:gestalt} demonstrate several of the gestalt principles which combine to order our perceptual experience from the top down. These laws help to order our perception of charts as well: points which are colored or shaped the same are perceived as belonging to a group (similarity), points within a bounding interval or ellipse are perceived as belonging to the same group (common region), and regression lines with confidence intervals are perceived as single units (continuity and common region). 

The processing of visual stimuli utilizes low-level feature detection, which occurs automatically in the preattentive perceptual phase, and higher-level mental heuristics which are informed by experience. Both types of mental processes utilize physical location, color, and shape  to organize perceptual stimuli and direct attention to graphical features which stand out. 

Research on preattentive perception is important because features that are perceived preattentively require less mental effort to process from raw visual stimuli than non-preattentive features. Top-down gestalt heuristics are subsequently applied to the categorized features in order to make sense of the visual scene once the attentive stage of perception is reached. 

\newdo{There are two sequential transitions here that should either be combined or separated a bit... I'm still considering how to do this well.}

This paper describes the design and the results of a user study exploring the hierarchy of gestalt principles in the perception of statistical graphics. We utilize information from previous studies~\citep{heer:2014, robinson:03, healey1996high} concerning the hierarchy of preattentive feature perception in order to maximize the effect of preattentive feature differences. 


Statistical graphics can be difficult to examine experimentally; qualitative studies rely on descriptions of the plot by participants who may not be able to articulate their observations precisely, while quantitative studies may only be able to examine whether the viewer can accurately read numerical information from the chart, instead of exploring the overall utility of the data display holistically. Here, we are describing the setup and results of a study using statistical lineup methodology to provide quantitative and qualitative information.

\paragraph{Statistical lineups} are an important experimental tool for quantifying the significance of a finding in a graphical display. 
Lineups fuse commonly used psychological tests (target identification, visual search) \citep{visualreasoning} with statistical hypothesis tests, thereby enabling formal experimental evaluation of statistical graphics. 

Lineups are an experimental tool designed to serve as a visually conducted hypothesis test, separating significant effects from those that would be expected under a null hypothesis \citep{buja2009statistical, majumder2013validation,hofmann2012graphical, wickham2010graphical}. 
A statistical lineup consists of (usually) 20 sub-plots, arranged in a grid (examples are shown in Figure~\ref{fig:plotExamples}). 
Of these plots, one plot is the ``target plot'', generated from either real data or an alternate model (equivalent to $H_A$ in hypothesis testing); the other 19 plots are generated either using bootstrap samples of the real data or by generating ``true null" plots from the null distribution $H_0$. 
If a participant can identify the target plot from the field of distractors, this counts as evidence against the null hypothesis. 
Based on the number of evaluations and the number of target identifications, significance of a finding is then determined in the same sense as for a conventional hypothesis test. 
Performance on lineups has been shown to depend primarily on logical reasoning ability, and does not depend significantly on statistical training \citep{visualreasoning}.

Apart from the hypothesis testing construct, the use of statistical lineups to test statistical graphics conforms nicely to psychological testing constructs such as visual search \citep{demita1981validity,treisman1980feature}, where a single target is embedded in a field of distractors and response time, accuracy, or both are used to measure the complexity of the underlying psychological processes leading to identification. 

% modification
In this paper we {\bf modify the lineup protocol} by introducing a second target to each lineup. 
% competing signals
The two targets represent two different, competing signals; an observer's choice then demonstrates empirically which signal is more salient. 
Cognitively, the presence of two targets leads to a dual-target search scenario~\citep{fleck2010generalized}, which effectively introduces a `masking' effect where the more salient target is selected and the search for more targets stops (``satisfaction of search"), i.e.\ people tend to pick the more salient target and not notice the second target, even though in the absence of the more salient target they would have picked it out. 

From a statistical perspective, this is analogous to a Bayesian framework in which the relative strengths of two competing models are evaluated by a comparison with each other and with a common null.

In the present study, participants were allowed to submit multiple selections to prevent any forced-choice scenario which might skew the results. However, only 0.6\% of the evaluations resulted in an identification of both targets. 

The search for multiple targets is a more demanding cognitive task \citep{cain2011anticipatory} that is more sensitive to contextual effects \citep{adamo2015targets}, but without any time constraints imposed by the experimental protocol, participants can be expected to identify at least one of the plots with accuracy comparable to a single-target search task. 
% identification of one
%Identification of the less-salient target can be expected to suffer due to ``satisfaction of search",\citep{fleck2010generalized} but this will emphasize the any differences between the two competing targets.

%By tracking the proportion of observers choosing either target plot (a measure of overall lineup difficulty) as well as which proportion of observers choose one target over the other target, we can determine the relative strength of the two competing signals amid a field of distractors. 
%At this level, signal strength is determined by the experimental data and the generating model; we are measuring the ``power" (in a statistical sense) of the human perceptual system, rather than raw numerical signal. 

Using this testing framework, we  apply different aesthetics, such as color and shape, as well as plot objects which display statistical calculations, such as trend lines and bounding ellipses. 
These additional plot layers, discussed in more detail in the next section, are designed to emphasize one of the two competing targets and affect the overall visual signal of the target plot relative to the null plots. 
We expect that in a situation similar to the third plot of Figure~\ref{fig:gestalt}, the addition of two trend lines would emphasize the ``good continuation" of points in the plot, producing a stronger visual signal, even though the underlying data has not changed. 
Similarly, the grouping effect in the first plot in the figure should be enhanced if the points in each group are colored differently, as this adds similarity to the proximity heuristic. 
In plots that are ambiguous, containing some clustering of points as well as a linear relationship between $x$ and $y$, additional aesthetic cues may ``tip the balance" in favor of recognizing one type of signal over the other.

The study in this paper is designed to inform our understanding of the perceptual implications of these additional aesthetics, in order to provide guidelines for the creation of data displays which provide visual cues consistent with gestalt heuristics and preattentive perceptual preferences. 

The next section discusses the particulars of the experimental design, including the data generation model, plot aesthetics, selection of color and shape palettes, and other important considerations. 
Experimental results are presented in section~\ref{sec:Results}, and implications and conclusions are discussed in section~\ref{sec:Conclusion}. 

\section{Experimental Setup and Design} \label{sec:ExperimentalDesign}
In this section, we discuss the models generating data for the two types of signal plots and the null plots, the selection of plot aesthetic combinations and aesthetic values, and the design and execution of the experiment.

\subsection{Data Generation}
Conventional lineups require a single ``target" data set and a method for generating null plots. 
When utilizing real data for target plots, null plots are often generated through permutations.

Here, it is possible to generate true null plots from a null model that do not depend on the data used in the target plot. 
This experiment measures two competing gestalt heuristics, proximity and good continuation, using two data-generating models. 
Both models provide data in the same range of values in $X$ and $Y$;  $M_C$ generates data  with $K$ clusters, while $M_T$ generates data with a positive linear relationship between $X$ and $Y$. 
Null datasets are created using a mixture model $M_0$ which combines $M_C$ and $M_T$. 
In order to facilitate mixing these two models, controls on cluster centers generated by $M_C$ ensure that $X$ and $Y$ have a positive linear relationship with a correlation $\rho \in (0.25, 0.75)$, similar to the linear relationship between datasets generated by $M_0$. 

These constraints provide some assurance that participants who select a plot with data generated from $M_T$ are doing so because of visual cues indicating a linear trend (rather than a lack of clustering compared to plots with data generated from $M_0$), and participants who select a plot with data generated from $M_C$ are doing so because of visual cues indicating clustering, rather than a lack of a linear relationship relative to plots with data generated from $M_0$. 

\subsubsection{Regression Model \texorpdfstring{$M_T$}{Mt}}
This model has the parameter $\sigma_T$ to reflect the amount of scatter around the trend line. It generates $N$ points $(x_i, y_i), i=1, ..., N$ where $x$ and $y$ have a positive linear relationship. The data generation mechanism is as follows: 

\begin{algorithm}\hfill\newline
  Input Parameters: sample size $N$, $\sigma_T$ standard deviation around the linear trend \\
  Output: $N$ points, in form of vectors $x$ and $y$.
  \begin{enumerate}
    \item Generate $\tilde{x}_i$, $i=1, ..., N$, as a sequence of evenly spaced points in $[-1, 1]$.\\
    This step ensures that the full range in $x$ is used, which in turn keeps the ratio of $x$ to $y$ range constant.
    \item Jitter $\tilde{x}_i$ by adding small uniformly distributed perturbations to each of the values: $x_i = \tilde{x}_i + \eta_i$, where $\eta_i \sim \text{Unif}(-z, z)$, $z = \frac{2}{5(N-1)}$.
    \item Generate $y_i$ as a linear regressand of $x_i$: $y_i = x_i + e_i$, $e_i \sim N(0, \sigma^2_T)$. Several values of $\sigma^2_T$ are shown in Figure \ref{fig:trends}. 
    \item Center and scale $x_i$, $y_i$.
  \end{enumerate}
\end{algorithm}

We compute the coefficient of determination for all of the plots to assess the amount of linearity in each panel, computed as 
\begin{equation}\label{eq:linearMeasure}
R^2 = 1 - \frac{RSS}{TSS},
\end{equation}
where TSS is the total sum of squares, $TSS = \sum_{i=1}^N \left(y_i - \bar{y}\right)^2$ and $RSS = \sum_{i=1}^N e_i^2$, the residual sum of squares.
The expected value of the coefficient of determination $E\left[R^2\right]$ in this scenario is 
\[
E\left[R^2\right] =  \frac{1}{1 + 3\sigma^2_T},
\]
because
$E[RSS] = N\sigma^2_T$ and $E[TSS] = \sum_{i=1}^N E\left[y_i^2\right]$  (as $E[Y] = 0$), where 
$$
E\left[y_i^2\right] = E\left[x_i^2 + e_i^2 + 2 x_ie_i\right] = \frac{1}{3} + \sigma^2_T. 
$$
The use of $R^2$ to assess the strength of the linear relationship (rather than the correlation) is indicated because human perception of correlation strength more closely aligns with $R^2$ \citep{bobko1979perception,lewandowsky1989perception}. 

\begin{figure}[ht]
<<trends, fig.width=8, fig.height=2.5, out.width='\\textwidth', echo=FALSE>>=
res <- data.frame(sd.trend = rep(c(0.1, 0.2, 0.3, 0.4), each = 45))
res <- res %>% group_by(sd.trend) %>% do(sim.line(N = 45, sd.trend = as.numeric(.[1,1])))

res$label <- paste("sigma[T] :",res$sd.trend)
qplot(x,y, data = res, pch = I(1)) + facet_grid(facets = .~label, labeller = "label_parsed") + theme_bw() + 
  theme(plot.margin = unit(c(0,0,0,0), "cm"))
@
\caption[Parameters affecting $M_T$]{\label{fig:trends} Set of scatterplots showing one draw each from the trend model $M_T$ for parameter values of  $\sigma_T \in \{0.1, 0.2, 0.3, 0.4\}$.}
\end{figure}

\subsubsection{Cluster Model \texorpdfstring{$M_C$}{Mc}} 
We begin by generating $K$ cluster centers on a $K \times K$ grid, then we generate points around selected cluster centers. 
\begin{algorithm}\hfill\newline
  Input Parameters:  $N$ points, $K$ clusters, $\sigma_C$ cluster standard deviation \\
  Output: $N$ points, in form of vectors $x$ and $y$. 
  \begin{enumerate}
    \item Generate cluster centers $(c^x_{i}, c^y_{i})$ for each of the $K$ clusters, $i=1, ..., K$:
      \begin{enumerate}
        \item in form of two vectors $c^{x}$ and $c^y$ of permutations of $\{1, ..., K\}$, such that
        \item the correlation between cluster centers \text{cor}$(c^{x}, c^{y})$ falls into a range of $[.25, .75]$.
      \end{enumerate}
      \item Center and standardize cluster centers $(c^x, c^y)$:  
      \[
        \tilde{c}^x_{i} = \frac{c^x_{i} - \bar{c}}{s_c} \ \ \text{ and } \ \ \tilde{c}^y_{i} = \frac{c^y_{i} - \bar{c}}{s_c},
      \]
      where $\overline{c} = (K+1)/2$ and $s_c^2 = \frac{K(K+1)}{12}$ for all $i = 1, ..., K$.
    \item For the $K$ clusters, we want to have nearly equal sized groups, but allow some variability. Cluster sizes  $g = (g_1, ..., g_K)$ with $N = \sum_{i=1}^K g_i$, for clusters $1, ..., K$ are therefore determined as a draw from a multinomial distribution: 
    \[
    g \sim \text{Multinomial }(K, p) \text{ where } p = \tilde{p}/\sum_{i=1}^K \tilde{p}_i, \text{ for } \tilde{p} \sim N \left(\frac{1}{K}, \frac{1}{2 K^2} \right).
    \]
     
    \item Generate points around cluster centers by adding small normal perturbations: 
      \begin{eqnarray*}
        x_i &=& \tilde{c}^x_{g_i} + e^x_i, \text{ where } e^x_i \sim N(0, \sigma^2_C),\\
        y_i &=& \tilde{c}^y_{g_i} + e^y_i, \text{ where } e^y_i \sim N(0, \sigma^2_C).
      \end{eqnarray*}
    \item Center and scale $x_i$, $y_i$.
  \end{enumerate}
\end{algorithm} 

\begin{figure}[bht]
<<cluster, fig.width=8, fig.height=4.5, out.width='\\textwidth', echo=FALSE>>=

colors <-  c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", 
             "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf")
shapes <- c(1,0,3,4,8,5,2,6,-0x25C1, -0x25B7)

colortm <- read.csv("../..//Data/color-perceptual-kernel.csv")
# colortm[3,4] <- 0
# colortm[4,3] <- 0
colortm[8,] <- 0
colortm[,8] <- 0

shapetm <- read.csv("../../Data/shape-perceptual-kernel.csv")
# shapetm[9:10,] <- 0
# shapetm[, 9:10] <- 0
shapetm[9,] <- 0
shapetm[,9] <- 0
shapetm[10,] <- 0
shapetm[,10] <- 0

color3pal <- best.combo(3, colors, colortm)
color5pal <- best.combo(5, colors, colortm)
shape3pal <- best.combo(3, shapes, shapetm)
shape5pal <- best.combo(5, shapes, shapetm)

res <- data.frame(sd.cluster= rep(c(0.15, 0.20, 0.25, 0.30), each=45))
res <- res %>% 
  group_by(sd.cluster) %>% 
  do(data.frame({set.seed(325098573); sim.clusters(K=3, N=45, sd.cluster=as.numeric(.[1,1]))}))

res$K <- 3
res$color <- color3pal[res$group]
res$shape <- shape3pal[res$group]

res2 <- data.frame(sd.cluster= rep(c(0.15, 0.20, 0.25, 0.30), each=75))
res2 <- res2 %>% 
  group_by(sd.cluster) %>% 
  do({set.seed(325098573); sim.clusters(K=5, N=75, sd.cluster=as.numeric(.[1,1]))})

res2$K <- 5
res2$color <- color5pal[res2$group]
res2$shape <- shape5pal[res2$group]
res <- rbind(res, res2)
suppressMessages(library(ggplot2))
res$label <- paste("sigma[C] :",res$sd.cluster)
res$Klabel <- paste("K :",res$K)

ggplot(aes(x=x, y=y, color=color, shape=shape), data=res) + 
  geom_point() + 
  facet_grid(facets=Klabel~label, labeller="label_parsed") + theme_bw() + 
  theme(plot.margin=unit(c(0,0,0,0), "cm"), legend.position="none") + 
  scale_shape_identity() + scale_color_identity() + theme(aspect.ratio=1)
@
\caption[Parameters affecting $M_C$]{\label{fig:clusters} 
Scatterplots of clustering output for different inner cluster spread $\sigma_C$  (left to right) and different number of clusters $K$ (top and bottom), generated using the same random seed at each parameter setting. 
The colors and shapes shown are those used in the lineups for $K=3$ and $K=5$.}
\end{figure}
As a measure of cluster cohesion we use a coefficient to assess the amount of variability within each cluster, compared to total variability. 
Note that for the purpose of clustering, variability is measured as the variability in both $x$ and $y$ from a common mean, i.e.\ we implicitly assume that the values in $x$ and $y$ are on the same scale. 
This ensures that $\sigma_C$is a scaling parameter that regulates the amount of cluster cohesion (see Figure~\ref{fig:clusters}).  

For two numeric variables $x$ and $y$ and grouping variable $g$ with $g_i \in \{1, ..., K\}, i = 1, ..., n$, we compute the  {\it cluster index} $C^2$ as follows: 
let $j(i)$ be the function that maps index $i = 1, ..., n$ to one of the clusters $1, ..., K$ given by the grouping variable $g$. 
Then for each  level of $g$, we find  a cluster center as $\bar{x}_{j(i)}$ and  $\bar{y}_{j(i)}$, and we determine the strength of the clustering by comparing the within cluster variability with the overall variability: 

\begin{eqnarray}\label{eq:clusterMeasure}
C^2 &=& 1 - \frac{CSS}{TSS},\\
\nonumber CSS &=& \sum_{i=1}^n \left(x_{j(i)} - \overline{x}_{j(i)}\right)^2 + \left(y_{j(i)} - \overline{y}_{j(i)} \right)^2, \\
\nonumber TSS &=& \sum_{i=1}^n \left(x_i - \bar{x}\right)^2 + \left(y_i - \bar{y}\right)^2.
\end{eqnarray}

The cluster index $C^2$, which is approximately inversely linear in $\sigma_C^2$, measures the actual amount of clustering in the generated data.



\subsubsection{Null Model \texorpdfstring{$M_0$}{M0}}
The generative model for null data is a mixture model $M_0$ that draws $n_c \sim \text{Binomial}(N, \lambda)$ observations from the cluster model, and $n_T = N - n_c$ from the regression model $M_T$. 
Observations are assigned to specific clusters using hierarchical clustering, which creates groups consistent with any structure present in the generated data. 
This provides a plausible grouping for use in aesthetic and statistics requiring categorical data (color, shape, bounding ellipses). 

\begin{figure}[hbt]
<<lambda, fig.width=8, fig.height=3.5, out.width='.95\\textwidth', echo=FALSE>>=

colors <-  c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", 
             "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf")

res <- data.frame(lambda=rep(c(0, .25, .5, .75, 1), each=45))
res <- res %>% group_by(lambda) %>% do(data.frame({set.seed(325098573); mixture.sim(as.numeric(.[1,1]), K=3, N=45, sd.trend=.25, sd.cluster=.25)}))
#res <- ldply(lambda, function(x) { data.frame(lambda=x, {set.seed(325098573); mixture.sim(x, K=3, N=45, sd.trend=.25, sd.cluster=.25)}) })
res$K <- 3
res$color <- color3pal[res$group]
res$shape <- shape3pal[res$group]
#res2 <- ldply(lambda, function(x) { data.frame(lambda=x, {set.seed(325098573); mixture.sim(x, K=5, N=75, sd.trend=.25, sd.cluster=.2)}) })

res2 <- data.frame(lambda=rep(c(0, .25, .5, .75, 1), each=45))
res2 <- res2 %>% group_by(lambda) %>% do(data.frame({set.seed(325098573); mixture.sim(as.numeric(.[1,1]), K=5, N=45, sd.trend=.25, sd.cluster=.2)}))

res2$K <- 5
res2$color <- color5pal[res2$group]
res2$shape <- shape5pal[res2$group]
res <- rbind(res, res2)
res$label <- paste("lambda :",res$lambda)
res$Klabel <- paste("K :",res$K)
ggplot(aes(x=x, y=y, color=color, shape=shape), data=res) + 
  geom_point() + 
  facet_grid(facets=Klabel~label, labeller="label_parsed") + theme_bw() + 
  theme(plot.margin=unit(c(0,0,0,0), "cm"), legend.position="none") + 
  scale_shape_identity() + scale_color_identity() + theme(aspect.ratio=1)
@
\caption[Mixing parameter for null model $M_0$]{\label{fig:lambda} Scatterplots of data generated from $M_0$ using different values of $\lambda$, generated using the same random seed at each $\lambda$ value.}
\end{figure}

Null data in this experiment is generated using $\lambda = 0.5$, that is, each point in a null data set is equally likely to have been generated from $M_C$ and $M_T$ to ensure maximal distance of the null plots from either target. 

\subsubsection{Parameters used in Data Generation}
Models $M_C$, $M_T$, and $M_0$ provide the foundation for this experiment; by manipulating cluster standard deviation $\sigma_C$ and regression standard deviation $\sigma_T$ for varying numbers of clusters $K=3, 5$, we  systematically control the statistical signal present in the target plots and generate corresponding null plots that are mixtures of the two distributions. 
For each parameter set $\{K, N, \sigma_C, \sigma_T\}$, as described in Table~\ref{tab:parameters}, we  generate a lineup dataset consisting of one set drawn from $M_C$, one set drawn from $M_T$, and 18 sets drawn from $M_0$. 

\begin{table}[hbtp]
  \rowcolors{2}{gray!25}{white}
\begin{center}
\begin{tabular}{lll}
\bf Parameter & \bf Description & \bf Choices\\\hline
$K$ & \# Clusters &  \begin{tabular}{l}3, 5 \end{tabular} \\
$N$ & \# Points &  \begin{tabular}{l}$15\cdot K$\end{tabular} \\
$\sigma_T$ & Scatter around trend line &   \begin{tabular}{l} .25, .35, .45  \end{tabular}\\
$\sigma_C$ & Scatter around cluster centers & \begin{tabular}{ll} .25, .30, .35 ($K=3$)\\ .20, .25, .30 ($K=5$) \end{tabular}
\\\hline
\end{tabular}
\end{center}
\caption{Parameter settings for generation of lineup datasets. \label{tab:parameters}}
\end{table}

The parameter values were chosen in an approach similar to that taken in \citet{niladri:2014}: for each combination of $\sigma_T\in\{0.2, 0.25, ..., 0.5\}$, $\sigma_C\in\{0.1, 0.15, ..., 0.4\}$, and $K\in\{3,5\}$ we simulated 1000 lineup datasets. 
Then trend and cluster strength indices, $R^2$ and $C^2$, were computed  for the simulated target plots, and compared to the most extreme value for each of the 18 null plots of the same lineup data.

The resulting distributions allow us to objectively assess the difficulty of detecting the target datasets computationally (without relying on human perception) within the full parameter space. 
That is, a target plot with $R^2=0.95$ is very easy to identify when surrounded by null plots with $R^2=0.5$, while null plots with $R^2=0.9$ make the target plot more difficult to identify. 

Figure~\ref{fig:targetsignal-0} shows  densities of each measure computed from the  maximum of 18 null plots compared to the measure in the signal plot for one combination of parameters.
There is some overlap in the distribution of $R^2$ for the null plots compared to the target plot displaying data drawn from $M_T$. 
As a result, the distribution of the cluster statistic values are more easily separated from the null data sets than the distribution of the trend statistic, e.g. $\sigma_C = 0.20$ is producing cluster target data sets that are a bit easier to identify numerically than trend targets with a parameter value of $\sigma_T = 0.25$.

\begin{figure}[ht]
\centering
<<null-distribution-1, echo=FALSE, cache=T, fig.width=8, fig.height=3>>=
source("../../Code/MixtureLineups.R")
sT = 0.25
sC = 0.20
N = 45
K = 3
M = 1000

if (file.exists("../../Data/SmallSimulation.Rdata")) {
  load("../../Data/SmallSimulation.Rdata")
} else {
  nulldist<- function(N, sT=0.25, sC=0.2) {
    nulls <- data.frame(t(replicate(N, {
      lp <- data.frame(t(replicate(18, {
        mix = mixture.sim(lambda=0.5, K=3, N=45, sd.cluster=sC, sd.trend=sT)
        reg <- lm(y~x, data=mix)
        
  c(fline=summary(reg)$r.squared, fgroup=cluster(mix))
  })))
    c(fline=max(lp$fline), fgroup=max(lp$fgroup))
  })))
  
  trends <- replicate(10, {
    mix = mixture.sim(lambda=0, K=3, N=45, sd.cluster=sC, sd.trend=sT)
    reg <- lm(y~x, data=mix)
    c(fline=summary(reg)$r.squared)
  })
  
  clusters <- replicate(10, {
    mix = mixture.sim(lambda=1, K=3, N=45, sd.cluster=sC, sd.trend=sT)
    clust <- lm(y~factor(group) + 0, data=mix)
    res <- summary(aov(clust))
    c(fgroup=cluster(mix))
  })
  
  list(nulls=nulls, trends=trends, clusters=clusters)
}

res <- nulldist(N=N, sC=sC, sT=sT)
  library(compiler)
  tmp <- function(M=1000, N=45, K=3, sT=0.3, sC=0.25) {
    data.frame(
      t(
        replicate(M, 
                  {
                    input.pars <- list(N=N, K=K, sd.trend=sT, sd.cluster=sC)
                    c(unlist(input.pars), eval.data(gen.data(input.pars)))
                    }
                  )
        )
      )
    }
  nulldist <- cmpfun(tmp)
  
  res <- nulldist(M=M, N=45, K=3, sT=sT, sC=sC)
  
  save(res, file="./fig/nulldist.Rdata")
}

longres <- melt(res, id.vars = 1:4, variable.name = "type", value.name = "value")
longres$dist <- c("Data", "Most Extreme of\n18 Null Dists")[1 + grepl("null", longres$type)]
longres$type <- gsub("null.", "", longres$type, fixed = T)
longres$Statistic <- longres$type
longres$Statistic[longres$type == "cluster"] <- "Cluster Measure"
longres$Statistic[longres$type == "line"] <- "R squared"
longres$Statistic[longres$type == "gini"] <- "Gini Impurity"
longres$Statistic <- factor(longres$Statistic, levels = c("R squared", "Cluster Measure", "Gini Impurity"))

ggplot(data = subset(longres, Statistic != "Gini Impurity")) + 
  geom_area(aes(x = value, y = ..density.., color = dist, fill = dist), 
            stat = "density", position = "identity", 
            alpha = 0.5) + 
  xlab("Simulated Distribution of Test Statistic") + 
  ylab("Density") + 
  facet_grid(. ~ Statistic,  labeller = label_both) + 
  scale_color_manual("Distribution", values = c("black",  "gray")) + 
  scale_fill_manual("Distribution", values = c("transparent", "gray")) +  
  theme_bw() + 
  theme(legend.position = c(.15, .8))

@
\caption[Simulation-based test statistic density for null and target plots]{
\label{fig:targetsignal-0}
Density of test statistics measuring trend strength and cluster strength for target distributions and null plots based on 1,000 draws of lineup data with $\sigma_T= 0.25, \sigma_C=0.20$ and $K=3$.
}
\end{figure}

Graphical summaries of simulation results for a range of values for $\sigma_C$ and $\sigma_T$ are provided in Appendix \ref{app:parametersimulation}. 
Using information from the simulation, we identified values and generate lineup data sets for each  $\sigma_T$ and $\sigma_C$ (as shown in Table~\ref{tab:parameters}) corresponding to ``easy", ``medium" and ``hard" numerical comparisons between corresponding target data sets and null data sets. 
It is important to note that the numerical measures we have described in equations~\eqref{eq:linearMeasure} and~\eqref{eq:clusterMeasure} only provide information on the numerical discriminability of the target datasets from the null datasets; the simulation cannot provide us with exact information on the perceptual discriminability.
It has been established that human perception of scatterplots does not replicate statistical measures exactly \citep{bobko1979perception, mosteller1981eye, lewandowsky1989perception}.

Each of the generated datasets is then plotted as a lineup using aesthetics which emphasize clusters and/or linear relationships, in order to experimentally determine how these aesthetics change a participant's preference and ability to identify each target plot. 
The next section describes the aesthetic combinations and their anticipated effect on participant responses. 

\begin{table}[hbtp]
\centering
\scalebox{0.8}{
\begin{tabular}{ccccc}
\begin{tabular}{c} \phantom{.}\\ \phantom{.} \end{tabular} && \multicolumn{3}{c}{\cellcolor{gray!25} Trend Emphasis} \\
& Strength & 0 & 1 & 2 \\
\cellcolor{gray!25}\begin{tabular}{c} \phantom{.}\\ \phantom{.} \end{tabular} & 0 &  \cellcolor{gray!5} None &  \cellcolor{gray!15} Trend &  \cellcolor{gray!25} Trend + Error \\
\cellcolor{gray!25}\begin{tabular}{c} \\ Cluster \end{tabular} & 1 &  \cellcolor{gray!15}\begin{tabular}{c}Color\\ Shape\end{tabular} & \cellcolor{gray!5} Color + Trend \\
\cellcolor{gray!25}\begin{tabular}{c}  Emphasis\\ \phantom{.} \end{tabular} & 2 & \cellcolor{gray!25}\begin{tabular}{c} Color + Shape\\ Color + Ellipse \end{tabular} && \cellcolor{gray!5}\begin{tabular}{c} Color + Ellipse +\\
Trend + Error \end{tabular}\\
\cellcolor{gray!25}\begin{tabular}{c} \phantom{.}\\ \phantom{.} \end{tabular} & 3 & \cellcolor{gray!35} Color + Shape + Ellipse 
\end{tabular}}
\caption[Aesthetics affecting perception of statistical plots]{Plot aesthetics and statistical layers which impact perception of statistical plots, according to gestalt theory. \label{tab:plotaesthetics}}
\end{table}
\subsection{Lineup Rendering}
\subsubsection{Plot Aesthetics}
Gestalt perceptual theory suggests that perceptual features such as shape, color, trend lines, and boundary regions modify the perception of ambiguous graphs, emphasizing clustering in the data (in the case of shape, color, and bounding ellipses) or linear relationships (in the case of trend lines and prediction intervals), as demonstrated in Figure~\ref{fig:gestalt}. 
For each dataset we examine the effect of the plot aesthetics (color, shape) and statistical layers (trend line, boundary ellipses, prediction intervals) shown in Table~\ref{tab:plotaesthetics} on target identification.
Examples of these plot aesthetics are shown in Figure~\ref{fig:plotExamples}.

\begin{figure}[ht]
\centering
\begin{subfigure}[t]{0.24\textwidth}
  \caption{Plain}\vspace{-0.15in}
  \includegraphics[width=\textwidth]{fig/samplepics-1}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
  \caption{Color}\vspace{-0.15in}
  \includegraphics[width=\textwidth]{fig/samplepics-2}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
  \caption{Shape}\vspace{-0.15in}
  \includegraphics[width=\textwidth]{fig/samplepics-3}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
  \caption{Shape + Color}\vspace{-0.15in}
  \includegraphics[width=\textwidth]{fig/samplepics-4}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
  \caption{Color + Ellipse}\vspace{-0.15in}
  \includegraphics[width=\textwidth]{fig/samplepics-5}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
  \caption{\scalebox{.8}{Color + Shape + Ellipse}}\vspace{-0.15in}
  \includegraphics[width=\textwidth]{fig/samplepics-6}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
  \caption{Trend}\vspace{-0.15in}
  \includegraphics[width=\textwidth]{fig/samplepics-7}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
  \caption{Trend + Error }\vspace{-0.15in}
  \includegraphics[width=\textwidth]{fig/samplepics-8}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
  \caption{Trend + Color}\vspace{-0.15in}
  \includegraphics[width=\textwidth]{fig/samplepics-9}
\end{subfigure}
\begin{subfigure}[t]{.5\textwidth}
\centering
  \caption{\scalebox{.8}{Trend + Error + Color + Ellipse}}\vspace{-0.15in}
  \includegraphics[width=.49\textwidth]{fig/samplepics-10}
\end{subfigure}
\caption[Sample lineup stimuli for each of the 10 aesthetic combinations]{
Overview of each of the 10 plot feature combinations compared in this study, with $K=3$, $\sigma_T=0.25$ and $\sigma_C=0.20$. 
\label{fig:plotExamples}
}
\end{figure}

\afterpage{\clearpage}

We expect that relative to a plot with no extra aesthetics or statistical layers, the addition of color, shape, and 95\% boundary ellipses increases the probability of a participant selecting the target plot with data generated from $M_C$, the cluster model, and that the addition of these aesthetics  decreases the probability of a participant selecting the target plot with data generated from $M_T$, the trend model. 

Similarly, we expect that relative to a plot with no extra aesthetics or statistical layers, the addition of a trend line and prediction interval (``error band") increases the probability of a participant selecting the target plot with data generated from $M_T$, the trend model, and decreases the probability of a participant selecting the target plot with data generated from $M_C$, the cluster model.

\subsubsection{Color and Shape Palettes}
Colors and shapes used in this study were selected in order to maximize preattentive feature differentiation. \citet{heer:2014} provide sets of 10 colors and 10 shapes, with corresponding distance matrices, determined by user studies. Using these perceptual kernels for shape and color, we identified a maximally differentiable set of 3 and 5 colors each. 

\begin{figure}[bhtp]\centering
\caption{Color and shape palettes investigated for differentiability in \protect\citet{heer:2014}. }
\begin{subfigure}[t]{0.475\textwidth}
<<color-palette, dev='cairo_pdf', echo=FALSE, fig.width=5, fig.height=1, out.width='\\textwidth'>>=
colors <- c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", 
            "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf")
qplot(x=1:10, y=0, color=colors, size=I(5)) + scale_color_identity() + theme_lineup()
@
\caption[Color palette used to maximize preattentive perception]{Color Palette. For the present study  gray was removed from the palette to make the experiment more inclusive of participants with color deficiency.\label{fig:colors}}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.475\textwidth}
<<shape-palette, dev='cairo_pdf', echo=FALSE, fig.width=5, fig.height=1, out.width='\\textwidth',message=F, warning=F>>=
shapes <- c(1,0,3,4,8,5,2,6,-0x25C1, -0x25B7)

qplot(x=1:10, y=0, shape=shapes, size=I(5)) + scale_shape_identity() + theme_lineup()
@
\caption[Shape palette used to maximize preattentive perception]{Shape palette. Due to varying point size between Unicode and non-Unicode characters, the last two shapes were not used in this study.\label{fig:shapes}}
\end{subfigure}
\end{figure}
%\afterpage{clearpage}

The color palette used in \citet{heer:2014} and shown in Figure~\ref{fig:colors} is derived from colors available in the visualization software Tableau~\citep{tableau}. 

% \comment{the following statement sounds like a non-sequitur - removing gray to take care of red/green color deficiency does not sound like the first idea that comes to ones mind. How can we re-phrase that to make it less startling?}
In order to produce experimental stimuli accessible to the approximately 4\% of the population with red-green color deficiency \citep{colorvision}, we removed the gray hue from the palette, as gray is often difficult to distinguish from red and green for those with protanopia and deuteranopia, the most common types of colorblindness. This modification also resulted in maximally different color combinations that did not include red-green combinations, which would also impact the ability of color-deficient individuals to participate fully in this experiment.  

Software compatibility issues led us to exclude two shapes used in \citet{heer:2014} and shown in Figure~\ref{fig:shapes}. The left and right triangle shapes (available only in unicode within R) were excluded from our investigation due to size differences between unicode and non-unicode shapes. After optimization over the sum of all pairwise distances, the maximally different shape sequences for the 3 and 5 cluster datasets also conform to the guidelines in \citet{robinson:03}: for $K=3$ the shapes are from Robinson's group 1, 2, and 9, for $K=5$ the shapes are from groups 1, 2, 3, 9, and 10. Robinson's groups are designed so that shapes in different groups show differences in preattentive properties; that is, they are easily distinguishable. In addition, all shapes are non-filled, making them consistent with one of the simplest solutions to overplotting of points in the tradition of \citet{tukey, cleveland:85} and \citet{few}. For this reason we abstained from the additional use of alpha-blending of points to diminish the effect of overplotting in the plots.

\subsection{Experimental Design}
The study is designed hierarchically, as a factorial experiment for combinations of $\sigma_C$, $\sigma_T$, and $K$, with three replicates at each parameter combination. These parameters are used to generate lineup datasets which serve as blocks for the plot aesthetic level of the experiment; each dataset is rendered with every combination of aesthetics described in Table~\ref{tab:plotaesthetics}. Participants are assigned to generated plots according to an augmented balanced incomplete block scheme: each participant is asked to evaluate 10 plots, which consist of one plot at each combination of $\sigma_C$ and $\sigma_T$, randomized across levels of $K$, with one additional plot providing replication of one level of $\sigma_C\times\sigma_T$. Each of a participant's 10 plots presents a different aesthetic combination.

\subsection{Hypotheses}
The primary purpose of this study is to understand how visual aesthetics affect signal detection in the presence of competing signals. We expect that plot modifications which emphasize similarity and proximity, such as color, shape, and 95\% bounding ellipses,  increase the probability of detecting the clustering relationship, while plot modifications which emphasize good continuation, such as trend lines and prediction intervals, increase the probability of detecting the linear relationship. 

A secondary purpose of the study is to relate signal strength (as determined by dataset parameters $\sigma_C$, $\sigma_T$, and $K$) to signal detection in a visualization by a human observer.


\subsection{Participant Recruitment}
Participants were recruited using Amazon's Mechanical Turk service\citep{amazon}, which connects interested workers with ``Human Intelligence Tasks" (HITs), which are (typically) short tasks that are not easily automated. Only workers with at least 100 previous HITs at a 95\% successful completion rate were allowed to sign up for completing the task. These restrictions reduce the amount of data cleaning required by ensuring that participants have experience with the Mechanical Turk system, as well as a vested interest in doing well. 


Participants had to complete a pre-trial before being able to access the experiment. 
%Before access to the experiment was given, participants had to complete a an example task similar to the task in the experiment before deciding whether or not to complete the HIT. 
The lineups used in the pre-trial contained only a single target,% (5 trend and 5 cluster trials were provided), 
and participants had to correctly identify the target in at least two lineups. %before being allowed into  the HIT and proceeding to the experimental phase. 
The webpage used to collect data from Amazon Turk participants is available at \url{https://erichare.shinyapps.io/lineups/}. No data was recorded from the pre-trial because participants had not provided informed consent at this point. 

Once participants completed the example task and provided informed consent, they could accept the HIT through Amazon and were directed to the main experimental task. 

\subsection{Task Description}
Participants were required to complete ten lineups, answering ``Which plot is the most different from the others?". Participants were asked to provide a short reason for their choice, such as ``Strong linear trend" or ``Groups of points", and to rate their confidence in their selection from 0 (least confident) to 5 (most confident). 
After the first question, basic demographic information was collected: age range, gender, and highest level of education. 

Throughout the experiment, participants were not informed about the inclusion of a second target into the lineup plots.  
The small number of participants choosing multiple plots in their answer suggests that most participants did not discover that two target plots were present in each lineup and were thus naive to the true purpose of the experiment.

\section{Results}\label{sec:Results}
<<results-setup,echo=F,include=F>>=
lineups <- read.csv("../../Data/data-picture-details-gini.csv", stringsAsFactors=FALSE)
lineups$pic_id_old <- lineups$pic_id
lineups$pic_id <- 1:nrow(lineups)

users <- read.csv("../../Data/turk16_users_anon.csv", stringsAsFactors=F, header=F, skip = 1)
names(users) <- c("nick_name", "age", "gender", "education", "ip_address")
users$age <- factor(users$age, levels=0:10, labels=c("NA", "<18", "18-25", "26-30", "31-35", "36-40", "41-45", "45-50", "51-55", "56-60", "61+"))
users$gender <- factor(users$gender, levels=0:2, labels=c("NA", "Male", "Female"))
users$education <- users$education %>% replace(users$education==5, 4) %>% 
  factor(levels = 0:4, labels = c(NA, "High School or less", "Some college", "Bachelor's degree", "Grad school or higher"))

userdata <- read.csv("../../Data/turk16_results_anon.csv", stringsAsFactors=FALSE)
userdata$response.id <- 1:nrow(userdata)

tmp <- merge(userdata[!is.na(userdata$pic_id),], lineups[,c("pic_id", "sample_size", "test_param", "param_value", "p_value", "obs_plot_location")], all.x=T, all.y=F)
tmp$k <- as.numeric(substr(tmp$param_value, 3, 3))
tmp$sd.line <- as.numeric(substr(tmp$param_value, 12, 15))
tmp$sd.cluster <- as.numeric(substr(tmp$param_value, 25, 28))

correct.ans <- function(x,y){
  x1 <- as.numeric(str_trim(unlist(str_split(x, ","))))
  answers <- str_trim(unlist(str_split(y, ",")))
  lineplot <- as.numeric(answers[1])
  groupplot <- as.numeric(answers[2])
  giniplot <- ifelse(groupplot==as.numeric(answers[3]) | lineplot==as.numeric(answers[3]), NA, as.numeric(answers[3]))
  cbind(n.answers=length(x1), trend.correct=lineplot%in%x1, cluster.correct=groupplot%in%x1, both.correct = lineplot%in%x1 & groupplot%in%x1, neither.correct=!(lineplot%in%x1 | groupplot%in%x1), none.correct=!(lineplot%in%x1 | groupplot%in%x1 | giniplot%in%x1), gini.correct=giniplot%in%x1)
}

useranswers <- tmp %>% group_by(response.id) %>% do(data.frame(correct.ans(.["response_no"], .[,"obs_plot_location"]), drop=FALSE))

useranswers <- merge(useranswers, tmp)
useranswers$plottype <- gsub("turk16-", "", useranswers$test_param)
useranswers$plottype <- factor(useranswers$plottype, levels=c("plain", "trend", "color", "shape", "colorShape", "colorEllipse", "colorTrend",  "trendError", "colorShapeEllipse", "colorEllipseTrendError"))
useranswers$sd.cluster <- factor(useranswers$sd.cluster)
useranswers$sd.line <- factor(useranswers$sd.line)
useranswers$k <- factor(useranswers$k)
useranswers$start_time <- ymd_hms(useranswers$start_time)
useranswers$end_time <- ymd_hms(useranswers$end_time)
useranswers <- useranswers %>% group_by(param_value, test_param) %>% mutate(
  param_idx=as.numeric(factor(pic_id))
)

useranswers <- useranswers %>% group_by(ip_address, nick_name) %>% 
  mutate(ntrials=length(unique(pic_id)), 
         trial.no = rank(start_time), 
         trial.num=order(start_time))

# Remove data from <18 participants
useranswers <- filter(useranswers, !nick_name%in%users$nick_name[users$age=="<18"])
users <- filter(users, age!="<18")

modeldata <- useranswers[,c(1, 2, 9:31, 3:8)]

# Remove data from participants who did not complete 10 trials
incomplete.participants <- unique(modeldata$nick_name[modeldata$ntrials<10])
incomplete.participant.data <- sum(modeldata$ntrials<10)
message(paste0(sum(modeldata$ntrials<10), " trials removed because participant completed <10 trials total."))

# Remove data from participants who completed > 10 trials
modeldata <- filter(modeldata, ntrials>=10)
extra.participant.data <- sum(modeldata$trial.num>10)
message(paste0(sum(modeldata$trial.num>10), " trials removed because participant >10 trials."))
modeldata <- filter(modeldata, trial.num<=10)

# Remove users from database who didn't complete any trials
message(paste0(sum(!users$nick_name %in% modeldata$nick_name), " users removed from user database - no trials found."))
users <- users %>% filter(nick_name %in% modeldata$nick_name)

modeldata$outcome <- paste(c("", "trend")[1+as.numeric(modeldata$trend.correct==1)], 
                           c("", "cluster")[1+as.numeric(modeldata$cluster.correct==1)], 
                           c("", "neither")[1+as.numeric(modeldata$neither.correct==1)], 
                           c("", "gini")[1+as.numeric(modeldata$gini.correct==1)], 
                           sep="")
modeldata$outcome[modeldata$both.correct==1] <- "both"
modeldata$first.trial <- modeldata$trial.no == 1
modeldata$simpleoutcome <- gsub("gini", "", modeldata$outcome)
modeldata$simpleoutcome <- factor(modeldata$simpleoutcome, levels=c("neither", "cluster", "trend","both"))

modeldata <- merge(modeldata, lineups[,c("pic_id", "data_name", "param_value")], all.x=T, all.y=T)
modeldata$dataset <- factor(str_extract(modeldata$data_name, "set-\\d{1,3}") %>% str_replace("set-", "") %>% as.numeric)
modeldata$individualID <- factor(sprintf("%s-%s", modeldata$ip_address, modeldata$nick_name))
# data_name and k is not reliable. Use lineups-parameters-new instead
modeldata$k <- factor(modeldata$k, levels=c(3, 5))
modeldata$parameter.value <- factor(gsub("set-\\d{1,3}-", "", modeldata$data_name))
modeldata$start_time <- ymd_hms(modeldata$start_time)
modeldata$end_time <- ymd_hms(modeldata$end_time)
modeldata$trial.time <- with(modeldata, end_time-start_time)

lps <- read.csv("../../Data/lineups-parameters-new.csv")
modeldata <- merge(modeldata, lps[,c("set","k_new","sdline_new","sdgroup_new", "gini.min", "group.min", "no.ellipse", "max.range")], by.x="dataset", by.y="set")

# should be replaced, but carefully
modeldata <- modeldata %>% group_by(k) %>% mutate(
  trend.diff=c("easy", "medium", "hard")[as.numeric(droplevels(sd.line))], 
  cluster.diff=c("easy", "medium", "hard")[as.numeric(droplevels(sd.cluster))]
)

modeldata$trend.diff <- factor(modeldata$trend.diff, levels=c("easy", "medium", "hard"))
modeldata$cluster.diff <- factor(modeldata$cluster.diff, levels=c("easy", "medium", "hard"))
modeldata$cluster.diff2 <- factor(modeldata$cluster.diff, levels=c("easy", "medium", "hard"), labels=c("Cluster: Easy", "Cluster: Medium", "Cluster: Hard"))
modeldata$trend.diff2 <- factor(modeldata$trend.diff, levels=c("easy", "medium", "hard"), labels=c("Trend: Easy", "Trend: Medium", "Trend: Hard"))

parameter.design <- unique(modeldata[,c("dataset", "k", "trend.diff", "cluster.diff")])

plot.eval.tab <- apply(with(modeldata, table(dataset, plottype)), 1:2, sum)

# long dataset for table-esque plots
modeldata.long <- melt(modeldata, id.vars=which(!grepl("(correct)|(outcome)", names(modeldata))), value.vars=c("trend.correct", "cluster.correct", "neither.correct"), value.name="correct", variable.name="answer.type")
modeldata.long$answer.type <- gsub(".correct", "", modeldata.long$answer.type)
modeldata.long <- filter(modeldata.long, answer.type %in% c("cluster", "trend", "neither"))
modeldata.long$correct <- as.numeric(modeldata.long$correct)
modeldata.long$answer.type <- factor(modeldata.long$answer.type, levels=c("cluster", "trend", "neither"))
modeldata.long$plottype <- 
  modeldata.long$plottype %>%
  str_replace("color", "Color + ") %>% 
  str_replace("[sS]hape", "Shape + ") %>%
  str_replace("[tT]rend", "Trend + ") %>%
  str_replace("Ellipse", "Ellipse + ") %>%
  str_replace("Error", "Error + ") %>%
  str_replace("plain", "Plain") %>%
  str_replace("( \\+ )$", "") %>% 
  factor(levels=c("Plain", "Color", "Shape", "Trend", "Trend + Error", 
                  "Color + Shape", "Color + Ellipse", "Color + Trend", 
                  "Color + Shape + Ellipse", "Color + Ellipse + Trend + Error"),
         labels=c("Plain", "Color", "Shape", "Trend", "Trend + Error", 
                  "Color + Shape", "Color + Ellipse", "Color + Trend", 
                  "Color + Shape + Ellipse", "Color + Ellipse + Trend + Error"))

save(modeldata, file="../../Data/modeldata.Rdata")

totaltime <- modeldata %>% group_by(individualID) %>% summarize(
  total.experiment.time = max(end_time)-min(start_time)
)
@

\subsection{General results \& Demographics}
Data collection was conducted over a \Sexpr{round(as.numeric(difftime(max(modeldata$end_time), min(modeldata$start_time), units="hours")))} hour period, 
during which time \Sexpr{length(unique(userdata$nick_name))} individuals completed \Sexpr{nrow(userdata)} unique lineup evaluations. 
Participants who completed fewer than 10 lineups were removed from the study (\Sexpr{length(incomplete.participants)} participants, \Sexpr{incomplete.participant.data} evaluations), and lineup evaluations in excess of 10 for each participant were also removed from the study (\Sexpr{extra.participant.data} evaluations). 
After these data filtration steps, our data consist of \Sexpr{nrow(modeldata)} trials completed by \Sexpr{length(unique(modeldata$individualID))} participants. 

Of the participants who completed at least 10 lineup evaluations, \Sexpr{round(mean(users$gender=="Male")*100)}\% were male, relatively younger than the US population and relatively well educated (see Figure~\ref{fig:demographics}). 
Each plot was evaluated by between \Sexpr{min(plot.eval.tab)} and \Sexpr{max(plot.eval.tab)} individuals 
(\Sexpr{sprintf("Mean: %.2f, SD: %.2f", mean(plot.eval.tab), sd(plot.eval.tab))}).
\Sexpr{round(100*(1-mean(modeldata$neither.correct)), 1)}\% of the participant evaluations identified at least one of the two target plots successfully (Trend: \Sexpr{round(100*(mean(modeldata$trend.correct)), 1)}\%, Cluster: \Sexpr{round(100*(mean(modeldata$cluster.correct)), 1)}\%). 

\begin{figure}[ht]
\begin{subfigure}[t]{0.55\textwidth}
<<demographics-1, echo=FALSE, fig.width=8.53, fig.height=4, fig.keep='all', fig.show='hold', out.width='\\textwidth'>>=
qplot(age, data=users, fill=I("grey50"), colour=I("grey40")) + theme_bw() + xlab("Age of participants") + ylab("# Participants") + ggtitle("Participant Age Distribution")
@
\end{subfigure}
\begin{subfigure}[t]{0.43\textwidth}
<<demographics-2, echo=FALSE, fig.width=6.6, fig.height=4, fig.keep='all', fig.show='hold', out.width='\\textwidth'>>=
qplot(education, data=users, fill=I("grey50"), colour=I("grey40")) + theme_bw() + xlab("(self-reported) Highest level of participants' education") + ylab("# Participants") + ggtitle("Participant Education Levels")
@
\end{subfigure}
\caption{\label{fig:demographics}Basic demographics of participants.}
\end{figure}


From Figure~\ref{fig:targets} we see that participants identified more cluster targets than trend targets (there were more aesthetics expected to emphasize clustering in the data), but also did not primarily identify one target type over the other. Generally a participant picked both types over the course of ten lineups.

\begin{figure}[ht]
\centering
<<targets, fig.width=6, fig.height=4.0, out.width='0.55\\textwidth', echo=FALSE>>=
user.data <- modeldata %>% group_by(individualID) %>% 
  summarize(answers=length(individualID),
            cluster=sum(cluster.correct),
            trend=sum(trend.correct))
clusters <- as.data.frame(table(user.data$cluster))
trends <- as.data.frame(table(user.data$trend))
names(clusters) <- c("x", "Cluster")
clusters$Trend <- trends$Freq 
clm <- melt(clusters, measure.var=2:3)

ggplot() + geom_point(aes(x, value, colour=variable, shape=variable), size=3, data=clm) + theme_bw() + scale_colour_brewer("Target", palette="Set1") + scale_shape_discrete("Target") + theme(legend.position="bottom") + ylab("Number of participants") + xlab("Number of target identifications (out of ten)") + geom_line(aes(x, value, colour=variable, group=variable), data=clm) + ggtitle("Target Identification by Participants")
@
\caption{\label{fig:targets}Target identifications by participants. Generally,  participants are not primed for one target over the other.}
\end{figure}

For each plot type (aesthetic combination), we first consider the probability that a participant selects one of the two target plots, and then we consider the conditional probability of selecting the cluster target  over the trend target. 
<<target-model, echo=FALSE>>=
modeldata$one.correct <- 1 - modeldata$neither.correct
gol <- glmer(
  one.correct~sdline_new+sdgroup_new+k_new + trial.num + plottype + (1 | data_name) + (1 | individualID), 
  data = modeldata, family = binomial(), 
  control = glmerControl(
                   optimizer="bobyqa",
                   optCtrl = list(maxfun = 1e5)
                )
)
#gol.model <-  glmer(
#  one.correct~plottype + (1 | data_name) + (1 | individualID), 
#  data = modeldata, family = binomial(), control = glmerControl(optimizer = "bobyqa")
#)
gol.fixef <- data.frame(confint(gol, method="Wald"))[-(c(1:2,4:7)),] # exclude sigmas
names(gol.fixef) <- c("LB", "UB")
gol.fixef$OR <- fixef(gol)[-(2:5)]
gol.fixef[1,1:3] <- 0


suppressMessages(require(multcomp))
type_compare <- glht(gol, mcp(plottype="Tukey"))
gol.fixef$letters <- cld(type_compare)$mcletters$Letters
# not sure what to do about those warnings

gol.fixef$label <- gsub("plottype", "", names(fixef(gol)))[-(2:5)]
gol.fixef$label <- gol.fixef$label %>% 
  str_replace("color", "Color + ") %>% 
  str_replace("[sS]hape", "Shape + ") %>%
  str_replace("[tT]rend", "Trend + ") %>%
  str_replace("Ellipse", "Ellipse + ") %>%
  str_replace("Error", "Error + ") %>%
  str_replace("\\(Intercept\\)", "Plain + ") %>%
  str_replace("( \\+ )$", "") %>% 
  reorder(gol.fixef$OR)


sig <- anova(gol)

@

<<gol2, dependson='target-model', echo=FALSE>>=
gol.2 <- update(gol, .~.+gini.min) # large gini (i.e. homogeneity) leads to significant better probability of picking one of the targets
gol.2b <- update(gol.2, .~.+gini.min:plottype,
                 control = glmerControl(
                   optimizer="bobyqa",
                   optCtrl = list(maxfun = 1e5)
                )) # individual plot types are significantly different 
@

<<gol3, dependson='target-model', echo=FALSE>>=
modeldata$one.ellipse <- modeldata$no.ellipse == 1
gol.3 <- update(gol, .~.+one.ellipse) # not significant, nor is the number of missing ellipses, nor the absence of at least one ellipse
gol.3b <- update(gol.3, .~.+one.ellipse:plottype) 
@

<<gol4, dependson='target-model', echo=FALSE>>=
gol.4 <- update(gol, .~.+max.range) # not significant
gol.4b <- update(gol, .~.+max.range:plottype) # not significant
@

<<gol5, dependson='target-model', echo=FALSE>>=
gol.5 <- update(gol, .~.+log(as.numeric(trial.time)), 
                control = glmerControl(
                  optimizer="bobyqa",
                  optCtrl = list(maxfun = 2e5)
                )) # there is a (highly) significant effect of (log) response time on accuracy. The size of the effect is on average -0.405486 for each unit increase in (log) response time.

gol.5b <- update(gol, .~. + poly(log(as.numeric(trial.time)),2), 
                control = glmerControl(
                  optimizer="bobyqa",
                  optCtrl = list(maxfun = 1e5)
                ))
@

\subsection{Target Plot Identifications}

\begin{figure}[h!bt]\centering
<<faceoffdata, echo=FALSE, fig.width=9, fig.height=4.75, out.width='.75\\textwidth'>>=
md <- data.frame(plottype = levels(modeldata$plottype), 
                 tendency=c("none", "trend", "cluster", "cluster", "cluster", "cluster", "conflict", "trend", "cluster", "conflict"))

modeldata$label <- modeldata$plottype
modeldata$label <- modeldata$plottype %>% 
  str_replace("color", "Color + ") %>% 
  str_replace("[sS]hape", "Shape + ") %>%
  str_replace("[tT]rend", "Trend + ") %>%
  str_replace("Ellipse", "Ellipse + ") %>%
  str_replace("Error", "Error + ") %>%
  str_replace("plain", "Plain") %>%
  str_replace("( \\+ )$", "")

faceoff <- subset(modeldata, trend.correct | cluster.correct)

fac.order <- levels(with(faceoff, reorder(label,label,function(x) -length(x))))
modeldata$label <- factor(modeldata$label, levels=fac.order)

totals <- modeldata %>% group_by(label, plottype) %>% summarize(eval=length(label), type="Total")
correct.totals <- faceoff %>% group_by(label, plottype) %>% summarize(eval=length(label), type="Correct")

sub.totals <- modeldata %>% group_by(label, plottype, simpleoutcome) %>% summarize(eval=length(label))
sub.totals <- merge(sub.totals, md, by="plottype")

totals <- rbind(totals, correct.totals)
totals <- merge(totals, md, by="plottype")
totals$tendency <- factor(totals$tendency, levels=c("none", "trend", "cluster", "conflict"))
totals$label <- factor(totals$label, levels=fac.order, ordered=T)

md <- merge(md, subset(totals, type=="Correct")[,c("plottype", "label", "eval")], by="plottype")
md <- md[order(md$tendency, md$eval),]

totals$label <- factor(totals$label, levels=md$label)
  
totals$nlabel=as.numeric(totals$label)
sub.totals$nlabel = as.numeric(factor(sub.totals$label, levels=md$label))
sub.totals$simpleoutcome <- factor(
  sub.totals$simpleoutcome,
  levels=c("cluster", "both", "trend", "neither"))
sub.totals$tendency <- factor(sub.totals$tendency, levels=c("none", "trend", "cluster", "conflict"))
ggplot() +
  geom_bar(aes(label, fill=simpleoutcome, weight=eval), data=sub.totals, 
           colour="grey40") +
  facet_grid(.~tendency, space="free", scales="free") +
  xlab("") + ylab("# of target evaluations") +
  theme_bw() +
  theme(axis.text.x = element_text(angle=30, hjust=1, vjust=1)) +
  scale_fill_manual("Outcome", values=c("grey20", "grey30", "grey20", "grey90"),
                    guide = guide_legend(reverse=TRUE)) 
@
\caption{\label{fig:onetarget} In dark: number of evaluations by plot type, in which at least one of the targets was identified. Each of the dark areas is split into two, according to the type of target, with evaluations where both targets were identified between the two.
Due to the design of the experiment, each plot type was evaluated almost the same number of times (between 1195 and 1208 times, outlined rectangles).}
\end{figure}

In order to assess which of the two stimuli dominated in each of the plot types, we concentrate first on all those responses in which participants identified at least one of the targets (\Sexpr{sum(1-modeldata$neither)} trials). 
Figure~\ref{fig:onetarget} shows an overview of the number of evaluations by plot type (outlines) and the number of times participants chose at least one of the targets (dark shaded areas). 
Plot types  associated with  clustering as shown in Table~\ref{tab:plotaesthetics} lead to significantly fewer correct evaluations ($\chi^2_{9} = 389$, $p$-value $< 0.0001$). A  possible cause for this is discussed in more detail in section~\ref{sec:sentiment}. 


\subsection{Face-Off: Trend versus Cluster}\label{sec:faceoff}
 For all trials in which at least one of the targets was correctly identified, we compare the probability of selecting the cluster target generated by $M_C$  with the probability of selecting the trend target generated by $M_T$. 
Define $C_{ijk}$ to be the event $$\{\text{Participant }k\text{ selects the cluster target for dataset }j\text{ with aesthetic set }i\}$$
and $T_{ijk}$ to be the analogous selection of the trend target.
We model the cluster versus trend decision using a logistic regression with a random effect for each dataset to account for different difficulty levels in the generated data, and a random effect for participant to account for skill level, as shown in equation~\ref{eqn:faceoffModel}. 
\begin{align}
\text{logit }P(C_{ijk} | C_{ijk}\cup T_{ijk}) & = \textbf{W}\alpha +  \textbf{X}\beta + \textbf{J}\gamma + \textbf{K}\eta + \epsilon, \label{eqn:faceoffModel}
\end{align}
where \begin{itemize}
\item[$\alpha$] is a vector of fixed effects $\left(\mu, \alpha_T, \alpha_C, \alpha_K \right)$. $\mu$ is a baseline average of the probability to pick the cluster target over the trend target.  $\alpha_T$ and $\alpha_C$ are parameters for the effect of the standard error around trend lines $s_T \in \{0.25, 0.35, 0.45\}$ and with cluster variability $s_C \in \{0.2, 0.25, 0.3, 0.35\}$, and $\alpha_K$ is the effect of the number of clusters $K \in \{3, 5\}$.
\item[$\beta_i$] describe plot aesthetics,
\item[$\gamma_j$] $\overset{iid}{\sim} N\left(0, \sigma^2_{\text{data}}\right)$,  random effect for dataset specific characteristics,
\item[$\eta_k$] $\overset{iid}{\sim} N\left(0, \sigma^2_{\text{participant}}\right)$, random effect for participant characteristics,
\item[$\epsilon_{ijk}$] $\overset{iid}{\sim}  N\left(0, \sigma^2_e\right)$, error associated with a single trial evaluation.
\end{itemize}
We also assume that random effects for dataset and participant are orthogonal. 


The estimated log odds of a decision in favor of cluster over trend target for each of the plot types are shown in Figure~\ref{fig:faceoff}. From left to right the (log) odds of selecting the cluster target over the trend target increase. 
As hypothesized, the strongest signal for identifying groups, is color + shape + ellipse, while trend + error results in the strongest signal in favor of trends. 
Most of the effects are not significantly different, as seen in the letter values \citep{piepho:04} based on Tukey's Post Hoc difference tests on the left hand side of the figure, representing pairwise comparisons of all of the designs, adjusted for multiple comparison. 
The estimates for parameters $\alpha_C$ and $\alpha_T$, quantifying the effect of increased variability within clusters and around the trend line, are highly significant  and work as hypothesized: with an increase in the variability, the strength of the target's signal decreases and correspondingly the probability for detecting the corresponding target decreases significantly (see Table~\ref{tab:results} in appendix section~\ref{sec:faceoff} for exact numbers and a discussion of further co-variates).

\newdo{We found the interaction effects between $s_C$ and plot type and $s_T$ and plot type to be not significant, i.e. the results for the plot types are stable in that sense, that it does not seem to matter how difficult (at least within our parameter setting) a lineup is. XXX Susan, do we want to include this? it's in models gvl.4d and gvl.4e - I didn't include those models in the paper yet, but they are in expanded-model.R.
Numbers: $s_T$: plottype interaction: 11.15,9,0.2656 
$s_C$:plottype interaction: 6.9026,9,0.6473}
<<group-vs-line, echo=FALSE, warning = FALSE>>=
faceoff$one.ellipse <- faceoff$no.ellipse == 1

gvl.4 <-  glmer(cluster.correct~sdline_new + sdgroup_new +k_new + plottype + (1|data_name) + (1|individualID), data=faceoff, family=binomial(), control=glmerControl(optimizer="bobyqa"))

gvl.fixef <- data.frame(confint(gvl.4, method="Wald"))[-(1:2),] # exclude sigmas
names(gvl.fixef) <- c("LB", "UB")
gvl.fixef$OR <- fixef(gvl.4)
gvl.fixef <- rbind(gvl.fixef, c(0,0,0))
row.names(gvl.fixef)[14] <- "plottypePlain"

gvl.all <- gvl.fixef
gvl.fixef <- gvl.fixef[c(14,5:13),]

suppressMessages(require(multcomp))
type_compare <- glht(gvl.4, mcp(plottype="Tukey"))
gvl.fixef$letters <- cld(type_compare)$mcletters$Letters
# not sure what to do about those warnings
@

<<gvl5, dependson='group-vs-line', echo=FALSE>>=
gvl.5 <-  update(gvl.4, .~.+first.trial +log(as.numeric(trial.time)), control=glmerControl(optimizer="bobyqa"))
@

<<gvl6, dependson='group-vs-line', echo=FALSE>>=
gvl.6 <-  update(gvl.4, .~.+factor(conf_level), control=glmerControl(optimizer="bobyqa"))
@

<<gvl7, dependson='group-vs-line', echo=FALSE>>=
gvl.7 <-  update(gvl.4, .~.+one.ellipse, control=glmerControl(optimizer="bobyqa"))
aov7 <- anova(gvl.4, gvl.7)
gvl.7b <-  update(gvl.7, .~.+one.ellipse:plottype, control=glmerControl(optimizer="bobyqa"))
aov7b <- anova(gvl.7, gvl.7b)
@

<<gvl8, dependson='group-vs-line', echo=FALSE>>=
gvl.8 <-  update(gvl.4, .~.+gini.min, control=glmerControl(optimizer="bobyqa")) # not significant, not even a little bit
@

<<gvl9, dependson='group-vs-line', echo=FALSE>>=
gvl.9 <-  update(gvl.4, .~.+factor(no.ellipse), control=glmerControl(optimizer="bobyqa")) # only 1 missing ellipse has significant negative effect! beautiful!
@

\begin{figure}[h!bt]\centering
<<group-vs-line-fig, dependson='group-vs-line-fig', echo=FALSE, fig.width=9, fig.height=4, out.width='.95\\textwidth', warning = FALSE>>=
faceoff$one.ellipse <- faceoff$no.ellipse == 1

gvl.4 <-  glmer(cluster.correct~sdline_new + sdgroup_new +k_new + plottype + (1|data_name) + (1|individualID), data=faceoff, family=binomial(), control=glmerControl(optimizer="bobyqa"))

gvl.fixef <- data.frame(confint(gvl.4, method="Wald"))[-(1:2),] # exclude sigmas
names(gvl.fixef) <- c("LB", "UB")
gvl.fixef$OR <- fixef(gvl.4)
gvl.fixef <- rbind(gvl.fixef, c(0,0,0))
row.names(gvl.fixef)[14] <- "plottypePlain"

gvl.all <- gvl.fixef
gvl.fixef <- gvl.fixef[c(14,5:13),]

suppressMessages(require(multcomp))
type_compare <- glht(gvl.4, mcp(plottype="Tukey"))
gvl.fixef$letters <- cld(type_compare)$mcletters$Letters
# not sure what to do about those warnings


gvl.fixef$label <- gsub("plottype", "", row.names(gvl.fixef))
gvl.fixef$label <- gvl.fixef$label %>% 
  str_replace("color", "Color + ") %>% 
  str_replace("[sS]hape", "Shape + ") %>%
  str_replace("[tT]rend", "Trend + ") %>%
  str_replace("Ellipse", "Ellipse + ") %>%
  str_replace("Error", "Error + ") %>%
  str_replace("\\(Intercept\\)", "Plain + ") %>%
  str_replace("( \\+ )$", "") %>% 
  reorder(gvl.fixef$OR)

ggplot(data=gvl.fixef) + 
  geom_hline(yintercept=0, colour="gray70") + 
  geom_pointrange(aes(x=label, y=OR, ymin=LB, ymax=UB), size=.75) + 
  coord_flip() + 
  theme_bw() + 
  ggtitle("Odds of selecting Cluster over Trend Target") + 
  xlab("") +
  geom_text(aes(y=-1, x=label, label=letters)) + 
  scale_y_continuous(
    "Odds (on log scale) of selecting Cluster over Trend Target and 95% Wald Intervals\n(Reference level: Plain plot)", 
    breaks=c(-1, log(c(1/c(2, 1.75, 1.5,1.25), c(1,1.25,1.5, 1.75,2 ))), 1), labels=c("<--Trend\n  Target", "1/2  ","1/1.75", "1/1.5", "1/1.25","1","1.25","1.5", "1.75", "2", "Cluster-->\nTarget  "), limits=c(-1,1))
# dt <- xtable(gvl.fixef[order(gvl.fixef$OR), c("label", "OR", "LB", "UB")], digits=4)
# print(dt, include.rownames=FALSE)
@
\caption[Estimated odds of decision for cluster versus trend target]{\label{fig:faceoff} Estimated odds of decision for cluster versus trend target based on evaluations that resulted in the identification of one of these targets. Plot types are significantly different if they do not share a letter as given on the left hand side of the plot.}
\end{figure}

Examining the model results from the perspective of Gestalt heuristics, it is clear that the similarity/proximity effect, as indicated by spatial clustering and aesthetics such as color and shape, dominates the equation, including dominating the color + trend (similarity vs. continuity) condition. 

When trend line and prediction intervals (``error bands", or ``error" as an aesthetic description) are present in the same plot, the additional Gestalt principle of common region is recruited, in addition to the continuity heuristic present due to the trend line and the linear relationship between $x$ and $y$. The interaction between these heuristics dominates the perceptual experience, decreasing the probability that a participant will select the cluster target plot in favor of the trend target. 

This interaction effect explains the different outcomes seen by the two conditions with conflicting aesthetics: the color+trend condition is more likely to result in cluster plot selection, while the color + ellipse + trend + error condition is more likely to result in trend plot selection, because the combined effect of the gestalt heuristics present in the trend and prediction interval elements is stronger than the effect of color and ellipse elements, which only invoke Gestalt heuristics of similarity and common region. 

In summary, the results from this experiment show that in order to gain a significant difference from a plain representation and visually emphasize groups or trends, we need to make use of a statistical layer associated with a statistical interval/probability region in the form of an error band or an ellipse.

The lineup experimental protocol allows us to collect participant justifications for their target selection. 
These short explanations provide some additional insight into participant reasoning, and further support the gestalt explanation for the experimental results. 


\subsection{Participant Reasoning}\label{sec:sentiment}
As part of each trial, participants were asked to provide a short justification of their plot choice. Figure~\ref{fig:wordles} gives an overview of summaries of participants' reasoning in form of word clouds. In the word clouds, stopwords are excluded from participants' reasons, unless they refer to quantities, such as `none', `all', `some', `few', etc. Reasons are also stemmed, so that words such as `group', `groups', `grouping', `grouped', and so on, all appear as the same (most prevalent) word in the cloud. What can be seen is a strong focus in terms of the reasoning depending on the outcome. If the participant chose one of the targets, the reasoning reflects this choice. When neither of the targets is chosen, there is less focus in the response. The word clouds look surprisingly similar independently of plot type - with the exception of the Ellipse + Color plot: here, the mentioning of specific colors is indicative of participants' distraction from the intended target towards an imbalance of the color/cluster distribution. 

\begin{figure}[h!t]\centering
<<sentiment, eval=F, echo=FALSE, fig.width=7, fig.height=7, out.width='.3\\textwidth', fig.show='hold'>>=
lexicaldata <- modeldata
lexicaldata$choice_reason <- tolower(lexicaldata$choice_reason)
lexicaldata$choice_reason <- gsub("^_", "", lexicaldata$choice_reason)

words <- lexicaldata %>% group_by(plottype, simpleoutcome) %>% 
  summarize(list = paste(choice_reason, collapse=" ")) %>% filter(simpleoutcome != "both", plottype %in% c("plain", "color", "trend", "colorEllipse"))

library(wordcloud)
library(tm)
quantities <- c("all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only" )
stops <- setdiff(stopwords("english"), quantities)

for (i in 1:nrow(words)) {
words.corpus <- Corpus(DataframeSource(data.frame(words$list[i])))
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, function(x) removeWords(x, stops))

corpus.temp <- tm_map(words.corpus, stemDocument, language = "english")  
# stemCompletion doesn't seem to work, I am essentially following along that function
stems <- unlist(strsplit(corpus.temp[[1]]$content, split=" "))
complete <- unlist(strsplit(words.corpus[[1]]$content, split=" "))
dframe <- data.frame(stems, complete)
dframe <- dframe %>% group_by(stems) %>% mutate(mode=names(sort(table(complete), decreasing=TRUE))[1])
corpus.final <- corpus.temp
corpus.final[[1]]$content <- paste(dframe$mode, collapse=" ")



tdm <- TermDocumentMatrix(corpus.final)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
pal <- brewer.pal(9, "BuGn")
pal <- pal[-(1:2)]
wordcloud(d$word,d$freq, scale=c(8,.3),min.freq=2,max.words=100, random.order=T, rot.per=.15, colors="black", vfont=c("sans serif","plain"))
}
@

\centering
\begin{subfigure}[t]{0.275\textwidth}
  \caption{Plain, neither target} \vspace{-0.15in}
  \includegraphics[width=0.9\textwidth]{fig/sentiment-1}
\end{subfigure}
\begin{subfigure}[t]{0.275\textwidth}
  \caption{Plain, cluster target}\vspace{-0.15in}
  \includegraphics[width=0.9\textwidth]{fig/sentiment-2}
\end{subfigure}
\begin{subfigure}[t]{0.275\textwidth}
  \caption{Plain, trend target}\vspace{-0.15in}
  \includegraphics[width=0.9\textwidth]{fig/sentiment-3}
\end{subfigure}

\begin{subfigure}[t]{0.275\textwidth}
  \caption{Trend, neither target}\vspace{-0.2in}
  \includegraphics[width=.9\textwidth]{fig/sentiment-4}
\end{subfigure}
\begin{subfigure}[t]{0.275\textwidth}
  \caption{Trend, cluster target}\vspace{-0.2in}
  \includegraphics[width=.9\textwidth]{fig/sentiment-5}
\end{subfigure}
\begin{subfigure}[t]{0.275\textwidth}
  \caption{Trend, trend target}\vspace{-0.2in}
  \includegraphics[width=.9\textwidth]{fig/sentiment-6}
\end{subfigure}

\begin{subfigure}[t]{0.275\textwidth}
  \caption{Color, neither target}\vspace{-0.2in}
  \includegraphics[width=.9\textwidth]{fig/sentiment-7}
\end{subfigure}
\begin{subfigure}[t]{0.275\textwidth}
  \caption{Color, cluster target}\vspace{-0.2in}
  \includegraphics[width=.9\textwidth]{fig/sentiment-8}
\end{subfigure}
\begin{subfigure}[t]{0.275\textwidth}
  \caption{Color, trend target}\vspace{-0.2in}
  \includegraphics[width=.9\textwidth]{fig/sentiment-9}
\end{subfigure}

\begin{subfigure}[t]{0.275\textwidth}
  \caption{Color + Ellipse, neither}\vspace{-0.2in}
  \includegraphics[width=.9\textwidth]{fig/sentiment-10}
\end{subfigure}
\begin{subfigure}[t]{0.275\textwidth}
  \caption{Color + Ellipse, cluster}\vspace{-0.2in}
  \includegraphics[width=.9\textwidth]{fig/sentiment-11}
\end{subfigure}
\begin{subfigure}[t]{0.275\textwidth}
  \caption{Color + Ellipse, trend}\vspace{-0.2in}
  \includegraphics[width=.9\textwidth]{fig/sentiment-12}
\end{subfigure}
\caption[Wordclouds of participant responses for selected plot types]{\label{fig:wordles}Wordclouds of participants' reasoning by outcome for a selected number of plot types. Mostly, the reasoning and the choice of the target are highly associated. For the Color + Ellipse plot, participants were distracted from either target by an imbalance in the cluster/color distribution, as can be seen from the reasoning in the bottom left wordcloud.}
\end{figure}

Further examination of individual participants' responses illustrates that group size (and thus, missing ellipses in the target plot) was a factor in the decision to identify specific null plots as different. Participants provided responses such as ``There is no circle highlighting the yellow symbols in this plot" %line 13515 
and 
``Lack of a circle around the red symbols", %line 13516
 which highlight the visual cues which were missing from the identified null plots.  
%

This was due to an unintended side-effect of using the k-means algorithm for cluster allocation in the null plots: due to an imbalance in the size of clusters, an additional cue was introduced into the null plots.
%Our cluster allocation algorithm for null target plots may have produced unintentional results; rather than providing unambiguous gestalt cues which reinforced group separation, instead, our null plots provided mixed cues which varied the number of points in a cluster and the presence of the additional similarity cue. 
%Numerically, these null data sets had uneven cluster allocation; 
The estimation of bounding ellipses fails for groups with fewer than three points and in these cases, ellipses were not drawn. Visually, the conspicuous absence of an ellipse led participants to select null plots with that feature. 
When we include a variable encoding the absence of one of the ellipses in a lineup into model~(\ref{eqn:faceoffModel}) it is highly significant ($\chi^2_1$=\Sexpr{round(aov7$Chisq[2],1)}, $P$-value=\Sexpr{round(aov7$`Pr(>Chisq)`[2],4)}), with an estimate of $-0.8075$, indicating that if one of the ellipses in a lineup is missing, the probability of picking the cluster target is reduced to less than half (44.6\%) of the trend target. If we additionally consider the effect of a missing ellipse on individual plot types, it is also significant ($\chi^2_9$=\Sexpr{round(aov7b$Chisq[2],1)}, $P$-value=\Sexpr{round(aov7b$`Pr(>Chisq)`[2],4)}).  Appendix section~\ref{sec:accuracy} gives more details on this model and its effects.


\section{Discussion and Conclusions}\label{sec:Conclusion}

Taken together, the results presented suggest that plot aesthetics influence the perception of the dominant effect in the displayed data. 
This effect is not simply additive (otherwise, the two conflicting aesthetic conditions would result in similarly neutral effects); rather, the effect is consistent with layering of gestalt perceptual heuristics. 
Plot layers which add additional heuristics show larger effects than plot layers which duplicate heuristics that are already in play. 
For example, adding ellipses to a plot which has color aesthetics increases cluster recognition by recruiting the common region heuristic in addition to the point similarity heuristic recruited by color. 
Adding shape to a plot which has color aesthetics increases cluster recognition only slightly, but does not add additional gestalt heuristics (though point similarity is emphasized through two different mechanisms). 

Statistically, this is important because the addition of ellipses or prediction intervals provides important statistical context, while reinforcing the visual emphasis by addition of the common region heuristic. Graphics which more effectively convey the statistical results are composed of aesthetic layers which recruit multiple gestalt heuristics in order to present a unified message. This represents a departure from the ``show the data" mentality, but is still consistent with the goal of good graphics, that is, to convey the data in a way that is easily understandable while still providing appropriate detail. 

While more studies are necessary to fully explore the nonadditive mechanism of additional gestalt heuristics and understand their effect in other types of plots, these results demonstrate the importance of carefully constructing graphs to convey the most important aspects of the displayed data. 

\bibliographystyle{asa}
\bibliography{references}
\newpage
\begin{appendix}
\section{Simulation Study of the Parameter Space}\label{app:parametersimulation}

Using 1000 simulations for each of the 98 combinations of parameters ($K=\{3,5\}$, $\sigma_C=\{.1, .15, .2, .25, .3, .35, .4\}$, $\sigma_T=\{.2, .25, .3, .35, .4, .45, .5\}$), we explored the effect of parameter value on the distribution of summary statistics describing the strength of the linear relationship ($R^2$) and cluster strength for null and target plots. 

Figures~\ref{fig:simulationLineIntervals} and~\ref{fig:simulationClusterIntervals} show the 25th and 75th percentiles of the distribution of $R^2$ and cluster strength summary statistics for each set of parameter values. These plots guide our evaluation of ``easy", ``medium" and ``hard" parameter values for trend and cluster tasks. 

Additionally, we note that there is an interaction between $\sigma_C$ and $\sigma_T$: the distinction between target and null on a fixed setting of clustering becomes increasingly difficult as the standard deviation for the linear trend is increased, and vice versa. There may additionally be a three-way interaction between $\sigma_C, \sigma_T$, and $K$: the width of the blue intervals (bottom figure) changes  between different levels of $K$ and for different levels of $\sigma_C$ and $\sigma_T$. These interactions suggest that in order to examine differences in aesthetics, we must block by parameter settings (this can be accomplished through blocking by dataset). Each dataset is non-deterministic, because we have a random process generating from different parameter settings, not a deterministic run setting as in an engineering setting. It is thus important to use replicates of each parameter setting to ensure that we can separate data-level effects from parameter-level effects. 

<<simulationparameters,echo=F,include=F, fig.width=10, fig.height=6.5, out.width='.8\\textwidth'>>=
load("../../Data/SimulationDatasetCriteriaTurk16.Rdata")

dataset.criteria$ParameterSet <- with(dataset.criteria, sprintf("sdT%.2f-sdC%.2f", sd.trend, sd.cluster))
dataset.criteria$ParameterSet[dataset.criteria$type=="cluster"] <- with(dataset.criteria[dataset.criteria$type=="cluster",], sprintf("sdC%.2f-sdT%.2f", sd.cluster, sd.trend))

dataset.criteria$lsc <- paste("sigma[C]: ", round(dataset.criteria$sd.cluster, 2))
dataset.criteria$lst <- paste("sigma[T]: ", round(dataset.criteria$sd.trend, 2))
dataset.criteria$lK <- paste("K: ", dataset.criteria$K)
qplot(data=subset(dataset.criteria, type=="line"), x=LB, xend=UB, y=sd.cluster, yend=sd.cluster, color=dist, geom="segment") +
  geom_point(aes(x=LB, y=sd.cluster, color=dist))  + 
  geom_point(aes(x=UB, y=sd.cluster, color=dist)) + 
  facet_grid(lst~lK, scales="free", labeller="label_parsed")  + theme_bw() + 
  scale_color_brewer("Distribution",palette="Set1") + theme(legend.position="bottom") + xlab("Interquartile intervals of Max (18) null distribution (blue) \nand target distribution (red) of linearity measured in R squared.") + ylab(expression("Cluster variability":sigma[C]))

qplot(data=subset(dataset.criteria, type=="cluster"), x=LB, xend=UB, y=sd.trend, yend=sd.trend, color=dist, geom="segment") +
  geom_point(aes(x=LB, y=sd.trend, color=dist))  + 
  geom_point(aes(x=UB, y=sd.trend, color=dist)) + 
  facet_grid(lsc~lK, scales="free", labeller="label_parsed")  + theme_bw() + 
  scale_color_brewer("Distribution", palette="Set1") + theme(legend.position="bottom") + xlab("Interquartile intervals of Max (18) null distribution (blue) \nand target distribution (red) of amount of clustering.") + ylab(expression( "Variability along the trend":sigma[T]))

tmp <- subset(dataset.criteria, type=="gini")
tmp$dist <- gsub("Max", "Min", tmp$dist)
qplot(data=tmp, x=LB, xend=UB, y=sd.trend, yend=sd.trend, color=dist, geom="segment") +
  geom_point(aes(x=LB, y=sd.trend, color=dist))  + 
  geom_point(aes(x=UB, y=sd.trend, color=dist)) + 
  facet_grid(lsc~lK, scales="free", labeller="label_parsed")  + theme_bw() + 
  scale_color_brewer("Distribution", palette="Set1") + theme(legend.position="bottom") + xlab("Interquartile intervals of Min (18) null distribution (blue) \nand target distribution (red) of Gini Impurity.") + ylab(expression( "Variability along the trend":sigma[T]))
@

\begin{figure}[bht]\centering
\begin{subfigure}[t]{.8\textwidth}
\caption{$R^2$ values for target and null data distributions. \label{fig:simulationLineIntervals}}
\vspace{-.15in}
\includegraphics[width=\textwidth]{fig/simulationparameters-1}
\end{subfigure}
\begin{subfigure}[t]{.8\textwidth}
\caption{Cluster cohesion statistics $C^2$. \label{fig:simulationClusterIntervals}}
\vspace{-.15in}
\includegraphics[width=\textwidth]{fig/simulationparameters-2}
\end{subfigure}
\caption{Simulated interquartile ranges between target and most extreme statistic from one of the 18 null plots. }
\end{figure}
\clearpage

\section{Simulation based inference in a two-target lineup scenario}\label{sec:simulation}

Assume that there are two targets embedded in a lineup of overall size $m$, where $m$ in our experiment is taken to be $m=20$. Let $A$ be the event that one of these targets is chosen.
Under the null hypothesis that both targets are consistent with being created based on data from the null model, we can assume that under the null hypothesis the expected value of the probability that an observer picks one of these plots from the lineup is $2/m = E[ P(A \mid H_o) ]$.
For the distribution of $A \mid H_o$ we employ a simulation-based strategy:
Under the null hypothesis, we can assume, that the $p$-value corresponding to a hypothesis test `the presented data is consistent with the null model' has a standard uniform distribution, i.e. $p_i \sim U[0,1]$ i.i.d.~for all $1 \le i \le m$. We assume that the choice observers make can be modeled using a multinomial distribution, where the probability $\pi_i$ to pick panel $i$ is inversely linear to $p_i$, with $\sum_{i=1}^m \pi_i = 1$.

\begin{figure}[h!tbp]
<<twotarget-simu, echo=FALSE, results='hide'>>=
library(vinference) # install from heike/vinference github
set.seed(20140501)
K <- 10
res <- data.frame(rep=rep(1:10, each=K+1))
res <- res %>% group_by(rep) %>% do(
  dVsim(x=0:K, K=K, m=20, scenario=3, target=1:2, N=100000)
  )
res <- res %>% dplyr::ungroup() %>% dplyr::group_by(x)
means <- res %>% 
  dplyr::summarise(avg = mean(scenario3))
@
<<twotarget, dependson='twotarget-simu', echo=FALSE, out.width='0.7\\textwidth', fig.width=8, fig.height=5>>=
qplot(x, scenario3, data=res, shape=I(21), alpha=I(0.8)) +
theme_bw() + 
ylab("estimated probability") + xlab("Number of times (out of 10) one of the targets is picked") + 
  geom_segment(aes(x=x-0.3, xend=x+0.3, y=avg, yend=avg), data=means) + 
  geom_point(aes(x=x, y=binom), colour="steelblue", pch=4, size=4) + 
  scale_x_continuous(breaks=0:K, labels=0:K)
@
\caption{\label{fig:simulation} 
Ten simulations of size $b_2 = 1,000$ and $b_1 = 100$ for lineups of size $m=20$ assuming $K=10$ evaluations. 
The averages of the ten simulation runs are shown as lines. The crosses are probabilities from  Binomial $B_{2/20, 10}$.
}
\end{figure} 

W.l.o.g.\ we can assume that the two target plots are in positions 1 and 2. 
Given that a lineup was evaluated by $K$ individuals, the simulation process for the conditional probability of identifying one of the targets given that both are consistent with the null model, $P(A|H_o)$, is then as follows:
%
\begin{enumerate}
\item Pick two values $p_i \sim U[0,1], i=1, 2$.
\item Repeat $b_1$ times:
\begin{enumerate}
    \item Pick $m-2$ values $p_i \sim U[0,1], i=3, ..., m$.
    \item Pick $K$ values from a Multinomial distribution with $\pi = \frac{1-p}{|| 1- p||}$, i.e. $x_j \sim M_\pi, i=1, ..., K$
    \item Return the number of times that $x_j$ is 1 or 2. 
\end{enumerate} 
\end{enumerate}
Repeat the above process $b_2$ times, and average results for a distribution of $A \mid H_o$. 
The choice of $b_1$ and $b_2$ decides on the number of decimal places to which the estimated distribution can be used reliably. 

Figure~\ref{fig:simulation} shows the result of this simulation approach for a lineup of size 20 assuming $K=10$ evaluation. The density of $A \mid H_o$ is plotted for ten runs (open circles). The variability in the results is relatively small - for comparison, the density of a Binomial distribution $B_{2/20, 10}$ is shown using crosses. The main difference between the densities is the probability of zero or only one identification, while the tail probabilities are very similar.

%' \section{Group Size Inequality}\label{sec:gini}
%' \newdo{Susan, sorry - but I'd be in favor of taking this section out. It's too much detail. What I'd like to do instead, is including another variable in the models that captures some statistic describing the imbalance in the group - e.g. gini index or smallest group size or something along those lines. That will tie it into the cluster versus trend target analysis }
%' \newtext{In order to examine the effect of group size inequality, we used simulation (as described in Appendix \ref{app:parametersimulation} to examine the distribution of group size (as measured by gini impurity) in null datasets compared with cluster target datasets generated by the model. This establishes whether there were any systematic differences in group size inequality between data generated from $M_0$ (null data) and data generated from $M_C$ (cluster data). Figure \ref{fig:simulationGiniIntervals} demonstrates that the cluster plots have lower group size differences (e.g. are more equally sized) than null plots at all parameter combinations. It is therefore possible that some participants identified extraordinarily unequal group sizes present in null plots as significantly different from the other lineup plots, ignoring any cluster signal. }
%' 
%' \begin{figure}[ht]\centering
%' <<simulationparameters-gini,echo=F,fig.width=10, fig.height=6.5, out.width='.8\\linewidth', dependson='simulationparameters'>>=
%' tmp <- subset(dataset.criteria, type=="gini")
%' 
%' tmp$lsc <- paste("sigma[C]: ", round(tmp$sd.cluster, 2))
%' tmp$lst <- paste("sigma[T]: ", round(tmp$sd.trend, 2))
%' tmp$lK <- paste("K: ", tmp$K)
%' 
%' tmp$dist <- gsub("Max", "Min", tmp$dist)
%' qplot(data=tmp, x=LB, xend=UB, y=sd.trend, yend=sd.trend, color=dist, geom="segment") +
%'   geom_point(aes(x=LB, y=sd.trend, color=dist))  + 
%'   geom_point(aes(x=UB, y=sd.trend, color=dist)) + 
%'   facet_grid(lsc~lK, scales="free", labeller="label_parsed")  + theme_bw() + 
%'   scale_color_brewer("Distribution", palette="Set1") + theme(legend.position="bottom") + xlab("Interquartile intervals of Min (18) null distribution (blue) \nand target distribution (red) of Gini Impurity.") + ylab(expression( "Variability along the trend":sigma[T]))
%' @
%' \caption[Simulated IQR of Gini impurity for cluster and null distributions]{Simulated interquartile range of group size inequality statistic values for cluster and null data distributions. \label{fig:simulationGiniIntervals}}
%' \end{figure}
%' 
%' \newtext{Numerically, the null data sets did have uneven group allocation; bounding ellipse estimation failed for groups with fewer than 3 points and in these cases, ellipses were not drawn. Visually, the conspicuous absence of an ellipse will lead participants to select null plots with that feature (see section~\ref{sec:sentiment} for a more detailed look at participants' responses).}
%' 
%' \newtext{This effect actually provides some additional information as to the hierarchy of gestalt features: for plots displaying the same data (including at least one plot with cluster size $<3$), participants were more likely to identify the cluster target plot under the Color and Shape aesthetics than under Color + Ellipse or Color + Shape + Ellipse conditions. 
%' The presence of the ellipse (and the gestalt common region heuristic) dominated the effect of point similarity (albeit not as originally intended). 
%' In future experiments, it will be advantageous to control the variability in cluster size in order to remove the conflicting visual influence of gestalt common region heuristics with the greater similarity and proximity present in the target plot. }

\section{Modelling results}
There are two main types of models discussed in this section:  the models for accuracy  (section~\ref{sec:accuracy}), response times (section~\ref{sec:response}) and confidence levels (section~\ref{sec:confidence}) are based on all \Sexpr{nrow(modeldata)} available lineup evaluations, while the faceoff model (section~\ref{sec:faceoff}) uses only lineups where at least one of the targets was identified to investigate which variables have an effect on the balance between the targets.

\subsection{Accuracy Model}\label{sec:accuracy}
The lineup protocol providess an easy way of measuring accuracy of evaluations by assessing the number of participants who identified the data plot. In the modified version, we can use this as well by regarding any lineup evaluation resulting in an identification of at least one of the two targets as `accurate'. We therefore want to model the probability that participant $k$ identifies (at least) one of the targets on the lineup (using aesthetics set $i$) of dataset $j$:
\begin{align}
\text{logit }P(C_{ijk} \cup T_{ijk}) & = \textbf{W}\alpha +  \textbf{X}\beta + \textbf{J}\gamma + \textbf{K}\eta + \epsilon, \label{eqn:responseModel}
\end{align}
where \begin{itemize}
\item[$\alpha$] is a vector of fixed effects $(\mu, \alpha_T, \alpha_C, \alpha_K, \alpha_O)$, where $\mu$ an average baseline accuracy (and should not be interpreted, because $s_C$ and $s_T$ are assumed to be zero),  $\alpha_T$ and $\alpha_C$ for the effect of the standard error around trend lines $s_T \in \{0.25, 0.35, 0.45\}$ and clusters $s_C \in \{0.2, 0.25, 0.3, 0.35\}$, and $\alpha_K$ for the effect of the number of clusters $K \in \{3, 5\}$, $\alpha_O$ an order effect, i.e.\ an effect on accuracy over time,
\item[$\beta_i$] describe plot types,
\item[$\gamma_j$] $\overset{iid}{\sim} N\left(0, \sigma^2_{\text{data}}\right)$,  random effect for dataset specific characteristics,
\item[$\eta_k$] $\overset{iid}{\sim} N\left(0, \sigma^2_{\text{participant}}\right)$, random effect for participant characteristics,
\item[$\epsilon_{ijk}$] $\overset{iid}{\sim}  N\left(0, \sigma^2_e\right)$, error associated with a single trial evaluation.
\end{itemize}
We also assume that random effects for dataset and participant are orthogonal. 

<<gols, echo=FALSE>>=
gols.aov.2 <- anova(gol, gol.2)
gols.aov.2b <- anova(gol.2, gol.2b)
gols.aov.5 <- anova(gol, gol.5)
gols.aov.5b <- anova(gol.5, gol.5b)
@

%' <<pred.gol5b, echo=FALSE>>=
%' dframe <- data.frame(expand.grid(
%'   plottype=levels(modeldata$plottype),
%'   sdline_new=0.25,
%'   sdgroup_new=0.2,
%'   k_new=3,
%'   time=exp(seq(1,7, length.out=50)),
%'   gini.min=0
%' ))
%' dframe$k_new=factor(dframe$k_new)
%' preds <- predict(gol.5b, newdata=dframe, re.form=~0, type="response")
%' qplot(log(time),preds, data=dframe, colour=plottype, geom="line")
%' @
Table~\ref{tab:gol} shows an overview of the parameters of the accuracy model and their estimates. Both $\alpha_T$ and $\alpha_C$ have large negative effects that are highly significant. This indicates that as the signal in the target plots weakens (by an increase in variability around the trend line or within cluster variability), accuracy of participants decreases on average. $\alpha_K$ has a small negative effect, i.e.\ participants are on average answering lineups with three clusters with more accuracy than lineups with five clusters. However, this effect is not significant.
The order effect $\alpha_O$ is small, but significant; as participants answer more lineups, their accuracy decreases on average by about 2\% for each evaluation. 
\newdo{XXX Susan, this is a fatigue effect -- do you have a reference for this?}
\newdo{I can't remember how we ordered trials. It's possible that we put easier trials at the beginning, which would explain the effect. Fatigue is entirely possible too; I can dig up some reference relating to that just in case. Can you check with Eric about the ordering?}

\begin{table}[htbp]
% xtable(summary(gol)$coefficients, digits=c(0,2,2,2,4))
\centering
\begin{tabular}{rrrrr}
  \hline
Parameter & Estimate & Std.\ Error & $z$-value & Pr($>$$|$z$|$) \\ 
  \hline
$\mu$ & 6.47 & 1.08 & 6.01 & $< 0.0001$ \\ 
  $\alpha_T$ & -2.55 & 1.20 & -2.12 & 0.0339 \\ 
  $\alpha_C$ & -8.59 & 2.40 & -3.58 & 0.0003 \\ 
  $\alpha_{K}$ & -0.11 & 0.11 & -0.93 & 0.3523 \\ 
  $\alpha_O$  & -0.02 & 0.01 & -2.33 & 0.0199 \\ [3pt]
{\bf Plot type} \hfill Plain & 0.00 & --- & --- & --- \\ 
  Trend + Error & -0.08 & 0.14 & -0.57 & 0.5701 \\ 
  Shape & -0.08 & 0.14 & -0.57 & 0.5694 \\ 
  Trend & -0.35 & 0.13 & -2.61 & 0.0090 \\ 
  Color + Shape & -0.51 & 0.13 & -3.95 & 0.0001 \\ 
  Color & -0.60 & 0.13 & -4.64 & $< 0.0001$ \\ 
  Color + Trend & -0.70 & 0.13 & -5.49 & $< 0.0001$ \\ 
  Color + Ellipse + Trend + Error & -1.12 & 0.12 & -9.05 & $< 0.0001$ \\ 
  Color + Shape + Ellipse & -1.40 & 0.12 & -11.45 & $< 0.0001$ \\ 
  Color + Ellipse & -1.47 & 0.12 & -12.09 & $< 0.0001$ \\ 
   \hline
\end{tabular}
\caption{\label{tab:gol}Parameters and estimates of the accuracy model. }
\end{table}


We additionally investigate two more effects: (log) response time and the effect of imbalances in the group allocation on accuracy.  

\subsubsection{Effect of response time}
The effect of (log) response times on accuracy is highly significant ($\chi^2_1$=\Sexpr{round(gols.aov.5$Chisq[2],1)}, $P$-value=\Sexpr{round(gols.aov.5$`Pr(>Chisq)`[2],4)}). With each unit increase in (log) response time the probability for a target identification is reduced on average by about 1/3. However, in the long run, a secondary effect takes place, and response time has a positive effect on accuracy again. Fitting an additional quadratic term in the model is also highly significant ($\chi^2_1$=\Sexpr{round(gols.aov.5b$Chisq[2],1)}, $P$-value=\Sexpr{round(gols.aov.5b$`Pr(>Chisq)`[2],4)}), and leads to an overall minimum accuracy over time at a response time of about 150 seconds.
\newdo{This is somewhat atypicial, if I understand how speed/accuracy tradeoff tends to work: typically, the more time you spend on a problem the higher your likelihood of completing it successfully - that is, it is assumed that participants can solve it eventually no matter what. Instead, what we're seeing (if I understand this correctly) is that there's a speed-accuracy tradeoff after 150 seconds, but before that point, lower response time is associated with higher accuracy - that is, either you see it or you don't, and if you hang out past 150 seconds, then you can sometimes reason your way back into it. We probably need to explain this a bit more - it's not clear, for instance, that you're just adding more terms to the model fit above. The R code is clear, but the text is less so.}

\subsubsection{Effect of group imbalances}\label{sec:gini}

\newtext{Gini impurity measures the homogeneity of group allocations. Let $n_i$ be the number of elements in the $i$th cluster, $i = 1, ..., K$, with $n = \sum_{i=1}^K n_i$ and let $p_i = n_i / n$ be the frequency of cluster $i$. Then the gini impurity is calculated as 
\[
G(p_1, ..., p_K) = \frac{K}{K-1} \sum_{i=1}^K p_i (1 - p_i).
\]
$G$ is an index between 0 and 1, where 0 is maximum diversity - in the sense, that there is only one group present, i.e.\ $p_i = 0$ for all but one of the groups. A gini impurity of 1 indicates perfect homogeneity, i.e.\ $p_i = 1/K$ for all $i$.} 

Other features measuring the imbalance within a lineup that we consider besides (a) gini impurity are (b)  the difference between the maximum and the minimum number of elements in each of the groups of a lineup, and (c) the number of ellipses missing from the lineup.

The probability of picking at least one of the targets significantly increases with an increase in gini impurity, that is, with more equal group sizes ($\chi^2_1$=\Sexpr{round(gols.aov.2$Chisq[2],1)}, $P$-value=\Sexpr{round(gols.aov.2$`Pr(>Chisq)`[2],4)}). 

We further see a significant effect of gini impurity on individual plot types ($\chi^2_9$=\Sexpr{round(gols.aov.2b$Chisq[2],1)}, $P$-value $< 0.0001$).


% latex table generated in R 3.2.2 by xtable 1.8-0 package
% Mon Jan 25 12:40:45 2016
% <<tab:gols, echo=FALSE>>=
% xtable(anova(gol.model, gol.2, gol.2b))
% @

The range of group sizes does not have a significant effect on the probability to pick at least one of the targets, even if different plot types are taken into account.

Similarly, a single absent ellipse does not lead to a significant change in the probability of detecting one of the target plots. Neither the number of missing ellipses nor the absence of at least one ellipse have a significant effect on this probability, not even when we consider the impact of individual plot types.
%\newdo{they are all in the range of model gol.3}

To investigate the effect of these features on the balance between target models they are also included and discussed in the faceoff model of section~\ref{sec:faceoff}.

%\newpage
\subsection{Modelling response times}\label{sec:response}
While we do not have the same amount of control in an AMT study that we would have in a lab setting, we can accurately capture the time between presenting a lineup to a particpant and the time at which results are submitted. A histogram of these times is given in Figure~\ref{fig:histogram}. Response times are extremely skew. In the model we therefore use the log of response times $T = \left(t_{ijk}\right)_{n \times 1}$:
<<response, echo=FALSE>>=
#load("../../Data/modeldata.Rdata")

library(lubridate)
modeldata$start <- ymd_hms(modeldata$start_time)
modeldata$end <- ymd_hms(modeldata$end_time)
modeldata$time <- as.numeric(with(modeldata, end-start))
modeldata$k_new <- factor(modeldata$k_new)

library(lme4)
time <- lmer(log(time)~first.trial+plottype + simpleoutcome+k_new+sdline_new+sdgroup_new+(1|individualID)+(1|dataset), data=modeldata)
#time2 <- update(time, .~.-(1|dataset), data=modeldata)

#anova(time, time2) # hugely significant dataset effect? I wouldn't have guessed that. It might be an effect of the large number of evaluations

library(xtable)
library(multcomp)

table <- data.frame(summary(time)$coefficients)

table$pvals <- cftest(time)$test$pvalues
@

\begin{figure}
<<histogram, echo=FALSE, fig.width=7, fig.height=4, out.width='.6\\textwidth'>>=
library(ggplot2)
qplot(time, data=modeldata, binwidth = .1) + theme_bw() +
  ylab("# Evaluations") + xlab("Log Response time (in seconds)") +
  scale_x_log10() + 
  geom_vline(xintercept=median(modeldata$time), colour = "grey50")
@
\caption{\label{fig:histogram}Histogram of (log) response times. The median evaluation time (vertical line) is 29 seconds. }
\end{figure}

\begin{align}
\text{log } T & = \textbf{W}\alpha +  \textbf{X}\beta + \textbf{J}\gamma + \textbf{K}\eta + \epsilon, \label{eqn:responseModel}
\end{align}
where \begin{itemize}
\item[$\alpha$] is a vector of fixed effects $(\mu, \alpha_0, \alpha_T, \alpha_C, \alpha_K)$, where $\mu$ an average baseline response time (and should not be interpreted, because $s_C$ and $s_T$ are assumed to be zero), $\alpha_0$ the average effect of the first trial on the response time,  $\alpha_T$ and $\alpha_C$ for the effect of the standard error around trend lines $s_T \in \{0.25, 0.35, 0.45\}$ and clusters $s_C \in \{0.2, 0.25, 0.3, 0.35\}$, and $\alpha_K$ for the effect of the number of clusters $K \in \{3, 5\}$,
\item[$\beta_i$] describe plot types,
\item[$\gamma_j$] $\overset{iid}{\sim} N\left(0, \sigma^2_{\text{data}}\right)$,  random effect for dataset specific characteristics,
\item[$\eta_k$] $\overset{iid}{\sim} N\left(0, \sigma^2_{\text{participant}}\right)$, random effect for participant characteristics,
\item[$\epsilon_{ijk}$] $\overset{iid}{\sim}  N\left(0, \sigma^2_e\right)$, error associated with a single trial evaluation.
\end{itemize}
We also assume that random effects for dataset and participant are orthogonal. 

Table~\ref{tab:response} gives an overview of all parameters of the response time model and their estimates. Similar to what has been found in other lineup studies \citep{humanfactorslineups, hofmann2012graphical}, participants take on average 25\% longer to respond to the first lineup than to subsequent lineups. Aside from this, we see that as the difficulty of lineups increases (controlled by an increase in the parameters $s_C$ and $s_T$), the average amount of time participants spend on each evaluation significantly increases. 
Depending on the outcome of the evaluation, there are differences in the amount of time: if either one of the targets is identified, the amount of time taken to answer is significantly shorter than if neither of the targets is found. Answers take on average the longest, if both targets are identified (however, this only happens in 0.6\% of the responses). 
Plot aesthetics have a significant impact on the amount of time for responses, with increasing plot complexity associated with increased evaluation time. This may be a function of increased cognitive load, as participants must examine more features in order to identify which plot has the strongest signal. For instance, when color, ellipses, trend lines, and error bands are present, participants have to compare the allocation of color to points, the size, shape, and distance between each set of ellipses, the slope of each trend line, and the width of the error bands. While each participant almost certainly does not complete a full pairwise comparison of all 20 lineup plots across each feature set, the increased complexity of each additional feature does increase the space which must be examined using perceptual heuristics in order to identify the target plot correctly. This is consistent with \citet{borgo2012empirical}, who found that visual embellishments increase the time required to perform visual search tasks using data displays.
% xtable(table, digits=c(0,3,3,3,3), display=c("s","f","f","f","e"))
% latex table generated in R 3.2.2 by xtable 1.8-0 package
% Wed Jan 20 22:41:48 2016
\begin{table}[htbp]
\centering
\begin{tabular}{rrrrr}
  \hline
Parameter & Estimate & Std.\ Error & $z$ value & $P$-value \\ 
  \hline
$\mu$ & 2.660 & 0.108 & 24.586 & $< 0.0001$ \\ 
  $\alpha_0$ & 0.231 & 0.014 & 16.366 & $< 0.0001$ \\
    $\alpha_{K = 3}$ & 0.000 & --- & --- & --- \\ 
    $\alpha_{K = 5}$ & 0.124 & 0.029 & 4.334 & $< 0.0001$ \\ 
  $\alpha_T$ & 0.461 & 0.152 & 3.039 & 0.0024 \\ 
  $\alpha_C$ & 1.703 & 0.300 & 5.673 & $< 0.0001$ \\ [3pt]
{\bf Plot Type} \hfill  Plain & 0.000 & --- & --- & --- \\ 
  Shape & 0.112 & 0.019 & 5.889 & $< 0.0001$ \\ 
  Color & 0.133 & 0.019 & 7.012 & $< 0.0001$ \\ 
  Trend & 0.148 & 0.019 & 7.815 & $< 0.0001$ \\ 
  Trend + Error & 0.166 & 0.019 & 8.759 & $< 0.0001$ \\ 
  Color + Ellipse & 0.205 & 0.019 & 10.735 & $< 0.0001$ \\ 
  Color + Shape & 0.214 & 0.019 & 11.281 & $< 0.0001$ \\ 
  Color + Trend & 0.215 & 0.019 & 11.345 & $< 0.0001$ \\ 
  Color + Shape + Ellipse & 0.205 & 0.019 & 10.699 & $< 0.0001$ \\ 
  Color + Ellipse + Trend + Error & 0.252 & 0.019 & 13.215 & $< 0.0001$ \\ [3pt]
 {\bf Target} \hfill   Trend & -0.182 & 0.016 & -11.665 & $< 0.0001$ \\ 
  Cluster & -0.150 & 0.013 & -11.165 & $< 0.0001$ \\ 
  Neither & 0.000 & --- & --- & --- \\ 
  Both & 0.166 & 0.059 & 2.820 & 0.0048 \\ 
   \hline
\end{tabular}
\caption{\label{tab:response} Model parameters and estimates for (log) response time in seconds. The $P$-values are based on a normal approximation of the $t$ statistics.}
\end{table}

\subsection{Model of confidence levels}\label{sec:confidence}
With each lineup evaluation, participants were asked to give feedback on their level of confidence from 0 (least) to 5 (most). As an approximation, we can fit a mixed effects model with this variable as the dependent, and investigate its relationship with the parameters controlling difficulty of a lineup, the time taken to evaluate the lineup and its outcome. Let $C = \left(c_{ijk} \right)$ be the confidence level participant $k$ reports on the lineup (using aesthetics set $i$) of dataset $j$:

\begin{align}
C & = \textbf{W}\alpha +  \textbf{X}\beta + \textbf{J}\gamma + \textbf{K}\eta + \epsilon, \label{eqn:confidenceModel}
\end{align}
where \begin{itemize}
\item[$\alpha$] is a vector of fixed effects $(\mu, \tau,  \alpha_T, \alpha_C, \alpha_K)$, where $\mu$ an average baseline confidence level, $\tau$ is the effect of time taken to respond on a participant's confidence, $\alpha_T$ and $\alpha_C$ are the effects of the standard error around trend lines $s_T \in \{0.25, 0.35, 0.45\}$ and clusters $s_C \in \{0.2, 0.25, 0.3, 0.35\}$, and $\alpha_K$ is the effect of the number of clusters $K \in \{3, 5\}$,
\item[$\beta_i$] describe plot types,
\item[$\gamma_j$] $\overset{iid}{\sim} N\left(0, \sigma^2_{\text{data}}\right)$,  random effect for dataset specific characteristics,
\item[$\eta_k$] $\overset{iid}{\sim} N\left(0, \sigma^2_{\text{participant}}\right)$, random effect for participant characteristics,
\item[$\epsilon_{ijk}$] $\overset{iid}{\sim}  N\left(0, \sigma^2_e\right)$, error associated with a single trial evaluation.
\end{itemize}
We also assume that random effects for dataset and participant are orthogonal. 

The approximation of confidence level (which is a bounded, discrete variable) is far from perfect, but the results are very interpretable.

Table~\ref{tab:conf} gives an overview of the parameters and estimates of the model. The longer a participant needs to evaluate a lineup, the lower on average will be the value of confidence reported along with it. Similarly, an increase in lineup difficulty (as controlled by increased values of $s_C$ and $s_T$) goes hand in hand with a significant decrease in confidence. If neither one or both of the two targets were identified, the reported confidence level is significantly lower than if one of the two targets was identified\protect\footnote{The decrease in confidence when both targets are identified may be due to the additional complexity of dual-target search \citep{fleck2010generalized, cain2011anticipatory, adamo2015targets}}. Aesthetics in general did not have a significant effect on confidence levels. However, individual aesthetics did lead to a significant increase in confidence: any plot showing ellipses increases the level of confidence on average by about 0.1. 

% \newdo{I don't  agree with the conclusion below: the dependent variable is not accuracy, but self-reported confidence. For speed-accuracy we need to fit a different model -- model gol.5 does attempt this, and shows that we do, indeed have a highly significant speed-accuracy tradeoff. }

\newtext{These results suggest that the speed of evaluation is not significantly contributing to shifting the balance between selecting one target over the other. 
}
<<modconfidence, echo=FALSE>>=
conf <- lmer(data=modeldata, conf_level~log(as.numeric(trial.time))+plottype+simpleoutcome+sdline_new+sdgroup_new+k_new+(1|dataset)+(1|ip_address))

pconf <- cftest(conf)
table <- data.frame(summary(conf)$coefficients)
table$pvalues <- pconf$test$pvalues
@

% xtable(table, digits=c(0,3,3,3, 3), display=c("s","f","f", "f","e"))
\begin{table}[htbp]
\centering
\begin{tabular}{rrrrr}
  \hline
Parameter & Estimate & Std.\ Error & $z$-value & $P$-value \\ 
  \hline
(Intercept) & 5.800 & 0.186 & 31.190 & $< 0.0001$ \\ 
  $\tau$ & -0.253 & 0.016 & -16.116 & $< 0.0001$ \\ 
  $\alpha_T$ & -0.583 & 0.205 & -2.839 & 0.0045 \\ 
  $\alpha_C$ & -1.921 & 0.405 & -4.748 & $< 0.0001$ \\ 
  $\alpha_K$ & -0.068 & 0.019 & -3.525 & 0.0004 \\ [3pt]
  {\bf Outcome} \hfill   Shape & -0.010 & 0.034 & -0.280 & 0.7793 \\ 
  Plain & 0.000 & --- & --- & ---\\
  Color & 0.038 & 0.034 & 1.119 & 0.2630 \\ 
  Trend & 0.049 & 0.034 & 1.434 & 0.1514 \\ 
  Color + Shape & 0.018 & 0.034 & 0.521 & 0.6023 \\ 
  Color + Trend & 0.049 & 0.034 & 1.449 & 0.1474 \\ 
  Trend + Error & 0.049 & 0.034 & 1.429 & 0.1530 \\ 
  Color + Ellipse & 0.063 & 0.034 & 1.834 & 0.0666 \\ 
  Color + Shape + Ellipse & 0.101 & 0.034 & 2.935 & 0.0033 \\ 
  Color + Ellipse + Trend + Error & 0.101 & 0.034 & 2.959 & 0.0031 \\ [3pt]
  {\bf Outcome} \hfill Trend & 0.000 & --- & --- & --- \\ 
  Cluster & -0.022 & 0.023 & -0.989 & 0.3225 \\ 
  Neither & -0.228 & 0.028 & -8.187 & $< 0.0001$ \\
  Both & -0.231 & 0.103 & -2.247 & 0.0246 \\ 
   \hline
\end{tabular}
\caption{\label{tab:conf}Parameters and estimates for the model of participants' confidence.  }
\end{table}




\subsection{Faceoff Model}\label{sec:faceoff}
Figures~\ref{fig:outcome-parms} and~\ref{fig:outcome-parms-2} show the proportion of outcomes for either the cluster target, the trend target, both or none of them. Overall, cluster targets are picked more often than trend targets. For very small residual errors around the line fit and large within-cluster errors, the number of line target picks are highest. As the standard error around the trend line increases, the number of times the corresponding target is picked, is decreasing. Similarly, an increase in within-cluster error leads to a decrease of the number of cluster target picks. 
The effect of the different plot types is consistent across different parameter settings (the order of plot designs is given by the marginal effects as estimated in the faceoff model. Numerical estimates can be found in Table~\ref{tab:results}). The effect of plot types is most pronounced, when the ambiguity between the two targets is strong, i.e.\ close to a 50:50 decision between the targets. In those cases the additional aesthetics tip the balance in favor of one target over the other.
\begin{figure}
<<complex,echo=FALSE, fig.width=8, fig.height=10, out.width='\\textwidth'>>=
modeldata$simpleoutcome <- factor(modeldata$simpleoutcome, levels=c("trend", "both", "cluster", "neither"))
modeldata$label <- factor(modeldata$label, levels=gvl.fixef$label[order(gvl.fixef$OR)])
modeldata$st <- factor(sprintf("sigma[T]==%.2f", modeldata$sdline_new))
modeldata$sc <- factor(sprintf("sigma[C]==%.2f", modeldata$sdgroup_new))
modeldata$sc <- factor(modeldata$sc, levels=rev(levels(modeldata$sc)))
ggplot(data=subset(modeldata, k_new==3)) +
  geom_bar(aes(x=label, fill=simpleoutcome), position="fill") +
  facet_grid(st+sc~., 
             labeller=label_parsed) +
  scale_fill_manual("Outcome", values=c("darkorange", "grey40", "steelblue", "grey90")) + ylab("") + theme_bw() +
  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), 
        axis.title=element_blank(), 
        axis.text.x=element_text(angle=330, hjust=0, vjust=1),
        legend.position="bottom", 
        strip.text.y = element_text(angle=0))
@
\caption{\label{fig:outcome-parms}Outcome by plot type and parameter setting for lineups with trend and cluster targets. The cluster target consists of $K=3$ clusters.}
\end{figure}

\begin{figure}
<<complex-2, dependson='complex', echo=FALSE, fig.width=8, fig.height=10, out.width='\\textwidth'>>=
ggplot(data=subset(modeldata, k_new==5)) +
  geom_bar(aes(x=label, fill=simpleoutcome), position="fill") +
  facet_grid(st+sc~., 
             labeller=label_parsed) +
  scale_fill_manual("Outcome", values=c("darkorange", "grey40", "steelblue", "grey90")) + ylab("") + theme_bw() +
  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), 
        axis.title=element_blank(), 
        axis.text.x=element_text(angle=330, hjust=0, vjust=1),
        legend.position="bottom", 
        strip.text.y = element_text(angle=0))
@
\caption{Outcome by plot type and parameter setting for lineups with trend and cluster targets. The cluster target consists of $K=5$ clusters.\label{fig:outcome-parms-2}}
\end{figure}

%' 
%' different alternative, not as messy, but there is also not much to see
%' <<complex-3, dependson='complex', echo=FALSE, fig.width=8, fig.height=10, out.width='\\textwidth'>>=
%' modeldata <- modeldata %>% group_by(k_new, sdline_new, sdgroup_new) %>% mutate(
%'   rep = as.numeric(dataset) - min(as.numeric(dataset)) + 1
%' )
%' ggplot(data=subset(modeldata, k_new==5)) +
%'   geom_bar(aes(x=label, fill=simpleoutcome), position="fill") +
%'   facet_grid(st+sc~rep, labeller=label_parsed, space="free", scales="free") + 
%'   scale_fill_manual("Outcome", values=c("darkorange", "grey40", "steelblue", "grey90")) + ylab("") + theme_bw() +
%'   theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), 
%'         axis.title=element_blank(), 
%'         axis.text.x=element_text(angle=330, hjust=0, vjust=1),
%'         legend.position="bottom", 
%'         strip.text.y = element_text(angle=0))
%' @
%' 

% library(xtable)
% xtable(gvl.all[c(1:4,14,5:13),c(3,1,2)], digits=3)
% latex table generated in R 3.2.2 by xtable 1.8-0 package
% Tue Jan 12 15:17:45 2016
\begin{table}[htbp]
\centering
\begin{tabular}{rrrr}
  \hline
\bf Parameter & \bf Log Odds Ratio & \bf 95\% Lower & \bf 95\% Upper \\ 
  \hline
Intercept & 1.018 & -1.615 & 3.651 \\ 
$\alpha_T$ & 16.254 & 13.276 & 19.231 \\ 
$\alpha_C$ & -16.038 & -21.935 & -10.140 \\ 
$\alpha_K$ & -0.281 & -0.563 & 0.001 \\[3pt]
{\bf Plot Type} \hfill  Trend + Error & -0.650 & -0.877 & -0.423 \\ 
  Color + Ellipse + Trend + Error & -0.515 & -0.751 & -0.279 \\ 
  Plain & 0.000 & --- & --- \\ 
  Trend & 0.130 & -0.101 & 0.361 \\ 
  Color & 0.271 & 0.032 & 0.509 \\ 
  Shape & 0.277 & 0.043 & 0.510 \\ 
  Color + Shape & 0.314 & 0.076 & 0.551 \\ 
  Color + Ellipse & 0.459 & 0.207 & 0.711 \\ 
  Color + Trend & 0.459 & 0.219 & 0.700 \\ 
  Color + Shape + Ellipse & 0.573 & 0.317 & 0.830 \\ 
   \hline
\end{tabular}
\caption{\label{tab:results} Odds Ratios of picking the cluster target over the trend target (with the plain plot type as a baseline). The last two columns are 95\% confidence intervals. Within the plain plots, the odds of choosing the cluster target over the trend target is about 2:1. % precisely 711:359  
}
\end{table}

<<tabplus, echo=FALSE, warning=FALSE>>=
aov <- anova(gvl.4, gvl.5)
aov2 <- anova(gvl.4, gvl.6)
aov.7 <- anova(gvl.4, gvl.7)
aov.7b <- anova(gvl.7, gvl.7b)
aov.8 <- anova(gvl.4, gvl.8) # gini 
aov.9 <- anova(gvl.4, gvl.9) # factor(no.ellipse) 
@
Response time (composed of log(response time) and effect of first trial) does not have a significant effect on the decision between cluster and trend target ($\chi_2^2$=\Sexpr{round(aov$Chisq[2],1)}, $P$-value=\Sexpr{round(aov$`Pr(>Chisq)`[2],4)}). 
Nor does the confidence level of participants ($\chi_5^2$=\Sexpr{round(aov2$Chisq[2],1)}, $P$-value=\Sexpr{round(aov2$`Pr(>Chisq)`[2],4)}). 

What does have a significant effect on the balance between cluster target and trend target, is the absence of one of the ellipses in one of the panels of the lineup: a single missing ellipse cuts the probability that the cluster target is selected in less than half (44.6\%; $\chi_1^2$=\Sexpr{round(aov.7$Chisq[2],1)}, $P$-value=\Sexpr{round(aov.7$`Pr(>Chisq)`[2],4)}). 
We also find a significant effect if we additionally take the two-way interaction between a single missing ellipse and individual plot types into account ($\chi_9^2$=\Sexpr{round(aov.7b$Chisq[2],1)}, $P$-value=\Sexpr{round(aov.7b$`Pr(>Chisq)`[2],4)}).
\newtext{These effects are summarised in Figure~\ref{fig:gvl7b}.}
\begin{figure}
<<gols.7b, echo=FALSE, fig.width=7, fig.height=4, out.width='0.8\\textwidth'>>=

df7 <- data.frame(expand.grid(
  plottype =levels(modeldata$plottype),
  sdline_new = 0.275,
  sdgroup_new = 0.25,
  k_new=5,
  one.ellipse=c(TRUE, FALSE)
))

df7plus <- data.frame(expand.grid(
  data_name=unique(faceoff$data_name),
  plottype =levels(modeldata$plottype),
  sdline_new = 0.275,
  sdgroup_new = 0.25,
  k_new=5,
  one.ellipse=c(TRUE, FALSE)
))
df7$pred4 <- predict(gvl.4, newdata=df7, re.form=~0, type="response")
df7$pred7 <- predict(gvl.7, newdata=df7, re.form=~0, type="response")
df7$pred7b <- predict(gvl.7b, newdata=df7, re.form=~0, type="response")
df7 <- merge(df7, unique(modeldata[,c("plottype", "label")]), by="plottype")

rf7 <- data.frame(ranef(gvl.7b)$data_name)
rf7$data_name <- row.names(rf7)
names(rf7)[1] <- "rdata"
df7plus$pred7b <- predict(gvl.7b, newdata=df7plus, re.form=~0, type="link")
df7plus <- merge(df7plus, rf7, by="data_name")
df7plus$predlink <- with(df7plus, pred7b+rdata)
df7plus$predprob <- with(df7plus, exp(predlink)/(1+exp(predlink)) )
df7plus <- merge(df7plus, unique(modeldata[,c("plottype", "label")]), by="plottype")

df7$plottype <- reorder(df7$plottype, df7$pred4)
qplot(pred4, label, data=df7, shape=I(3)) + 
#  geom_point(aes(x=predprob, colour=one.ellipse, shape=one.ellipse), alpha=0.25, size=1.5, data=df7plus) +
  geom_point(aes(x=pred7b, colour=one.ellipse, shape=one.ellipse), size=3) +
  theme_bw() + ylab("") +
  theme(legend.position="bottom") + 
  scale_colour_manual("One ellipse missing", values=c("steelblue", "darkorange")) +
  scale_shape("One ellipse missing") +
  xlab("Probability to pick cluster target\n(given one of the targets was picked)") 
@
\caption{\label{fig:gvl7b} Overview of the probability to pick the cluster target given the different plot types. $s_C$ and $s_T$ are set to 0.25 and  0.275, respectively, and $K=3$ is assumed. The plus symbols indicate probabilities from the base model (\ref{eqn:faceoffModel}), the filled triangle and circle represent predicted probabilities under a model including the two-way interaction between a single missing ellipse and plot types. The Trend plots and Shapes are the least affected by the imbalance in groups, while the Color plot shows huge differences in the predicted probability.}
\end{figure}

\end{appendix}
\end{document}