\documentclass[10pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames, table]{xcolor}
\graphicspath{{figure/}}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage[colorinlistoftodos]{todonotes}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\done}[2][inline]{\todo[color=SpringGreen, #1]{#2}}  % for todos that have been seen and dealt with
\newcommand{\meh}[2][inline]{\todo[color=White, #1]{#2}}   % for todos that may no longer be relevant 
\newcommand{\comment}[2][inline]{\todo[color=SkyBlue, #1]{#2}} % for comments that may not be "to-do"s
%\newcommand{\mcomment}[1]{\todo[color=SkyBlue]{#1}} % for margin comments
\newcommand{\newtext}[1]{\todo[inline, color=White]{ \color{OliveGreen}{#1}}} % new text - not necessarily something to be done
\newcommand{\newdo}[1]{\todo[inline, color=Lime]{#1}} % new to do item
%
%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

\title{Group beats Trend!? \\Testing feature hierarchy in statistical graphics}
\author{Susan VanderPlas, Heike Hofmann\thanks{Department of Statistics and Statistical Laboratory, Iowa State University}}
\begin{document}
\maketitle
\begin{abstract}abstract goes here
\end{abstract}
<<setup, fig.keep='all', cache=FALSE, echo=FALSE, eval=TRUE, include=FALSE>>=
rm(list=ls())
options(replace.assign=TRUE,width=70)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE)

library(stringr)
library(reshape2)
library(plyr)
library(dplyr)
library(magrittr)

library(ggplot2)
library(grid)
suppressMessages(library(gridExtra))

library(nullabor)
library(digest)
library(Cairo)

library(lme4)

library(xtable)

source("../../Code/MixtureLineups.R")
source("../../Code/theme_lineup.R")
@


\section{Introduction and background}
\comment{Discussion of pre-attentive visual features \citep{healey:preattentive} - with a focus on hierarchy of pre-attentive features: color trumps shape - do we also see this in our results, and if so, by how much?}

\todo{Soft introduction}

Our understanding of the perception of statistical charts is informed by several levels of research. Cognitive psychologists and neuroscientists often focus on pre-attentive perception, which occurs automatically in the first 200 ms of exposure to a visual stimulus. Gestalt psychologists focus instead on perception as a holistic experience; they consider the heuristics used to transform visual stimuli into useful, coherent information. Finally, statistical graphics researchers apply low-level perceptual research and gestalt ideas to statistical charts, using tools such as lineups \citep{Buja:2009hp, mahbub:2013, wickham:2010,Hofmann:2012ts} to determine which graphics are accurately perceived and communicate information effectively. 

Research into the preattentive stage of visual perception provides us with some information about the temporal hierarchy of graphical feature processing. Color, line orientation, and shape are processed preattentively; that is, within 200 ms, it is possible to identify a single target in a field of distractors, if the target differs with respect to color or shape \citep{goldstein2009encyclopedia}. 
\citet{healey1999large} extended this work, demonstrating that certain features of three-dimensional data displays are processed preattentively, but that neither target identification nor three-dimensional data processing always translate into faster or more accurate inference about the data displayed, particularly when participants must integrate several preattentive features to understand the data. 

\newdo{add citations and text describing preattentive interference.}
\newdo{add a bit of transition between preattentive stages of perception and gestalt rules.}

Gestalt rules of perception also impact statistical graphics. These rules describe the way we organize visual input, focusing on the holistic experience rather than the individual perceptual features. For example, rather than perceiving four legs, a tail, two eyes, two ears, and a nose, we perceive a dog. The rules of perceptual grouping or organization, as stated in \citet{goldstein2009encyclopedia} are:
\begin{itemize}
\item Proximity: two elements which are close together are more likely to belong to a single unit.
\item Similarity: the more similar two elements are, the more likely they belong to a single unit.
\item Common fate: two elements moving together likely belong to a single unit.
\item Good continuation: two elements which blend together smoothly likely belong to one unit.
\item Closure: elements which can be assembled into closed or convex objects likely belong together. 
\item Common region: elements contained within a common region likely belong together. 
\item Connectedness: elements physically connected to each other are more likely to belong together.
\end{itemize}

\begin{figure}
<<gestalt1, echo =FALSE, out.width='0.32\\textwidth', fig.width=3, fig.height=3, fig.show='hold'>>=
x1 <- rnorm(25, mean=4, sd=0.5)
x2 <- rnorm(25, mean=0, sd=0.5)
y1 <- rnorm(25, mean=1, sd=0.5)
y2 <- rnorm(25, mean=2, sd=0.5)
qplot(c(x1,x2), c(y1,y2)) + theme_bw() + xlab("x") + ylab("y")

x <- rnorm(60)
y <- rnorm(60)
group <- rep(1:4, length=60)
qplot(x,y, color=factor(group%/%2), shape=factor(group%/%2)) + theme_bw() + xlab("x") + ylab("y") + scale_color_brewer(palette="Set1") + theme(legend.position="none")

x1 <- runif(50,-.75,1.25)
y1 <- x1^2 - x1 + rnorm(50, sd=0.1)
x2 <- runif(25,-.75,1.25)
y2 <- x2 + rnorm(25, sd=0.1)

qplot(c(x1,x2),c(y1,y2)) + theme_bw() + xlab("x") + ylab("y") 
@
\caption{\label{fig:gestalt} \emph{Proximity} renders the fifty points of the first scatterplot as two distinct (and equal-sized) groups. Shapes and colors create different groups of points in the middle scatterplot, invoking the Gestalt principle of \emph{Similarity}. \emph{Good Continuation} renders the points in the scatterplot on the right hand side into two groups of points on curves: one a straight line with an upward slope, the other a curve that initially decreases and at the end of the range shows an uptick.} 
\end{figure}

The plots in figure \ref{fig:gestalt} demonstrate several of the gestalt principles which combine to order our perceptual experience from the top down. These laws help to order our perception of charts as well: points which are colored or shaped the same are perceived as belonging to a group (similarity), points within a bounding interval or ellipse are perceived as belonging to the same group (common region), and regression lines with confidence intervals are perceived as single units (connectedness, closure, and/or common region). 
\newdo{clarify next sentence}
The use of physical location, color, and shape to organize graphical units mentally utilizes both preattentive processing and higher-order gestalt schemas, identifying and grouping similar graphical features and simultaneously directing attention to graphical features which stand alone. 

Research on preattentive perception is important because features that are perceived preattentively do not require as much mental effort to process from raw visual stimuli; theoretically, subsequent top-down gestalt heuristics can be applied to such stimuli more quickly. 

\comment{We should also look at the time to response -- it would be interesting, to see if the conflicting stimuli need more time to come to a decision. It's obviously not milliseconds that we measure, but it might still be informative. (We would need to exclude everybody's first attempt). }


This study is designed to understand the hierarchy of gestalt principles in perception of statistical graphics. We utilize information from previous studies \citep{heer:2014, robinson:03} concerning the hierarchy of preattentive feature perception in order to maximize the effect of preattentive feature differences. 

\comment{might be useful to have a small diagram describing the perceptual process (with preattentive processing way at the top and gestalt heuristic processing in the middle, with "cognitive effort" at the bottom). Not sure if it's necessary, though. HH: good idea, let's see how much space we'll have. }

\subsection{Statistical Lineups}

\todo[inline]{Intro to lineups \citep{Buja:2009hp, mahbub:2013, wickham:2010, Hofmann:2012ts}.}
\comment{Describe the lineup protocol, including basic statistics. Link to the psychological "target and distractors" approach, which can be used to justify the addition of a second target, even with the PITA of the statistical complications. }


In this study, we modify the lineup protocol by introducing a second target to each lineup. The two targets represent two different, competing signals; the participant's choice then demonstrates empirically which signal is more salient.
\comment{We should add that we allow users to select multiple targets, so that we don't get them into the position of having to guess between targets.}
By tracking the proportion of observers choosing either target plot (a measure of overall lineup difficulty) as well as which proportion of observers choose one target over the other target, we can determine the relative strength of the two competing signals amid a field of distractors. At this level, signal strength is determined by the data and generating model; we are measuring the ``power" (in a statistical sense) of the human perceptual system. 

Using this testing framework, we can apply different aesthetics, such as color and shape, as well as plot objects which display statistical calculations, such as trend lines and bounding ellipses. These additional plot layers, discussed in more detail in the next section, are designed to emphasize one of the two competing targets and affect the overall visual signal of the target plot relative to the null plots. We expect that in a situation similar to the third plot of figure \ref{fig:gestalt}, the addition of two trend lines would emphasize the ``good continuation" of points in the plot, producing a stronger visual signal, even though the underlying data has not changed. Similarly, the grouping effect in the first plot in the figure would be enhanced if the points in each group were colored differently, as the proximity heuristic would be supplemented by similarity. In plots that are ambiguous, containing some clustering of points as well as a linear relationship between $x$ and $y$, additional aesthetic cues may ``tip the balance" in favor of recognizing one type of signal.

\comment{beautiful!!}
% A further extension of this testing framework are the use of color (in a qualitative color scheme), the use of shapes, and additional density lines - we anticipate that all of these features are going to emphasize the clustering component. 
% On the other hand, regression lines should emphasize any linear trends in the data.

This study is designed to inform our understanding of the perceptual implications of these additional aesthetics, in order to provide guidelines for the creation of data displays which provide visual cues consistent with gestalt heuristics and preattentive perceptual preferences. %awesome alliteration, right? HH: :)
The next section discusses the particulars of the experimental design, including the data generation model, plot aesthetics, selection of color and shape palettes, and other important considerations.


\section{Experimental Design}

In this section, we discuss the generating data models for the two types of signal plots and the null plots, the selection of plot aesthetic combinations and aesthetic values, and the design and execution of the experiment.


\comment{I know this will have to be rearranged, expanded, and transitions between sections will need to be added, but I want to get the paragraphs out.}

\subsection{Data Generation}

Lineups require a single ``target" data set (which we are expanding to two competing ``target" data sets), and a method for generating null plots. When utilizing real data for target plots, null plots are often generated through bootstrap sampling, but this introduces some dependencies between target and null plots which complicate the statistical analysis of the results.

\newdo{add citations}

When possible, it is desireable to generate true null plots, which are generated from the null model and do not depend on the data used in the target plot. 
This experiment will measure two competing gestalt heuristics, proximity and good continuation, using two data-generating models: $M_C$, which generates data with $K$ clusters, and $M_T$, which generates data with a positive correlation between $x$ and $y$. 
True null datasets are created using a mixture model $M_0$ which combines $M_C$ and $M_T$. Both $M_C$ and $M_T$ generate data in the same range of values. 
Additionally, $M_C$ generates clustered data with linear correlations that are within $\rho = (0.25, 0.75)$, similar to the linear relationship between datasets generated by $M_0$, and $M_T$ generates data with clustering similar to $M_0$. These constraints provide some assurance that participants who select a plot with data generated from $M_T$ are doing so because of visual cues indicating a linear trend (rather than a lack of clustering compared to plots with data generated from $M_0$), and participants who select a plot with data generated from $M_C$ are doing so because of visual cues indicating clustering, rather than a lack of a linear relationship relative to plots with data generated from $M_0$. 


\subsubsection{Regression Model $M_T$}

This model has the parameter $\sigma_T$ to reflect the amount of scatter around the trend line. It generates $N$ points $(x_i, y_i), i=1, ..., N$ where $x$ and $y$ have a positive linear relationship. The data generation mechanism is as follows: 

\begin{algorithm}\hfill\newline
  Input Parameters: sample size $N$, $\sigma_T$ standard deviation around the line \\
  Output: $N$ points, in form of vectors $x$ and $y$.
  \begin{enumerate}
    \item Generate $\tilde{x}_i$, $i=1, ..., N$, as a sequence of evenly spaced points from $[-1, 1]$ 
    \item Jitter $\tilde{x}_i$ by adding small uniformly distributed perturbations to each of the values: $x_i = \tilde{x}_i + \eta_i$, $\eta_i \sim \text{Unif}(-z, z)$, $z = 1/5 \cdot 2/(N-1)$
    \item Generate $y_i$: $y_i = a  x_i + e_i$, $e_i \sim N(0, \sigma^2_T)$
    \item Center and scale $x_i$, $y_i$
  \end{enumerate}
\end{algorithm}

We compute the correlation coefficient for all of the plots to assess the amount of linearity in each panel, computed as 
\[
r = 1 - RSS/TSS,
\]
where TSS is the total sum of squares, $TSS = \sum_{i=1}^N (y_i - \bar{y})^2$ and $RSS = \sum_{i=1}^N e_i^2$, the residual sum of squares.
The expected correlation coefficient $\rho$ in this scenario is 
\[
\rho = \frac{\frac{1}{3}}{\frac{1}{3} + \sigma^2_T},
\]
because
$E[RSS] = N\sigma^2_T$ and $E[TSS] = \sum_{i=1}^N E\left[y_i^2\right]$  (as $E[Y] = 0$), where 
$$
E\left[y_i^2\right] = E\left[x_i^2 + e_i^2 + 2 x_ie_i\right] = \frac{1}{3} + \sigma^2_T. 
$$
\begin{figure}[ht]
<<trends, fig.width=8, fig.height=3, out.width='\\textwidth', echo=FALSE, dependson='setup'>>=
sd <- c(0.1, 0.2, 0.3, 0.4)
res <- ldply(sd, function(x) { data.frame(sd.trend=x, sim.line(N=45, sd.trend=x)) })
res$label <- paste("sigma[T] :",res$sd.trend)
qplot(x,y, data=res, pch=I(1)) + facet_grid(facets=.~label, labeller="label_parsed") + theme_bw() + 
  theme(plot.margin=unit(c(0,0,0,0), "cm"))
@
\caption{\label{fig:trends} Set of scatterplots showing one draw each from the trend model $M_T$ for parameter values of  $\sigma_T \in \{0.1, 0.2, 0.3, 0.4\}$.}
\end{figure}

\subsubsection{Cluster Model $M_C$}
We begin by generating $K$ cluster centers on a $K \times K$ grid, then we generate points around selected cluster centers. 
\begin{algorithm}\hfill\newline
  Input Parameters:  $N$ points, $K$ clusters, $\sigma_C$ cluster standard deviation \\
  Output: $N$ points, in form of vectors $x$ and $y$. 
  \begin{enumerate}
    \item Generate cluster centers $(c^x_{i}, c^y_{i})$ for each of the $K$ clusters, $i=1, ..., K$:
      \begin{enumerate}
        \item in form of two vectors $c^{x}$ and $c^y$ of permutations of $\{1, ..., K\}$, such that
        \item the correlation between cluster centers \text{Cor}$(c^{x}, c^{y})$ falls into a range of $[.25, .75]$.
      \end{enumerate}
      \item Center and standardize cluster centers $(c^x, c^y)$:  
      \[
        \tilde{c}^x_{i} = \frac{c^x_{i} - \bar{c}}{s_c} \ \ \text{ and } \ \ \tilde{c}^y_{i} = \frac{c^y_{i} - \bar{c}}{s_c},
      \]
      where $\overline{c} = (K+1)/2$ and $s_c^2 = \frac{K(K+1)}{12}$ for all $i = 1, ..., K$.
    \item For the $K$ clusters, we want to have nearly equal sized groups, but allow some variability. Group sizes are therefore determined as a draw from a multinomial distribution: determine group sizes $g = (g_1, ..., g_K)$, with $N = \sum_{i=1}^K g_i$, for clusters $1, ..., K$ as a random draw 
    \[
    g \sim \text{Multinomial}(K, p) \text{ where } p = \tilde{p}/\sum_{i=1}^K \tilde{p}_i, \text{ for } \tilde{p} \sim N(\frac{1}{K}, \frac{1}{2 K^2}).
    \]
     
    \item Generate points around cluster centers: 
      \begin{enumerate}
        \item $x_i = \tilde{c}^x_{g_i} + e^x_i$, where $e^x_i \sim N(0, \sigma^2_C)$
        \item $y_i = \tilde{c}^y_{g_i} + e^y_i$, where $e^y_i \sim N(0, \sigma^2_C)$
      \end{enumerate}
    \item Center and scale $x_i$, $y_i$
  \end{enumerate}
\end{algorithm} 


As a measure of clustering we use a coefficient to assess the amount of variability within groups, compared to total variability. Note that for the purpose of clustering, variability is measured as the variability in both $x$ and $y$ from a common mean, i.e. we implicitly assume that the values in $x$ and $y$ are on the same scale (which we achieve by scaling in the final step of the generation algorithm).

\begin{figure}[ht]
<<cluster, fig.width=8, fig.height=4.5, out.width='\\textwidth', echo=FALSE,dependson='setup'>>=
sd <- c(0.15, 0.2, 0.25, 0.3)
colors <-  c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", 
             "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf")

res <- ldply(sd, function(x) { data.frame(sd.cluster=x, sim.clusters(K=3, N=45, sd.cluster=x)) })
res$K <- 3
res2 <- ldply(sd, function(x) { data.frame(sd.cluster=x, sim.clusters(K=5, N=75, sd.cluster=x)) })
res2$K <- 5
res <- rbind(res, res2)
suppressMessages(library(ggplot2))
res$label <- paste("sigma[C] :",res$sd.cluster)
res$Klabel <- paste("K :",res$K)
qplot(x,y, data=res, pch=I(1), color=factor(group), shape=factor(group)) + 
  facet_grid(facets=Klabel~label, labeller="label_parsed") + theme_bw() + 
  theme(plot.margin=unit(c(0,0,0,0), "cm"), legend.position="none") + 
  scale_color_manual(values=colors) + theme(aspect.ratio=1)
@
\caption{\label{fig:clusters} Scatterplots of clustering output for different inner cluster spread $\sigma_C$  (left to right) and different number of clusters $K$ (top and bottom).}
\end{figure}

\comment{For the study we used $a=1$, right?}
\subsubsection{Null Model $M_0$}
The generative model for null data is a mixture model $M_0$ that draws $n_c \sim \text{Binomial}(N, \lambda)$ observations from the cluster model, and $n_T = N - n_c$ from the regression model $M_T$. Observations are assigned groups using hierarchical clustering, which creates groups consistent with any structure present in the generated data. This provides a plausible grouping for use in aesthetic and statistics requiring categorical data (color, shape, bounding ellipses). 

\begin{figure}[ht]
<<lambda, fig.width=8, fig.height=3.5, out.width='\\textwidth', echo=FALSE>>=
lambda <- c(0, .25, .5, .75, 1)
colors <-  c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", 
             "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf")

res <- ldply(lambda, function(x) { data.frame(lambda=x, mixture.sim(x, K=3, N=45, sd.trend=.25, sd.cluster=.25)) })
res$K <- 3
res2 <- ldply(lambda, function(x) { data.frame(lambda=x, mixture.sim(x, K=5, N=75, sd.trend=.25, sd.cluster=.2)) })
res2$K <- 5
res <- rbind(res, res2)
res$label <- paste("lambda :",res$lambda)
res$Klabel <- paste("K :",res$K)
qplot(x,y, data=res, pch=I(1), color=factor(group), shape=factor(group)) + 
  facet_grid(facets=Klabel~label, labeller="label_parsed") + theme_bw() + 
  theme(plot.margin=unit(c(0,0,0,0), "cm"), legend.position="none") + 
  scale_color_manual(values=colors) + theme(aspect.ratio=1)
@
\caption{\label{fig:lambda} Scatterplots of data generated from $M_0$ using different values of $\lambda$.}
\end{figure}
Null data in this experiment is generated using $\lambda = 0.5$, that is, each point in a null data set is equally likely to have been generated from $M_C$ and $M_T$. 

\subsubsection{Parameters used in Data Generation}
These models provide the foundation for this experiment; by manipulating cluster standard deviation $\sigma_C$ and regression standard deviation $\sigma_T$ (directly related to correlation strength) for varying numbers of clusters $K=3, 5$, we can systematically control the statistical signal present in the target plots and generate corresponding null plots that are mixtures of the two distributions. For each parameter set $\{K, N, \sigma_C, \sigma_T\}$, as described in table \ref{tab:parameters}, we  generate a lineup dataset consisting of one set drawn from $M_C$, one set drawn from $M_T$, and 18 sets drawn from $M_0$. 
\begin{table}[h]
  \rowcolors{2}{gray!25}{white}
\begin{center}
\begin{tabular}{lll}
\bf Parameter & \bf Description & \bf Choices\\\hline
$K$ & \# Clusters & 3, 5 \\
$N$ & \# Points & $15\cdot K$ \\
$\sigma_T$ & Scatter around trend line &  .15, .25, .35\\
$\sigma_C$ & Scatter around cluster centers & \begin{tabular}{ll} .15, .20, .25 ($K=3$)\\ .20, .25, .30 ($K=5$) \end{tabular}
\\\hline
\end{tabular}
\end{center}
\caption{Parameter settings for generation of lineup datasets. \label{tab:parameters}}
\end{table}
\newtext{
The parameter values were chosen after examining the full parameter space through simulation; results are provided in appendix \ref{app:parametersimulation}. In accordance with the simulation, we identified values of $\sigma_T$ and $\sigma_C$ corresponding to "easy", "medium" and "hard" numerical comparisons between corresponding target data sets and null data sets. 
}

\newtext{Once the lineup dataset is generated, we can plot the lineups and apply aesthetics which emphasize clusters and/or linear relationships in the generated datasets, experimentally determining how these aesthetics change participants' ability to identify each target plot. The next section describes the aesthetic combinations we will use and their anticipated effect on participant responses. 
}

\subsection{Plot Aesthetics}
\newtext{
Gestalt perceptual theory suggests that perceptual features such as shape, color, trend lines, and boundary regions would modify the perception of ambiguous graphs, emphasizing clustered data (in the case of shape, color, and bounding ellipses) or linear relationships (in the case of trend lines and prediction intervals), as demonstrated in figure \ref{fig:gestalt}. For each dataset generated as described in the previous section, we will examine the combinations of plot aesthetics (color, shape) and statistical layers (trend line, boundary ellipses, prediction intervals) shown in table \ref{tab:plotaesthetics} in order to assess the visual impact of these aesthetics on target identification. Examples of these plot aesthetics are shown in figure \ref{fig:plotExamples}.
}
\clearpage

\begin{figure}[ht]
<<samplepics, eval=T, echo=F, fig.width=6, fig.height=6, out.width=".3\\linewidth", fig.show='hold', fig.align='center', warning=FALSE, message=FALSE>>=
# source("../../Code/MixtureLineups.R")
# source("../../Code/theme_lineup.R")
# 
# # Define colors and shapes
# colors <-  c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", 
#              "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf")
# shapes <- c(1,0,3,4,8,5,2,6,-0x25C1, -0x25B7)
# 
# colortm <- read.csv("../../Data/color-perceptual-kernel.csv")
# # colortm[3,4] <- 0
# # colortm[4,3] <- 0
# colortm[8,] <- 0
# colortm[,8] <- 0
# 
# shapetm <- read.csv("../../Data/shape-perceptual-kernel.csv")
# # shapetm[9:10,] <- 0
# # shapetm[, 9:10] <- 0
# shapetm[9,] <- 0
# shapetm[,9] <- 0
# shapetm[10,] <- 0
# shapetm[,10] <- 0
# 
# # Lineup Design
# data.parms <- expand.grid(N=45,
#                           K=3,
#                           sd.trend=.25,
#                           sd.cluster=.2)
# 
# plot.parms <- expand.grid(
#   color = c(0,1),
#   shape = c(0,1),
#   reg = c(0,1),
#   err = c(0,1),
#   ell = c(0,1)
# )[c(
#   1, # control
#   2, 3, # color, shape
#   4, 18, # color + shape, color + ellipse
#   20, # color + shape + ellipse
#   5, 13, # trend, trend + error
#   6, # color + trend
#   30 # color + ellipse + trend + error
#   ),]
# 
# get.aes <- function(r){
#   c("Color", "Shape")[which(as.logical(r[1:2]))]
# }
# 
# get.stats <- function(r){
#   c("Reg. Line", "Error Bands", "Ellipses")[which(as.logical(r[3:5]))]
# }
# 
# data <- data.frame(set=1, gen.data(as.list(data.parms[1,])))
# data.stats <- ddply(data, .(set, .sample), 
#                     function(df){
#                       r2 <- summary(lm(y~x, data=df))
#                       tmp <- summary(aov(lm(y~x+factor(group) + 0, data=df)))
#                       res <- tmp[[1]]$`Mean Sq`
#                       data.frame(.sample=unique(df$.sample), 
#                                  LineRSq = r2$r.squared, 
#                                  Fgroup = round(res[2]/res[3], 2), 
#                                  lineplot=unique(df$target2), 
#                                  groupplot=unique(df$target1))
#                       } )
# answers <- ddply(data.stats, .(set), summarize, lineplot=unique(lineplot), groupplot=unique(groupplot))
# 
# save(colors, shapes, colortm, shapetm, data, answers, data.parms, plot.parms, get.aes, get.stats, file="figure/lineupex/data.Rdata")
load("figure/lineupex/data.Rdata")
d_ply(data, .(set), function(df){
  i <- unique(df$set)
  for(j in 1:nrow(plot.parms)){
    p = gen.plot(df, get.aes(plot.parms[j,]), get.stats(plot.parms[j,]))
#     ggsave(plot = p, 
#            filename = sprintf("figure/lineupex/set-%d-plot-%d.pdf", i, j), 
#            width=6, height=6, units="in", dpi=150)
    print(p)
  }
})
@
\caption{Each of the 10 plot feature combinations tested in this study, with $K=3$, $\sigma_T=.25$ and $\sigma_C=.2$. \label{fig:plotExamples}}
\end{figure}
\clearpage
% 
% \begin{table}[h]
% \begin{center}
% \begin{tabular}{ll}
% Emphasis & Aesthetics \\\hline\hline
% Control & -- \\\hline
% \multirow{3}{*}{Group} & Color, Shape \\
% & Color + Shape, Color + Ellipse\\
% & Color + Shape + Ellipse\\\hline
% \multirow{2}{*}{Trend} & Line \\
% & Line + Error band\\\hline
% \multirow{2}{*}{Conflict} & Color + Trend Line, \\
% & Color + Ellipse + Trend Line + Error band\\\hline\hline
% \end{tabular}
% \end{center}
% \caption{Plot aesthetics and statistical layers which impact perception of statistical plots, according to gestalt theory. \label{tab:plotaesthetics}}
% \end{table}

\begin{table}[h]
\centering
\begin{tabular}{cc|c|c|c|}\cline{3-5}
 & \multicolumn{1}{c}{} & \multicolumn{3}{|c|}{Line Emphasis}\\\cline{2-5}
 & \multicolumn{1}{|c|}{Strength} & \multicolumn{1}{|c|}{0} & \multicolumn{1}{|c|}{1} & \multicolumn{1}{|c|}{2}\\\hline
 % ----- 
  \multicolumn{1}{|c|}{\multirow{8}{*}{\parbox[t]{.1\linewidth}{Cluster Emphasis}}} & 
  \multirow{2}{*}{0} & 
  \multirow{2}{*}{None} & 
  \multirow{2}{*}{Line} & 
  \multirow{2}{*}{Line + Prediction}\\
 % ----- 
  \multicolumn{1}{|c|}{} & & & & \\\cline{2-5} 
 % ----- 
  \multicolumn{1}{|c|}{} & 
  \multicolumn{1}{|c|}{\multirow{2}{*}{1}} & 
  Color & 
  \multirow{2}{*}{Color + Line} &
  \\\cline{3-3}
 % ----- 
  \multicolumn{1}{|c|}{} & 
  & 
  Shape &
  &
  \\ \cline{2-5}
 % ----- 
  \multicolumn{1}{|c|}{} & 
  \multicolumn{1}{|c|}{\multirow{2}{*}{2}} & 
  Color + Shape  & 
  \multirow{2}{*}{} & 
  \multirow{2}{*}{\parbox[t][-.2em][c]{.2\linewidth}{Color + Ellipse + Line + Prediction}} 
  \\ \cline{3-3}
 % ----- 
  \multicolumn{1}{|c|}{} & 
  &
  Color + Ellipse & 
  & 
  \\ \cline{2-5}
 % ----- 
  \multicolumn{1}{|c|}{} & 
  \multicolumn{1}{|c|}{\multirow{2}{*}{3}} & 
  \multirow{2}{*}{\parbox[t][-.2em][c]{.15\linewidth}{Color + Shape + Ellipse}} &
  &
  \\
 % ----- 
  \multicolumn{1}{|c|}{} & 
  & 
  &
  &
  \\ \hline
\end{tabular}
\caption{Plot aesthetics and statistical layers which impact perception of statistical plots, according to gestalt theory. \label{tab:plotaesthetics}}
\end{table}

\newtext{
We expect that relative to a plot with no extra aesthetics or statistical layers, the addition of color, shape, and 95\% boundary ellipses will increase the probability of a participant selecting the target plot with data generated from $M_C$, the cluster model, and that the addition of these aesthetics will decrease the probability of a participant selecting the target plot with data generated from $M_T$, the linear model. 
}

\newtext{
Similarly, we expect that relative to a plot with no extra aesthetics or statistical layers, the addition of a trend line and prediction interval will increase the probability of a participant selecting the target plot with data generated from $M_T$, the linear model, and decrease the probability of a participant selecting the target plot with data generated from $M_C$, the cluster model.
}

\subsection{Experimental Design}
\newtext{
The study is designed hierarchically, as a factorial experiment for combinations of $\sigma_C$, $\sigma_T$, and $K$, with three replicates at each parameter combination. These parameters are used to generate lineup datasets which serve as blocks for the plot aesthetic level of the experiment; each dataset is rendered with every combination of aesthetics described in table \ref{tab:plotaesthetics}. Participants are assigned to generated plots according to an augmented balanced incomplete block scheme: each participant is asked to evaluate 10 plots, which consist of one plot at each combination of $\sigma_C$ and $\sigma_T$, randomized across levels of $K$, with one additional plot providing replication of one level of $\sigma_C\times\sigma_T$. Each of a participant's 10 plots will present a different aesthetic combination.
}
\newdo{Need to find some graphic/table which makes this a bit more clear.}

\subsection{Color and Shape Palettes}
Colors and shapes used in this study were selected in order to maximize preattentive feature differentiation. \citet{heer:2014} provide sets of 10 colors and 10 shapes, with corresponding distance matrices, determined by user studies. Using these perceptual kernels for shape and color, we identified sets of 3 and 5 colors and shapes which maximize the sum of pairwise differences, subject to certain constraints imposed by software and accessibility concerns. 

\begin{figure}
<<color-palette, dev='cairo_pdf', echo=FALSE, fig.width=5, fig.height=1, out.width='.5\\linewidth',dependson='setup'>>=
colors <- c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", 
            "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf")
qplot(x=1:10, y=0, color=colors, size=I(5)) + scale_color_identity() + theme_lineup()
@
\caption{Colors in \citet{heer:2014}. This study removed grey from the palette to make the experiment more inclusive of participants with colorblindness.\label{fig:colors}}
\end{figure}

\newtext{The color palette used in \citet{heer:2014} and shown in figure \ref{fig:colors} is derived from colors available in Tableau visualization software.}
In order to produce experimental stimuli accessible to the approximately 4\% of the population with red-green color deficiency \citep{colorvision}, we removed the grey hue from the palette. This modification produced maximally different color combinations which did not include red-green combinations, while also removing a color (grey) which is difficult to distinguish for those with color deficiency.  

% Beyond that, we modified some color pairings observed in the general population to reflect individual's abilities: the red-green color pair is one of the pairs of the most distinct color pairings in the general population (XXX exact value?), but obviously is a poor choice for the approximately XXX\% of population with a red-green color vision deficiency.

\begin{figure}
<<shape-palette, dev='cairo_pdf', echo=FALSE, fig.width=5, fig.height=1, out.width='.5\\linewidth', dependson='setup',message=F, warning=F>>=
shapes <- c(1,0,3,4,8,5,2,6,-0x25C1, -0x25B7)

qplot(x=1:10, y=0, shape=shapes, size=I(5)) + scale_shape_identity() + theme_lineup()
@
\caption{Shapes in \citet{heer:2014}. In order to control for varying point size due to Unicode vs. non-Unicode characters, the last two shapes were removed.\label{fig:shapes}}
\end{figure}

\newtext{Software compatibility issues led us to exclude two shapes used in \citet{heer:2014} and shown in figure \ref{fig:shapes}. The left and right triangle shapes (available only in unicode within R) were excluded due to size differences between unicode and non-unicode shapes. After optimization over the sum of all pairwise distances, the maximally different shape sequences for the 3 and 5 group datasets also conform to the guidelines in \citet{robinson:03}: for $K=3$ the shapes are from Robinson's group 1, 2, and 9, for $K=5$ the shapes are from groups 1, 2, 3, 9, and 10. Robinson's groups are designed so that shapes in different groups show differences in preattentive properties; that is, they are easily distinguishable. In addition, all shapes are non-filled shapes, which means that they are consistent with one of the simplest solutions to overplotting of points in the tradition of \citet{tukey, cleveland:85} and \citet{few}. For this reason we abstained from the additional use of alpha-blending of points to diminish the effect of overplotting in the plots.}

\subsection{Hypotheses}
\newtext{
The primary purpose of this study is to understand how visual aesthetics affect signal detection in the presence of competing signals. We expect that plot modifications which emphasize similarity and proximity, such as color, shape, and 95\% bounding ellipses, will increase the probability of detecting the clustering relationship, while plot modifications which emphasize good continuation, such as trend lines and prediction intervals, will increase the probability of detecting the linear relationship. 
}

\newtext{
A secondary purpose of the study is to relate signal strength (as determined by dataset parameters $\sigma_C$, $\sigma_T$, and $K$) to signal detection in a visualization by a human observer.
}

\subsection{Participant Recruitment}
\newdo{describe amazon turk, participant instructions, screening procedures, etc.}

\section{Results}
<<results-setup,echo=F,include=F>>=
lineups <- read.csv("../../Images/Turk16/data-picture-details.csv", stringsAsFactors=FALSE)
lineups$pic_id_old <- lineups$pic_id
lineups$pic_id <- 1:nrow(lineups)

userdata <- read.csv("../../Data/turk16_results.csv", stringsAsFactors=FALSE)
userdata$response.id <- 1:nrow(userdata)
# table(userdata$ip_address, userdata$nick_name)

tmp <- merge(userdata[!is.na(userdata$pic_id),], lineups[,c("pic_id", "sample_size", "test_param", "param_value", "p_value", "obs_plot_location")], all.x=T, all.y=F)
tmp$k <- as.numeric(substr(tmp$param_value, 3, 3))
tmp$sd.line <- as.numeric(substr(tmp$param_value, 12, 15))
tmp$sd.cluster <- as.numeric(substr(tmp$param_value, 25, 28))

correct.ans <- function(x,y){
  x1 <- as.numeric(str_trim(unlist(str_split(x, ","))))
  answers <- str_trim(unlist(str_split(y, ",")))
  lineplot <- as.numeric(answers[1])
  groupplot <- as.numeric(answers[2])
  c(line.correct=lineplot%in%x1, group.correct=groupplot%in%x1, n.answers=length(x1), both.correct = lineplot%in%x1 & groupplot%in%x1, neither=!(lineplot%in%x1 | groupplot%in%x1))
}

useranswers <- ddply(tmp, .(response.id), function(df) correct.ans(df$response_no, df$obs_plot_location))
useranswers <- merge(useranswers, tmp)
useranswers$plottype <- gsub("turk16-", "", useranswers$test_param)
useranswers$plottype <- factor(useranswers$plottype, levels=c("plain", "trend", "color", "shape", "colorShape", "colorEllipse", "colorTrend",  "trendError", "colorShapeEllipse", "colorEllipseTrendError"))
useranswers$sd.cluster <- factor(useranswers$sd.cluster)
useranswers$sd.line <- factor(useranswers$sd.line)
useranswers$k <- factor(useranswers$k)
useranswers <- ddply(useranswers, .(param_value, test_param), transform, param_idx=as.numeric(factor(pic_id)))

modeldata <- useranswers[,c(1, 4, 7:25, 2, 3, 5, 6)]
modeldata$outcome <- paste(c("", "line")[1+as.numeric(modeldata$line.correct==1)], c("", "group")[1+as.numeric(modeldata$group.correct==1)], c("", "neither")[1+as.numeric(modeldata$neither==1)], sep="")
modeldata$outcome[modeldata$both.correct==1] <- "both"

modeldata <- merge(modeldata, lineups[,c("pic_id", "data_name", "param_value")], all.x=T, all.y=T)
modeldata$dataset <- factor(str_extract(modeldata$data_name, "set-\\d{1,3}") %>% str_replace("set-", "") %>% as.numeric)
modeldata$individualID <- factor(sprintf("%s-%s", modeldata$ip_address, modeldata$nick_name))
modeldata$k <- factor(modeldata$k, levels=c(3, 5))
modeldata$parameter.value <- factor(gsub("set-\\d{1,3}-", "", modeldata$data_name))
@

\subsection{General results}
\newdo{demographic information, $N$ people participated, completing on average $M$ plots, etc., overall accuracy rates by individual and plot type}

\newtext{We will first consider the effect of plot aesthetics on target selection for each target type (separately), and then we will analyze the effect of parameter values on participant performance.}
\subsection{Linear Target Model}
\newtext{We will model the probability of selecting the linear target plot as a function of plot type, with random effects for dataset (which encompasses parameter effects) and participant (accounting for variation in individual skill level). For plot type $i = 1, ..., 10$ displaying dataset $j=1, ..., 54$ by participant $k=1, ..., P$, 
}
\begin{align}
P(\text{success}) & =  \left(e^\theta\right)/\left(1+e^\theta\right)\label{eqn:linearModel}\\
\theta & =  \textbf{X}\beta + \textbf{J}\gamma + \textbf{K}\eta + \epsilon \tag{a}\\
\text{where } \beta_i & \hphantom{\sim} \text{describe the effect of specific plot aesthetics}\nonumber\\
\hphantom{where } \gamma_j & \overset{iid}{\sim} N\left(0, \sigma^2_{\text{data}}\right)\nonumber\\
\hphantom{where } \eta_k & \overset{iid}{\sim} N\left(0, \sigma^2_{\text{participant}}\right)\nonumber\\
\text{and } \epsilon_{ijk} & \overset{iid}{\sim}  N\left(0, \sigma^2\right)\nonumber
\end{align}
\newtext{
We note that any variance due to parameters $K$, $\sigma_T$, and $\sigma_C$ is contained within $\sigma^2_{\text{data}}$ and can be examined using a subsequent model. 
}

<<line-model, echo=FALSE, include=FALSE, dependson='results-setup', fig.width=8, fig.height=4>>=
line.model <- glmer(line.correct~ plottype + (1|individualID) + (1|dataset), 
                    data = modeldata, 
                    family = binomial(link="logit"), 
                    control=glmerControl(optimizer="bobyqa"))
line.fixef <- as.data.frame(summary(line.model)$coefficients)
line.fixef$LB <- line.fixef[,1] - 1.96*line.fixef[,2]
line.fixef$UB <- line.fixef[,1] + 1.96*line.fixef[,2]
line.fixef$OR <- exp(line.fixef[,1])
line.fixef$label <- gsub("(Intercept)", "", gsub("plottype", "", rownames(line.fixef)), fixed=T)

line.fixef2 <- line.fixef[2:10,]
line.fixef2 <- line.fixef2[order(line.fixef2$OR, decreasing = T),]
line.fixef2$label <- 
  line.fixef2$label %>% 
  str_replace("color", "Color + ") %>% 
  str_replace("[sS]hape", "Shape + ") %>%
  str_replace("[tT]rend", "Trend + ") %>%
  str_replace("Ellipse", "Ellipse + ") %>%
  str_replace("Error", "Error + ") %>%
  str_replace("( \\+ )$", "")
line.fixef2$label <- factor(line.fixef2$label, levels=line.fixef2$label[order(line.fixef2$OR, decreasing = T)], ordered=T)
line.fixef2$LB <- exp(line.fixef2$LB)
line.fixef2$UB <- exp(line.fixef2$UB)

qplot(data=line.fixef2, x=label, y=OR, ymin=LB, ymax=UB, geom="pointrange") + 
  coord_flip() + 
  ylab("Odds Ratio + 95% Wald Interval") + 
  xlab("Plot Type") + 
  ggtitle("Effect of Plot Aesthetics on P(Line Target Selected)") + 
  theme_bw()

names(line.fixef2)[1:4] <- c("Log Odds", "Std. Error", "Z", "P value")
names(line.fixef2)[8] <- "Plot Aesthetic"
print(xtable(line.fixef2[,c(8, 1:4)], caption=c("Fitted values of fixed effects for the model described in \\eqref{eqn:linearModel}. Only Trend+Error plots significantly increase the probability of detecting the linear target plot (with data generated from $M_T$), while most other aesthetic combinations decrease the probability of detecting the linear target plot.", "Fixed effects for linear target logistic model"), label="tab:line.fixef", align=c('r', 'r', 'l', 'l', 'l', 'l'), digits=c(0, 0, 4, 4, 2, 4)), include.rownames=F, file="figure/linear-fixef-table")
@

\input{figure/linear-fixef-table}

\begin{figure}\centering
\includegraphics[width=.75\linewidth]{figure/line-model-1}
\caption{Odds ratios describing the odds of detecting the linear target plot for each aesthetic, relative to a plain scatterplot. Only the combination of Trend + Error significantly increases the odds of linear target plot detection relative to the control plot. \label{fig:linear.fixef}}
\end{figure}

\newdo{Commentary goes here... too tired tonight}

\newdo{Discuss variance/covariance and random effects}

\subsection{Group Target Selection}

\newtext{We now examine the probability of selecting the group target plot as a function of plot type, with random effects for dataset (which encompasses parameter effects) and participant (accounting for variation in individual skill level). For plot type $i = 1, ..., 10$ displaying dataset $j=1, ..., 54$ by participant $k=1, ..., P$, 
}
\begin{align}
P(\text{success}) & =  \left(e^\theta\right)/\left(1+e^\theta\right)\label{eqn:groupModel}\\
\theta & =  \textbf{X}\beta + \textbf{J}\gamma + \textbf{K}\eta + \epsilon \tag{a}\\
\text{where } \beta_i & \hphantom{\sim} \text{describe the effect of specific plot aesthetics}\nonumber\\
\hphantom{where } \gamma_j & \overset{iid}{\sim} N\left(0, \sigma^2_{\text{data}}\right)\nonumber\\
\hphantom{where } \eta_k & \overset{iid}{\sim} N\left(0, \sigma^2_{\text{participant}}\right)\nonumber\\
\text{and } \epsilon_{ijk} & \overset{iid}{\sim}  N\left(0, \sigma^2\right)\nonumber
\end{align}
\newtext{
As before, we note that any variance due to parameters $K$, $\sigma_T$, and $\sigma_C$ is contained within $\sigma^2_{\text{data}}$ and can be examined using a subsequent model. 
}

<<group-model, echo=FALSE, include=FALSE, dependson='results-setup', fig.width=8, fig.height=4>>=
group.model <- glmer(group.correct~ plottype + (1|individualID) + (1|dataset), 
                    data = modeldata, 
                    family = binomial(link="logit"), 
                    control=glmerControl(optimizer="bobyqa"))
group.fixef <- as.data.frame(summary(group.model)$coefficients)
group.fixef$LB <- group.fixef[,1] - 1.96*group.fixef[,2]
group.fixef$UB <- group.fixef[,1] + 1.96*group.fixef[,2]
group.fixef$OR <- exp(group.fixef[,1])
group.fixef$label <- gsub("(Intercept)", "", gsub("plottype", "", rownames(group.fixef)), fixed=T)

group.fixef2 <- group.fixef[2:10,]
group.fixef2 <- group.fixef2[order(group.fixef2$OR, decreasing = T),]
group.fixef2$label <- 
  group.fixef2$label %>% 
  str_replace("color", "Color + ") %>% 
  str_replace("[sS]hape", "Shape + ") %>%
  str_replace("[tT]rend", "Trend + ") %>%
  str_replace("Ellipse", "Ellipse + ") %>%
  str_replace("Error", "Error + ") %>%
  str_replace("( \\+ )$", "")
group.fixef2$label <- factor(group.fixef2$label, levels=group.fixef2$label[order(group.fixef2$OR, decreasing = T)], ordered=T)
group.fixef2$LB <- exp(group.fixef2$LB)
group.fixef2$UB <- exp(group.fixef2$UB)

qplot(data=group.fixef2, x=label, y=OR, ymin=LB, ymax=UB, geom="pointrange") + 
  coord_flip() + 
  scale_y_continuous("Odds Ratio + 95% Wald Interval", breaks=c(.5, .75, 1, 1.25)) + 
  xlab("Plot Type") + 
  ggtitle("Effect of Plot Aesthetics on P(Group Target Selected)") + 
  theme_bw()

names(group.fixef2)[1:4] <- c("Log Odds", "Std. Error", "Z", "P value")
names(group.fixef2)[8] <- "Plot Aesthetic"
print(xtable(line.fixef2[,c(8, 1:4)], caption=c("Fitted values of fixed effects for the model described in \\eqref{eqn:groupModel}.", "Fixed effects for group target logistic model"), label="tab:group.fixef", align=c('r', 'r', 'l', 'l', 'l', 'l'), digits=c(0, 0, 4, 4, 2, 4)), include.rownames=F, file="figure/group-fixef-table")
@

\input{figure/group-fixef-table}

\begin{figure}\centering
\includegraphics[width=.75\linewidth]{figure/group-model-1}
\caption{Odds ratios describing the odds of detecting the cluster target plot for each aesthetic, relative to a plain scatterplot. The presence of error lines or bounding ellipses significantly decreases the probability of correct target detection, and no aesthetic successfully increases the probability of correct target detection. This may be due to differences in group size for null plots, with data generated under $M_0$ compared with the group target plot displaying data generated under $M_C$. \label{fig:group.fixef}}
\end{figure}

\subsection{Signal Strength}

\section{Discussion}

\bibliographystyle{asa}
\bibliography{references}

\section*{Simulation Studies of Parameter Space}\label{app:parametersimulation}
\subsection{Distribution of Test Statistics}

Simulating lineup data sets, we can compare test statistics measuring trend strength, cluster strength, and cluster size inequality for the null plots and target plots. These distributions allow us to objectively assess the difficulty of detecting the target datasets computationally (without relying on human perception). 
\todo{Add equations for test statistics}

<<null-distribution, echo=FALSE, include=FALSE, cache=T, fig.width=7, fig.height=2>>=
source("../../Code/MixtureLineups.R")
sT = 0.25
sC = 0.20
N = 45
K = 3
M = 1000

if (file.exists("./figure/nulldist.Rdata")) {
  load("./figure/nulldist.Rdata")
} else {
#   nulldist<- function(N, sT=0.25, sC=0.2) {
#     nulls <- data.frame(t(replicate(N, {
#       lp <- data.frame(t(replicate(18, {
#         mix = mixture.sim(lambda=0.5, K=3, N=45, sd.cluster=sC, sd.trend=sT)
#         reg <- lm(y~x, data=mix)
#         
#   c(fline=summary(reg)$r.squared, fgroup=cluster(mix))
#   })))
#     c(fline=max(lp$fline), fgroup=max(lp$fgroup))
#   })))
#   
#   trends <- replicate(10, {
#     mix = mixture.sim(lambda=0, K=3, N=45, sd.cluster=sC, sd.trend=sT)
#     reg <- lm(y~x, data=mix)
#     c(fline=summary(reg)$r.squared)
#   })
#   
#   clusters <- replicate(10, {
#     mix = mixture.sim(lambda=1, K=3, N=45, sd.cluster=sC, sd.trend=sT)
#     clust <- lm(y~factor(group) + 0, data=mix)
#     res <- summary(aov(clust))
#     c(fgroup=cluster(mix))
#   })
#   
#   list(nulls=nulls, trends=trends, clusters=clusters)
# }
# 
# res <- nulldist(N=N, sC=sC, sT=sT)
  library(compiler)
  tmp <- function(M=1000, N=45, K=3, sT=0.3, sC=0.25) {
    data.frame(
      t(
        replicate(M, 
                  {
                    input.pars <- list(N=N, K=K, sd.trend=sT, sd.cluster=sC)
                    c(unlist(input.pars), eval.data(gen.data(input.pars)))
                    }
                  )
        )
      )
    }
  nulldist <- cmpfun(tmp)
  
  res <- nulldist(M=M, N=45, K=3, sT=sT, sC=sC)
  
  save(res, file="./figure/nulldist.Rdata")
}

# require(ggplot2)
# qplot(fline, data=res$nulls, binwidth=0.02, fill=I("grey70"), color=I("grey20")) + geom_vline(aes(xintercept=res$trends), color="black") + theme_bw() + xlab("Max(18) Distribution of Cluster Measure under Null Model")
# ggsave("figure/fline.pdf", width=8, height=4)
# qplot(fgroup, data=res$nulls, binwidth=0.01, fill=I("grey70"), color=I("grey20")) + geom_vline(aes(xintercept=res$clusters), color="black") + theme_bw() + xlab("Max(18) Distribution of Trend Measure under Null Model")
# ggsave("figure/fgroup.pdf", width=8, height=4)

longres <- melt(res, id.vars=1:4, variable.name="type", value.name = "value")
longres$dist <- c("Data", "Max(18 Nulls)")[1+grepl("null", longres$type)]
longres$type <- gsub("null.", "", longres$type, fixed=T)
longres$Statistic <- longres$type
longres$Statistic[longres$type=="cluster"] <- "Cluster Measure"
longres$Statistic[longres$type=="line"] <- "R^2"
longres$Statistic[longres$type=="gini"] <- "Gini Coefficient"
longres$Statistic <- factor(longres$Statistic, levels=c("R^2", "Cluster Measure", "Gini Coefficient"))

qplot(data=longres, x=value, y=..density.., stat="density", color=dist, fill=dist, geom="area", alpha=I(.5), 
      main=expression(paste("Simulation Results " , group("(", list(K==3, sigma[T]==.25, sigma[C]==.2), ")"))), 
      xlab="Simulated Distribution of Test Statistic", 
      ylab="Density", 
      position="identity") + 
  facet_grid(.~Statistic, scales="free", labeller=label_both) + 
  scale_color_manual("Distribution", values=c("black",  "grey")) + 
  scale_fill_manual("Distribution", values=c("transparent", "grey")) +  
  theme_bw()
@

Figure~\ref{fig:targetsignal} show computed densities of the maximum null distribution measure compared with the measure in the signal plot. There is some overlap in the distribution of $R^2$ for the null plots compared with the target plot displaying data drawn from $M_T$. We have two measures comparing data drawn from $M_C$ and $M_0$; the cluster measure examines the variance in $x$ and $y$ described by the cluster center; the gini coefficient examines the inequality in group sizes. These simulations indicate that it may be possible to differentiate $M_C$ based on two different features in clustered data. In future experiments, it may be beneficial to control cluster size more tightly to remove this additional feature. 

The distribution of the cluster statistic values are more easily separated from the null plots than the distribution of the line statistic, indicating that $\sigma_C = 0.20$ is producing target plots that are a bit easier to spot than trend targets with a parameter value of $\sigma_T = 0.25$, however, the inequality of group sizes may distract participants from the intended target signal of cluster cohesion.

\begin{figure}[h]
\centering
\includegraphics[width=.9\textwidth]{figure/null-distribution-1}
\caption{\label{fig:targetsignal}Density of test statistics measuring trend strength, cluster strength, and cluster inequality for target distributions and null plots. }
\end{figure}


\subsection{Full Parameter Space Simulation Study}
Using 1000 simulations for each of the 98 combinations of parameters ($K=\{3,5\}$, $\sigma_C=\{.1, .15, .2, .25, .3, .35, .4\}$, $\sigma_T=\{.2, .25, .3, .35, .4, .45, .5\}$), we explored the effect of parameter value on the distribution of summary statistics describing the line strength ($R^2$) and cluster strength (short description here) for null and target plots. The plot below shows the 25th and 75th percentiles of the distribution of these summary statistics for each set of parameter values. These plots guided our evaluation of ``easy", ``medium" and ``hard" parameter values for line and cluster tasks. 

\comment{What we also see from these plots, is that we do have a $\sigma_C \sigma_T$ interaction: the distinction between target and null on a fixed setting of clustering becomes increasingly difficult as the standard deviation for the linear trend is increased, and vice versa. We might also have a three-way interaction between $\sigma_C, \sigma_T$, and $K$: the size of the blue intervals (bottom figure) changes in size between different levels of $K$, it changes for different levels of $\sigma_C$ and $\sigma_T$. I am not sure whether that is an actual three-way interaction or just all three two-way interactions, but it doesn't matter, at this point we are just talking about potentially saving one parameter. What is clear, is that we need to block by parameter setting. We do so, by blocking on each dataset. Each dataset is non-deterministic, though, because we have a random process generating from different parameter settings, not a deterministic run setting as in an engineering setting. We therefore need repetitions of the data generation to be able to separate the variability coming from within the parameter setting from the additional variability introduced by the subjects' evaluations of the lineups. }
<<simulationparameters,echo=F,include=T, fig.width=10, fig.height=6.5, out.width='.8\\linewidth'>>=
load("../../Data/SimulationDatasetCriteria.Rdata")

dataset.criteria$ParameterSet <- with(dataset.criteria, sprintf("sdT%.2f-sdC%.2f", sd.trend, sd.cluster))
dataset.criteria$ParameterSet[dataset.criteria$type=="cluster"] <- with(dataset.criteria[dataset.criteria$type=="cluster",], sprintf("sdC%.2f-sdT%.2f", sd.cluster, sd.trend))

dataset.criteria$lsc <- paste("sigma[C]: ", round(dataset.criteria$sd.cluster, 2))
dataset.criteria$lst <- paste("sigma[T]: ", round(dataset.criteria$sd.trend, 2))
dataset.criteria$lK <- paste("K: ", dataset.criteria$K)
qplot(data=subset(dataset.criteria, type=="line"), x=LB, xend=UB, y=sd.cluster, yend=sd.cluster, color=dist, geom="segment") +
  geom_point(aes(x=LB, y=sd.cluster, color=dist))  + 
  geom_point(aes(x=UB, y=sd.cluster, color=dist)) + 
  facet_grid(lst~lK, scales="free", labeller="label_parsed")  + theme_bw() + 
  scale_color_brewer("Distribution",palette="Set1") + theme(legend.position="bottom") + xlab("Interquartile intervals of Max (18) null distribution (blue) \nand target distribution (red) of linearity measured in R squared.") + ylab(expression("Cluster variability":sigma[C]))

qplot(data=subset(dataset.criteria, type=="cluster"), x=LB, xend=UB, y=sd.trend, yend=sd.trend, color=dist, geom="segment") +
  geom_point(aes(x=LB, y=sd.trend, color=dist))  + 
  geom_point(aes(x=UB, y=sd.trend, color=dist)) + 
  facet_grid(lsc~lK, scales="free", labeller="label_parsed")  + theme_bw() + 
  scale_color_brewer("Distribution", palette="Set1") + theme(legend.position="bottom") + xlab("Interquartile intervals of Max (18) null distribution (blue) \nand target distribution (red) of amount of clustering.") + ylab(expression( "Variability along the trend":sigma[T]))
@

\section*{Experimental Design}

Initially, assume a fully factorial, balanced design, with $r$ unique datasets per parameter set (replicates) and $P$ evaluations per $(\text{aesthetic}|\text{dataset})$. The experiment is conducted at three levels: parameter sets (with replication, so EUs are data sets), plot types (i.e. a certain set of aesthetics), and participant evaluations. At the first level, there are three parameters: $K =3$, $\sigma_T \in \{.15, .25, .35\}$, and $\sigma_C \in \{.15, .25, .35\}$. At the second level, there are blocks (by data set), and then 4 aesthetic combinations. 


\comment{We'll have to use contrasts to measure the effect of color individually, etc., for now let's just consider the ANOVA evaluation}

Finally, at the lowest level, there are participant effects. 

\comment{At the participant level, we need to decide if we're going to fully randomize, try to block, etc. - are participants going to get 10 different data sets? 5? Not sure how to conceptualize that, and I would imagine it will affect how we organize model evaluation. Grr, I hate mixed models. 

HH: yes, I would assume that participants get ten plots each, one from each of the designs in a random order. (We have the data base set up that way).}


\comment{Modified from Table 10.6 (pg 181) of Design of Experiments by Dr. Morris. The table in the book has a four-factor split plot design with three levels (randomized, block, block). }

\begin{table}[h]
\centering\tiny
\begin{tabular}{lllll}\hline\hline\\
\bf Level & \bf Factor & \bf Source & \bf DF & \bf Sum of Squares \\\hline
\multirow{11}{*}{\bf Dataset}
      & $K$    & $\alpha$  & 1 & 
      $\sum_i (4)(5)(r)(10P)(\overline{y}_{i\cdot\cdot\cdot\cdot\cdot}-
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & $\sigma^2_T$ & $\beta$  & 3 & 
      $\sum_j (2)(5)(r)(10P)(\overline{y}_{\cdot j\cdot\cdot\cdot\cdot}-
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & $\sigma^2_C$ & $\gamma$  & 4 & 
      $\sum_i (2)(4)(r)(10P)(\overline{y}_{\cdot\cdot k\cdot\cdot\cdot}-
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      &    & $(\alpha\beta)$ & 3 & 
      $\sum_{ij} (5)(r)(10P)(\overline{y}_{ij\cdot\cdot\cdot\cdot}-
                               \overline{y}_{i\cdot\cdot\cdot\cdot\cdot} - 
                               \overline{y}_{\cdot j\cdot\cdot\cdot\cdot} + 
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      &    & $(\alpha\gamma)$ & 4 & 
      $\sum_{ik} (4)(r)(10P)(\overline{y}_{i\cdot k\cdot\cdot\cdot}-
                               \overline{y}_{i\cdot\cdot\cdot\cdot\cdot} - 
                               \overline{y}_{\cdot\cdot k\cdot\cdot\cdot} + 
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      &    & $(\beta\gamma)$ & 12 & 
      $\sum_{jk} (2)(r)(10P)(\overline{y}_{\cdot jk\cdot\cdot\cdot}-
                               \overline{y}_{\cdot j\cdot\cdot\cdot\cdot} - 
                               \overline{y}_{\cdot\cdot k\cdot\cdot\cdot} + 
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      &    & \multirow{2}{*}{$(\alpha\beta\gamma)$} & \multirow{2}{*}{12} & 
      $\sum_{ijk} (r)(10P)(\overline{y}_{ijk\cdot\cdot\cdot}-
                             \overline{y}_{i\cdot\cdot\cdot\cdot\cdot} -
                             \overline{y}_{\cdot j\cdot\cdot\cdot\cdot} - 
                             \overline{y}_{\cdot\cdot k\cdot\cdot\cdot} $\\
      &&&&\hphantom{$\sum_{ijk} (r)(10P)($}$ + 
                             \overline{y}_{ij\cdot\cdot\cdot\cdot} +
                             \overline{y}_{\cdot jk\cdot\cdot\cdot} + 
                             \overline{y}_{i\cdot k\cdot\cdot\cdot}- 
                             \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & & \multirow{2}{*}{Resid.} & \multirow{2}{*}{$(2)(4)(5)(r-1)$} & 
      $\sum_{ijkl} (10P)(\overline{y}_{ijkl\cdot\cdot}-
                           \overline{y}_{i\cdot\cdot\cdot\cdot\cdot} -
                           \overline{y}_{\cdot j\cdot\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot k\cdot\cdot\cdot} + 
                           \overline{y}_{ij\cdot\cdot\cdot\cdot} +
                           \overline{y}_{\cdot jk\cdot\cdot\cdot} $\\
      & & & &
      \hphantom{$\sum_{ijkl} (10P)($}$ + 
                           \overline{y}_{i\cdot k\cdot\cdot\cdot} -
                           \overline{y}_{ijk\cdot\cdot\cdot} -
                           \overline{y}_{ij\cdot l\cdot\cdot} -
                           \overline{y}_{i\cdot kl\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot jkl\cdot\cdot\cdot} +                           
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Total & & $(2)(4)(5)(r)-1$ & 
      $\sum_{ijkl} (10P)(\overline{y}_{ijkl\cdot\cdot}-
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \hline\\\hline
\multirow{8}{*}{Plot}
      & Dataset & blocks & $(2)(4)(5)(r)-1$ & 
      $\sum_{ijkl} (10P)(\overline{y}_{ijkl\cdot\cdot}-
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Aes. & $\delta$ & 9 & 
      $\sum_{m} (2)(4)(5)(P)(\overline{y}_{\cdot\cdot\cdot\cdot m\cdot}-                        
                             \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Aes x $K$ & $(\alpha\delta)$ & 9 & 
      $\sum_{im} (4)(5)(P)(\overline{y}_{i \cdot\cdot\cdot m\cdot}- 
                           \overline{y}_{i \cdot\cdot\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & Aes x $\sigma_T$ & $(\beta\delta)$ & 27 & 
      $\sum_{jm} (2)(5)(P)(\overline{y}_{\cdot j\cdot\cdot m\cdot}- 
                           \overline{y}_{\cdot j\cdot\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & Aes x $\sigma_C$ & $(\gamma\delta)$ & 36 & 
      $\sum_{km} (2)(4)(P)(\overline{y}_{\cdot\cdot k\cdot m\cdot}- 
                           \overline{y}_{\cdot\cdot k\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Others & & 9(31) & \multirow{2}{*}{difference }\\
      & Resid & & 40(rP-1)-(40r-1)  & \\
      \cline{2-5}
      & Total & & $400r-1$ & 
      $\sum_{ijklm} (P)(\overline{y}_{ijklm\cdot}-
                        \overline{y}_{ijkl\cdot\cdot})^2$ \\
      \hline\\\hline
\multirow{4}{*}{Trial}
      & Picture & Sub-blocks & $400r-1$ & 
      $\sum_{ijklm} (P)(\overline{y}_{ijklm\cdot}-
                        \overline{y}_{ijkl\cdot\cdot})^2$ \\
      \cline{2-5}
      & Participants & $\tau$ & $P-1$ & 
      $\sum_{n} (2)(4)(5)(r)(10) (\overline{y}_{\cdot\cdot\cdot\cdot\cdot n} -
                            \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Resid & & $(400r-1)(P)$ & difference \\
      \cline{2-5}
      & Total & & $400(r)(P)-1$ & 
      $\sum_{ijklmn} (y_{ijklmn} - 
                      \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \hline\hline
\end{tabular}
\caption{Evaluation of sources of error in a full factorial version of the experiment, with $r$ replicates of each parameter combination and $P$ participant evaluations of each plot(data/aesthetic combination).}
\end{table}

We have a couple of options:
\begin{itemize}
\item keep the full factorial experiment, use one (at most two) replicates, and use higher level factorial effects to beef up any error variance terms. 
\item Do a full factorial experiment for $K=3$ and use a subset of the factorial experiment for $K=5$ (either using a subset of cases for $\sigma_T$ and $\sigma_C$, or a subset of combinations of the two cases/fractional factorial.)
\end{itemize}

\comment{
The fractional factorial option will be a pain to explain when we write things up; it will be simpler to explain using a subset of cases. Given that we don't particularly care about the third-order effects (and possibly not even the second-order effects) for the parameters, I'm inclined to say that the single-replicate option is the easiest way to go (and lets us keep the simple SSQ in the table, which is a huge bonus in my opinion). Even if we just use the third-order interaction effect as error, we still have 12 degrees of freedom; that should be plenty - we'd only need F=\Sexpr{round(qf(.95, 12, 12), 2)} to get a significant result for even the $(\sigma_T\sigma_C)$ test. 

HH: We might not care about interpreting the two-way interactions, but unfortunately they will be there (see comment at the back). So I would suggest to go with a full factorial design in $\sigma_C, \sigma_T$, and $K$, with three replications each (we need the replicates, also explained in the back). This gives us 18 parameter settings, and $18\cdot 3 = 54$ data sets. In case you still want to consider the effect of the number of datapoints $N$, we could switch from fully factorial to fractional factorial and replace the three-way interaction of $\sigma_C, \sigma_T$, and $K$ by the settings of $N$. That way we will keep the 18 settings. }

\begin{table}[h]
\caption{ANOVA table - only one replicate. Evaluation of sources of error in a full factorial version of the experiment, with one replicate of each parameter combination and $P$ participant evaluations of each plot(data/aesthetic combination).}
\centering\footnotesize
\begin{tabular}{lllll}\hline\hline\\
Level & Factor & Source & DF & Sum of Squares \\\hline
\multirow{5}{*}{Dataset}
      & $K$    & $\alpha$  & 1 & 
      $\sum_i (4)(5)(10P)(\overline{y}_{i\cdot\cdot\cdot\cdot}-
                          \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & $\sigma^2_T$ & $\beta$  & 3 & 
      $\sum_j (2)(5)(10P)(\overline{y}_{\cdot j\cdot\cdot\cdot}-
                          \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & $\sigma^2_C$ & $\gamma$  & 4 & 
      $\sum_i (2)(4)(10P)(\overline{y}_{\cdot\cdot k\cdot\cdot}-
                          \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & & Resid. & 22 & difference \\
      \cline{2-5}
      & Total & & 39 & 
      $\sum_{ijk} (10P)(\overline{y}_{ijk\cdot\cdot}-
                        \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \hline\\\hline
\multirow{7}{*}{Plot}
      & Dataset & blocks & 39 & 
      $\sum_{ijk} (10P)(\overline{y}_{ijk\cdot\cdot}-
                        \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Aes. & $\delta$ & 9 & 
      $\sum_{m} (2)(4)(5)(P)(\overline{y}_{\cdot\cdot\cdot m\cdot}-                        
                             \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Aes x $K$ & $(\alpha\delta)$ & 9 & 
      $\sum_{im} (4)(5)(P)(\overline{y}_{i \cdot\cdot m\cdot}- 
                           \overline{y}_{i \cdot\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & Aes x $\sigma_T$ & $(\beta\delta)$ & 27 & 
      $\sum_{jm} (2)(5)(P)(\overline{y}_{\cdot j\cdot m\cdot}- 
                           \overline{y}_{\cdot j\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & Aes x $\sigma_C$ & $(\gamma\delta)$ & 36 & 
      $\sum_{km} (2)(4)(P)(\overline{y}_{\cdot\cdot k m\cdot}- 
                           \overline{y}_{\cdot\cdot k\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Resid & & 9(31) & difference \\
      \cline{2-5}
      & Total & & 399 & 
      $\sum_{ijkm} (P)(\overline{y}_{ijkm\cdot}-
                       \overline{y}_{ijk\cdot\cdot})^2$ \\
      \hline\\\hline
\multirow{4}{*}{Trial}
      & Picture & Sub-blocks & 399 & 
      $\sum_{ijkm} (P)(\overline{y}_{ijkm\cdot}-
                       \overline{y}_{ijk\cdot\cdot})^2$ \\
      \cline{2-5}
      & Participants & $\tau$ & $P-1$ & 
      $\sum_{n} (2)(4)(5)(10) (\overline{y}_{\cdot\cdot\cdot\cdot n} -
                            \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Resid & & $399(P-1)$ & difference \\
      \cline{2-5}
      & Total & & $400P-1$ & 
      $\sum_{ijkmn} (y_{ijkmn} - 
                     \overline{y}_{\cdot\cdot\cdot\cdot})^2$ \\
      \hline\hline
\end{tabular}
\end{table}

\end{document}