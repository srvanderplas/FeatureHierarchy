\documentclass[10pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames]{xcolor}
\graphicspath{{figure/}}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage[colorinlistoftodos]{todonotes}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\done}[2][inline]{\todo[color=SpringGreen, #1]{#2}}  % for todos that have been seen and dealt with
\newcommand{\meh}[2][inline]{\todo[color=White, #1]{#2}}   % for todos that may no longer be relevant 
\newcommand{\comment}[2][inline]{\todo[color=SkyBlue, #1]{#2}} % for comments that may not be "to-do"s
%\newcommand{\mcomment}[1]{\todo[color=SkyBlue]{#1}} % for margin comments
\newcommand{\newtext}[1]{\todo[inline, color=White]{ \color{OliveGreen}{#1}}} % new text - not necessarily something to be done
\newcommand{\move}[1]{\todo[inline, color=Lime]{#1}} % new to do item
%
%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

\title{Group beats Trend!? \\Testing feature hierarchy in statistical graphics}
\author{Susan VanderPlas, Heike Hofmann\thanks{Department of Statistics and Statistical Laboratory, Iowa State University}}
\begin{document}
\maketitle
\begin{abstract}abstract goes here
\end{abstract}
<<setup, fig.keep='all', cache=FALSE, echo=FALSE, eval=TRUE>>=
rm(list=ls())
options(replace.assign=TRUE,width=70)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE)
library(reshape2)
suppressMessages(library(ggplot2))
library(plyr)
suppressMessages(library(gridExtra))
@


\section{Introduction and background}
\comment{Discussion of pre-attentive visual features \citep{healey:preattentive} - with a focus on hierarchy of pre-attentive features: color trumps shape - do we also see this in our results, and if so, by how much?}
\newtext{
Our understanding of the perception of statistical charts is informed by several levels of research. Cognitive psychologists and neuroscientists often focus on pre-attentive perception, which occurs automatically in the first 200 ms of exposure to a visual stimulus. Gestalt psychologists focus instead on perception as a wholistic experience; they consider the heuristics used to transform visual stimuli into useful, coherent information. Finally, statistical graphics researchers apply low-level perceptual research and gestalt ideas to statistical charts, using tools such as lineups \citep{Buja:2009hp, mahbub:2013, wickham:2010,Hofmann:2012ts} to determine which graphics are accurately perceived and communicate information effectively. 
}
\newtext{
Research into the preattentive stage of visual perception provides us with some information about the temporal hierarchy of graphical feature processing. Color, line orientation, and shape are processed preattentively; that is, within 200 ms, it is possible to identify a single target in a field of distractors, if the target differs with respect to color or shape \citep{goldstein2009encyclopedia}. \citet{healey1999large} extended this work, demonstrating that certain features of three-dimensional data displays are processed preattentively, but that this does not always translate into faster or more accurate inference about the data displayed, particularly when participants must integrate several preattentive features to understand the data. 
}
\newtext{
Gestalt rules of perception also impact statistical graphics. These rules describe the way we organize visual input, focusing on the wholistic experience rather than the individual perceptual features. For example, rather than perceiving four legs, a tail, two eyes, two ears, and a nose, we perceive a dog. The rules of perceptual grouping or organization, as stated in \citet{goldstein2009encyclopedia} are:
\begin{itemize}
\item Proximity: two elements which are close together are more likely to belong to a single unit.
\item Similarity: the more similar two elements are, the more likely they belong to a single unit.
\item Common fate: two elements moving together likely belong to a single unit.
\item Good continuation: two elements which blend together smoothly likely belong to one unit.
\item Closure: elements which can be assembled into closed or convex objects likely belong together. 
\item Common region: elements contained within a common region likely belong together. 
\item Connectedness: elements physically connected to each other are more likely to belong together.
\end{itemize}
}

\begin{figure}[htbp]\centering
\includegraphics[width=.9\textwidth]{gestalt}
\caption[The gestalt laws of perception]{The gestalt laws of perception}\label{fig:gestaltlaws}
% \protect\footnotetext{From \url{http://yusylvia.files.wordpress.com/2010/03/gestalt_illustration-01.jpg}}
\end{figure}

\newtext{
Figure \ref{fig:gestaltlaws} shows examples of many of the gestalt laws, which when combined help to order our perceptual experience. These laws help to order our perception of charts as well: points which are colored or shaped the same are perceived as belonging to a group (similarity), points within a bounding interval or ellipse are perceived as belonging to the same group (common region), and regression lines with confidence intervals are perceived as single units (connectedness, closure, and/or common region). The use of physical location, color, and shape to organize graphical units mentally utilizes both preattentive processing and higher-order gestalt schemas, identifying and grouping similar graphical features and simultaneously directing attention to graphical features which stand alone. 

}


Intro to lineups \citep{Buja:2009hp, mahbub:2013, wickham:2010, Hofmann:2012ts}

The change to lineups we make is to introduce a second target to each lineup. We then keep track of how many observers choose any one of the two targets (to assess the difficulty of a lineup), and additionally we  record how often observers choose one target over the other one. This is information that we can use to evaluate how strong the signal of one target is compared to the other one. 

A further extension of this testing framework are the use of color (in a qualitative color scheme), the use of shapes, and additional density lines - we anticipate that all of these features are going to emphasize the clustering component. 
On the other hand, regression lines should emphasize any linear trends in the data.

\section{Design Choices}
We choose colors and shapes for the lineups in our study to be the most different from a set of ten choices as evaluated by participants in the study by  \citet{heer:2014} on the so called perceptual kernels.

Unfortunately, this limits the choice to the set used in the Tableau software. In order to produce experimental stimuli accessible to the approximately 4\% of the population with red-green color deficiency \citep{colorvision}, we removed the grey hue from the palette. This modification produced maximally different color combinations which did not include red-green combinations, while also removing a color (grey) which is difficult to distinguish for those with color deficiency.  

% Beyond that, we modified some color pairings observed in the general population to reflect individual's abilities: the red-green color pair is one of the pairs of the most distinct color pairings in the general population (XXX exact value?), but obviously is a poor choice for the approximately XXX\% of population with a red-green color vision deficiency.

Shapes - there were some problems with reliable unicode representations in the lineups, which led to using slightly modified shapes. The left and right triangle shapes (available only in unicode using R) were excluded due to size differences between unicode and non-unicode shapes. 

All shapes are non-filled shapes, which means that they are consistent with one of the simplest solutions to overplotting of points in the tradition of \citet{tukey, cleveland:85} and \citet{few}. For this reason we abstained from the additional use of alpha-blending of points to diminish the effect of overplotting in the plots.

\comment{How do the shapes we picked compare to the shapes discussed in \citet{robinson:03} see http://stat-computing.org/newsletter/issues/scgn-14-1.pdf? }
\comment{For $K=3$ we have one shape each from Robinson's group 1, 2, and 9 - these groups are choses such that they all ``show differences in their preattentive properties"; for $K=5$, we have one shape each from groups 1, 2, 3,  9, and 10. }

\section{Generating Model}
We are working with two  models $M_C$ and $M_T$ to generate data for the target plots. The null plots are showing data generate from a mixture model $M_0$. Both models generate data in the same range of values. We made also sure that data from the clustering model $M_C$ shares the same correlation 
with the null data, while data from model $M_T$ exhibits a similar amount of clustering as the null data. 

\subsection{Regression Model $M_T$}

This model has the parameter $\sigma_T$ to reflect the amount of scatter around the trend line. 

% \comment{Isn't there some centering or scaling missing? How do we make sure, that the regression model and the cluster model are on the same x scale?  Only for $K=3$ does the scaling in the clustering model boil down to values between [-1,1].  For $K=5$ the values are further out. \\
% \textit{The scaling and centering happens in a different function. I've added that now.}}
\begin{algorithm}\hfill\newline
  Input Parameters: sample size $N$, $\sigma_T$ standard deviation around the line, slope $a$ (1 by default) \\
  Output: $N$ points, in form of vectors $x$ and $y$.
  \begin{enumerate}
    \item Generate $\tilde{x}_i$, $i=1, ..., N$, as a sequence of evenly spaced points from $[-1, 1]$ 
    \item Jitter $\tilde{x}_i$ by adding small uniformly distributed perturbations to each of the values: $x_i = \tilde{x}_i + \eta_i$, $\eta_i \sim \text{Unif}(-z, z)$, $z = 1/5*(2/(N-1)$
    \item Generate $y_i$: $y_i = a  x_i + e_i$, $e_i \sim N(0, \sigma^2_T)$
    \item Center and scale $x_i$, $y_i$
  \end{enumerate}
\end{algorithm}

We compute the correlation coefficient for all of the plots to assess the amount of linearity in each panel, computed as 
\[
r = 1 - RSS/TSS,
\]
where TSS is the total sum of square, $TSS = \sum_{i=1}^N (y_i - \bar{y})^2$ and $RSS = \sum_{i=1}^N e_i^2$.
The expected correlation coefficient $\rho$ in this scenario is 
\[
\rho = \frac{\frac{1}{3}a^2}{\frac{1}{3}a^2 + \sigma^2_T},
\]
because
$E[RSS] = N\sigma^2_T$ and $E[TSS] = \sum_{i=1}^N E\left[y_i^2\right]$  (as $E[Y] = 0$), where 
\[
E\left[y_i^2\right] = E\left[a^2 x_i^2 + e_i^2 + 2 ax_ie_i\right] = \frac{1}{3}a^2 + \sigma^2_T. 
\]
\begin{figure}[ht]
<<trends, fig.width=8, fig.height=3, out.width='\\textwidth', echo=FALSE>>=
source("../../Code/MixtureLineups.R")
sd <- c(0.1, 0.2, 0.3, 0.4)
res <- ldply(sd, function(x) { data.frame(sd.trend=x, sim.line(N=45, sd.trend=x)) })
suppressMessages(library(ggplot2))
res$label <- paste("sigma[T] :",res$sd.trend)
qplot(x,y, data=res, pch=I(1)) + facet_grid(facets=.~label, labeller="label_parsed") + theme_bw() + 
  theme(plot.margin=unit(c(0,0,0,0), "cm"))
@
\caption{\label{fig:trends} Set of scatterplots showing one draw each from the trend model $M_T$ for parameter values of  $\sigma_T \in \{0.1, 0.2, 0.3, 0.4\}$.}
\end{figure}

\subsection{Cluster Model $M_C$}
We begin by generating $K$ cluster centers on a $K \times K$ grid, then we generate points around the cluster center. 
\begin{algorithm}\hfill\newline
  Input Parameters:  $N$ points, $K$ clusters, $\sigma_C$ cluster standard deviation \\
  Output: \comment{specify output}
  \begin{enumerate}
    \item Generate cluster centers $(c^x_{i}, c^y_{i})$ for each of the $K$ clusters, $i=1, ..., K$:
      \begin{enumerate}
        \item in form of two vectors $c^{x}$ and $c^y$ of permutations of $\{1, ..., K\}$, such that
        \item the correlation between cluster centers \text{Cor}$(c^{x}, c^{y})$ falls into a range of $[.25, .75]$.
      \end{enumerate}
      \item Center and standard-normalize cluster centers $(c^x, c^y)$:  
      \[
        \tilde{c}^x_{i} = \frac{c^x_{i} - \bar{c}}{s_c} \ \ \text{ and } \ \ \tilde{c}^y_{i} = \frac{c^y_{i} - \bar{c}}{s_c},
      \]
      where $\overline{c} = (K+1)/2$ and $s_c^2 = \frac{K(K+1)}{12}$ for all $i = 1, ..., K$.
    \item For the $K$ clusters, we want to have nearly equal sized groups, but uphold some variability. Group sizes are therefore determined as a draw from a multinomial distribution: determine group sizes $g = (g_1, ..., g_K)$, with $N = \sum_{i=1}^K g_i$, for clusters $1, ..., K$ as a random draw 
    \[
    g \sim \text{Multinomial}(K, p) \text{ where } p = \tilde{p}/\sum_{i=1}^K \tilde{p}_i, \text{ for } \tilde{p} \sim N(\frac{1}{K}, \frac{1}{2 K^2}).
    \]
     
    \item Generate points around cluster centers: 
      \begin{enumerate}
        \item $x_i = \tilde{c}^x_{g_i} + e^x_i$, where $e^x_i \sim N(0, \sigma^2_C)$
        \item $y_i = \tilde{c}^y_{g_i} + e^y_i$, where $e^y_i \sim N(0, \sigma^2_C)$
      \end{enumerate}
    \item Center and scale $x_i$, $y_i$
  \end{enumerate}
\end{algorithm} 


As a measure of clustering we use a  coefficient to assess the amount of variability taken care of by including the grouping variable, compared to total variability. Note that for the purpose of clustering, variability is measured as the variability in both $x$ and $y$ from a common mean, i.e. we implicitly assume that the values in $x$ and $y$ are on the same scale (which we can easily achieve by an appropriate re-scaling).  

\comment{some examples? with colour?, K = 3 ... 6? maybe two different standard deviations? }  

\begin{figure}[ht]
<<cluster, fig.width=8, fig.height=6, out.width='\\textwidth', echo=FALSE>>=
sd <- c(0.15, 0.2, 0.25, 0.3)
colors <-  c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", 
             "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf")

res <- ldply(sd, function(x) { data.frame(sd.cluster=x, sim.clusters(K=3, N=45, sd.cluster=x)) })
res$K <- 3
res2 <- ldply(sd, function(x) { data.frame(sd.cluster=x, sim.clusters(K=5, N=75, sd.cluster=x)) })
res2$K <- 5
res <- rbind(res, res2)
suppressMessages(library(ggplot2))
res$label <- paste("sigma[C] :",res$sd.cluster)
res$Klabel <- paste("K :",res$K)
qplot(x,y, data=res, pch=I(1), colour=factor(group), shape=factor(group)) + 
  facet_grid(facets=Klabel~label, labeller="label_parsed") + theme_bw() + 
  theme(plot.margin=unit(c(0,0,0,0), "cm"), legend.position="none") + 
  scale_colour_manual(values=colors) 
@
\caption{\label{fig:clusters} .}
\end{figure}

\subsection{Null Model $M_0$}
The generative model for null data  is created as a mixture model $M_0$ that draws $n_c \sim B_{N, \lambda}$ observations from the cluster model, and $n_T = N - n_c$ from the regression model $M_T$.
% 
% Under the null model, $M_T$ slope may be between $(.2, .8)$
% \comment{We can't have anything different in the null model from the generating model, but you could vary the slope in $M_T$ itself, that would make sense.}


\comment{and a set of examples, here, with varying lambda}


\section{Experimental Setup}
\subsection{Design}
Factors:
\begin{table}[h]
\begin{center}
\begin{tabular}{lll}
Parameter & Description & Choices\\\hline
$K$ & \# Clusters & 3, 5 \\
$N$ & \# Points & $15\cdot K$ \\
$\sigma_T$ & Scatter around trend line &  .15, .25, .35\\
$\sigma_C$ & Scatter around cluster centers &  .15, .20, .25, 0.30\\\hline
\end{tabular}
\end{center}
\caption{Parameter settings for Data Generation.}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{ll}
Emphasis & Aesthetics \\\hline\hline
Control & -- \\\hline
Group & Color, Shape \\
& Color + Shape, Color + Ellipse, Color + Shape + Ellipse\\\hline
Trend & Line \\
& Line + Error band\\\hline
Conflict & Color + Trend Line, \\
& Color + Ellipse + Trend Line + Error band\\\hline\hline
\end{tabular}
\end{center}
\caption{Aesthetics and add-on design choices. }
\end{table}


\newtext{Based on simulations from the null we get a distribution for each of the two quality measures for the targets under each parameter setting. This gives us an objective measure to assess the difficulty of detecting each of the targets. An overview of the results can be seen in the appendix.}

\comment{Should we keep these two histograms to explain in more detail how the simulation from the null model works? }
<<null-distribution, echo=FALSE, include=FALSE, cache=T>>=
source("../../Code/MixtureLineups.R")
sT = 0.30
sC = 0.25
N = 5000

if (file.exists("./figure/nulldist.Rdata")) {
  load("./figure/nulldist.Rdata")
} else {
nulldist<- function(N, sT=0.3, sC=0.3) {
  nulls <- data.frame(t(replicate(N, {
    lp <- data.frame(t(replicate(18, {
      mix = mixture.sim(lambda=0.5, K=3, N=45, sd.cluster=sC, sd.trend=sT)
      reg <- lm(y~x, data=mix)
      
      c(fline=summary(reg)$r.squared, fgroup=cluster(mix))
    })))
    c(fline=max(lp$fline), fgroup=max(lp$fgroup))
  })))
  
  trends <- replicate(10, {
    mix = mixture.sim(lambda=0, K=3, N=45, sd.cluster=sC, sd.trend=sT)
    reg <- lm(y~x, data=mix)
    c(fline=summary(reg)$r.squared)
  })
  
  clusters <- replicate(10, {
    mix = mixture.sim(lambda=1, K=3, N=45, sd.cluster=sC, sd.trend=sT)
    clust <- lm(y~factor(group) + 0, data=mix)
    res <- summary(aov(clust))
    c(fgroup=cluster(mix))
  })
  
  list(nulls=nulls, trends=trends, clusters=clusters)
}

res <- nulldist(N=N, sC=sC, sT=sT)

save(res, file="./figure/nulldist.Rdata")
}

require(ggplot2)
qplot(fline, data=res$nulls, binwidth=0.02, fill=I("grey70"), colour=I("grey20")) + geom_vline(aes(xintercept=res$trends), colour="black") + theme_bw() + xlab("Max(18) Distribution of Cluster Measure under Null Model")
ggsave("figure/fline.pdf", width=8, height=4)
qplot(fgroup, data=res$nulls, binwidth=0.01, fill=I("grey70"), colour=I("grey20")) + geom_vline(aes(xintercept=res$clusters), colour="black") + theme_bw() + xlab("Max(18) Distribution of Trend Measure under Null Model")
ggsave("figure/fgroup.pdf", width=8, height=4)
@

Figures~\ref{fig:fline} and \ref{fig:fgroup} show histograms of the marginal densities ...
The red lines show ten samples each from the trend model and the cluster model. 
The lines for the cluster are, relatively, further to the right of the overall distribution than the red lines for the trend model, indicating that $\sigma_C = 0.25$ is producing target plots that are a bit easier to spot than trend targets with a parameter value of $\sigma_T = 0.30$.

\begin{figure}[h]
\centering
\includegraphics[width=.6\textwidth]{figure/fline.pdf}
\caption{\label{fig:fline}Histogram of $R^2$ values from \Sexpr{N} data sets of the null model ($K=3, N=45, \sigma_C=$\Sexpr{sC}, $\sigma_T= $\Sexpr{sT}). The lines in black are $R^2$ values of ten sample data sets from the Trend model $M_T$. }
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=.6\textwidth]{figure/fgroup.pdf}
\caption{\label{fig:fgroup}Histogram of the distribution of the cluster measure based on \Sexpr{N} data sets from the null model ($K=3, N=45, \sigma_C= $\Sexpr{sC}, $\sigma_T= $\Sexpr{sT}). The lines in black correspond to the cluster measure from ten sample data sets from the Clustering model $M_C$. }
\end{figure}



\subsection*{Design choices}
\begin{enumerate}
\item Plain: two targets with data from one of each of the two generative models are included in a set of eighteen panels of null data.
\item Color/Shape: points in each of the panels are colored/marked based on the results of a hierarchical clustering . 
\item Trend line: a line of the least square fit is drawn through the points.
\item Color \& Shape
\item Color \& trend line: this emphasises both the clustering and the regression - it is not clear, which signal will be stronger.
\item Color \& Ellipsoids: around the groups of the same color, ellipsoids are drawn to reflect the 95\% density estimate.
\end{enumerate}

\subsection{Hypothesis}
The plot most identified as the ``target" will change based on plot aesthetics which emphasize linear features or cluster features. This effect will be mediated by the signal strength of the line and cluster features. 

\comment{Could you re-phrase these items so they always give an increase in the visual outcome? (or always a decrease, but the emphasis is on the outcome not on increasing the parameters).}
\begin{itemize}
\item Increasing $N$ will increase signal strength for both line and clusters
\item Increasing $K$ will decrease signal strength for clusters (at the same variability, there's less space to spread clusters out resulting in less visual separability)
\item Increasing $\sigma_T$ will decrease signal strength for lines
\item Increasing $\sigma_C$ will decrease signal strength for clusters
\end{itemize}

Plot features will emphasize either lines or clusters as follows: 

\begin{itemize}
\item None (control)
\item Color (cluster emphasis)
\item Shape (cluster emphasis)
\item Color + shape (double cluster emphasis)
\item Ellipse + color (double cluster emphasis)
\item Line
\item Line + Prediction Interval (double line emphasis)
\item Color + line (conflict)
\item Color + line + Prediction Interval (conflict)
\end{itemize}

\comment{The primary purpose of the study is to detect how using visual aesthetics emphasizes one of two features and lead to detection of one feature over another feature.}
\comment{A secondary purpose of the study is to relate signal strength to detection in a visualization by a human observer.}

In a more organized representation:
\begin{table}[h]
\centering
\begin{tabular}{cc|c|c|c|}\cline{3-5}
 & \multicolumn{1}{c}{} & \multicolumn{3}{|c|}{Line Emphasis}\\\cline{3-5}
 & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{|c|}{1} & \multicolumn{1}{|c|}{2}\\\hline
\multicolumn{1}{|c|}{\multirow{5}{*}{Cluster Emphasis}} & 
  \multicolumn{1}{|c|}{0} & None & Line & Line + Prediction\\\cline{2-5}
\multicolumn{1}{|c|}{} & \multicolumn{1}{|c|}{1} & Color, Shape & Color + Line & \\\cline{2-5}
\multicolumn{1}{|c|}{} & \multicolumn{1}{|c|}{\multirow{2}{*}{2}} & Color + Shape  & & Color + Ellipse + Line + Prediction \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{|c|}{}& Color + Ellipse & & \\\cline{2-5}
\multicolumn{1}{|c|}{} & \multicolumn{1}{|c|}{3} & Color + Shape + Ellipse & &\\\hline
\end{tabular}
\end{table}

\subsection{Experimental Design}
Initially, assume a fully factorial, balanced design, with $r$ unique datasets per parameter set (replicates) and $P$ evaluations per $(\text{aesthetic}|\text{dataset})$. The experiment is conducted at three levels: parameter sets (with replication, so EUs are data sets), plot types (i.e. a certain set of aesthetics), and participant evaluations. At the first level, there are three parameters: $K \in \{3, 5\}$, $\sigma_T \in \{.3, .4, .5, .6\}$, and $\sigma_C \in \{.2, .25, .3, .35, .4\}$. At the second level, there are blocks (by data set), and then 10 aesthetic combinations. 


\comment{We'll have to use contrasts to measure the effect of color individually, etc., for now let's just consider the ANOVA evaluation}

Finally, at the lowest level, there are participant effects. 

\comment{At the participant level, we need to decide if we're going to fully randomize, try to block, etc. - are participants going to get 10 different data sets? 5? Not sure how to conceptualize that, and I would imagine it will affect how we organize model evaluation. Grr, I hate mixed models. 

HH: yes, I would assume that participants get ten plots each, one from each of the designs in a random order. (We have the data base set up that way).}


\comment{Modified from Table 10.6 (pg 181) of Design of Experiments by Dr. Morris. The table in the book has a four-factor split plot design with three levels (randomized, block, block). }

\begin{table}[h]
\centering\tiny
\begin{tabular}{lllll}\hline\hline\\
Level & Factor & Source & DF & Sum of Squares \\\hline
\multirow{11}{*}{Dataset}
      & $K$    & $\alpha$  & 1 & 
      $\sum_i (4)(5)(r)(10P)(\overline{y}_{i\cdot\cdot\cdot\cdot\cdot}-
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & $\sigma^2_T$ & $\beta$  & 3 & 
      $\sum_j (2)(5)(r)(10P)(\overline{y}_{\cdot j\cdot\cdot\cdot\cdot}-
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & $\sigma^2_C$ & $\gamma$  & 4 & 
      $\sum_i (2)(4)(r)(10P)(\overline{y}_{\cdot\cdot k\cdot\cdot\cdot}-
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      &    & $(\alpha\beta)$ & 3 & 
      $\sum_{ij} (5)(r)(10P)(\overline{y}_{ij\cdot\cdot\cdot\cdot}-
                               \overline{y}_{i\cdot\cdot\cdot\cdot\cdot} - 
                               \overline{y}_{\cdot j\cdot\cdot\cdot\cdot} + 
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      &    & $(\alpha\gamma)$ & 4 & 
      $\sum_{ik} (4)(r)(10P)(\overline{y}_{i\cdot k\cdot\cdot\cdot}-
                               \overline{y}_{i\cdot\cdot\cdot\cdot\cdot} - 
                               \overline{y}_{\cdot\cdot k\cdot\cdot\cdot} + 
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      &    & $(\beta\gamma)$ & 12 & 
      $\sum_{jk} (2)(r)(10P)(\overline{y}_{\cdot jk\cdot\cdot\cdot}-
                               \overline{y}_{\cdot j\cdot\cdot\cdot\cdot} - 
                               \overline{y}_{\cdot\cdot k\cdot\cdot\cdot} + 
                               \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      &    & \multirow{2}{*}{$(\alpha\beta\gamma)$} & \multirow{2}{*}{12} & 
      $\sum_{ijk} (r)(10P)(\overline{y}_{ijk\cdot\cdot\cdot}-
                             \overline{y}_{i\cdot\cdot\cdot\cdot\cdot} -
                             \overline{y}_{\cdot j\cdot\cdot\cdot\cdot} - 
                             \overline{y}_{\cdot\cdot k\cdot\cdot\cdot} $\\
      &&&&\hphantom{$\sum_{ijk} (r)(10P)($}$ + 
                             \overline{y}_{ij\cdot\cdot\cdot\cdot} +
                             \overline{y}_{\cdot jk\cdot\cdot\cdot} + 
                             \overline{y}_{i\cdot k\cdot\cdot\cdot}- 
                             \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & & \multirow{2}{*}{Resid.} & \multirow{2}{*}{$(2)(4)(5)(r-1)$} & 
      $\sum_{ijkl} (10P)(\overline{y}_{ijkl\cdot\cdot}-
                           \overline{y}_{i\cdot\cdot\cdot\cdot\cdot} -
                           \overline{y}_{\cdot j\cdot\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot k\cdot\cdot\cdot} + 
                           \overline{y}_{ij\cdot\cdot\cdot\cdot} +
                           \overline{y}_{\cdot jk\cdot\cdot\cdot} $\\
      & & & &
      \hphantom{$\sum_{ijkl} (10P)($}$ + 
                           \overline{y}_{i\cdot k\cdot\cdot\cdot} -
                           \overline{y}_{ijk\cdot\cdot\cdot} -
                           \overline{y}_{ij\cdot l\cdot\cdot} -
                           \overline{y}_{i\cdot kl\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot jkl\cdot\cdot\cdot} +                           
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Total & & $(2)(4)(5)(r)-1$ & 
      $\sum_{ijkl} (10P)(\overline{y}_{ijkl\cdot\cdot}-
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \hline\\\hline
\multirow{8}{*}{Plot}
      & Dataset & blocks & $(2)(4)(5)(r)-1$ & 
      $\sum_{ijkl} (10P)(\overline{y}_{ijkl\cdot\cdot}-
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Aes. & $\delta$ & 9 & 
      $\sum_{m} (2)(4)(5)(P)(\overline{y}_{\cdot\cdot\cdot\cdot m\cdot}-                        
                             \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Aes x $K$ & $(\alpha\delta)$ & 9 & 
      $\sum_{im} (4)(5)(P)(\overline{y}_{i \cdot\cdot\cdot m\cdot}- 
                           \overline{y}_{i \cdot\cdot\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & Aes x $\sigma_T$ & $(\beta\delta)$ & 27 & 
      $\sum_{jm} (2)(5)(P)(\overline{y}_{\cdot j\cdot\cdot m\cdot}- 
                           \overline{y}_{\cdot j\cdot\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & Aes x $\sigma_C$ & $(\gamma\delta)$ & 36 & 
      $\sum_{km} (2)(4)(P)(\overline{y}_{\cdot\cdot k\cdot m\cdot}- 
                           \overline{y}_{\cdot\cdot k\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Others & & 9(31) & \multirow{2}{*}{difference }\\
      & Resid & & 40(rP-1)-(40r-1)  & \\
      \cline{2-5}
      & Total & & $400r-1$ & 
      $\sum_{ijklm} (P)(\overline{y}_{ijklm\cdot}-
                        \overline{y}_{ijkl\cdot\cdot})^2$ \\
      \hline\\\hline
\multirow{4}{*}{Trial}
      & Picture & Sub-blocks & $400r-1$ & 
      $\sum_{ijklm} (P)(\overline{y}_{ijklm\cdot}-
                        \overline{y}_{ijkl\cdot\cdot})^2$ \\
      \cline{2-5}
      & Participants & $\tau$ & $P-1$ & 
      $\sum_{n} (2)(4)(5)(r)(10) (\overline{y}_{\cdot\cdot\cdot\cdot\cdot n} -
                            \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Resid & & $(400r-1)(P)$ & difference \\
      \cline{2-5}
      & Total & & $400(r)(P)-1$ & 
      $\sum_{ijklmn} (y_{ijklmn} - 
                      \overline{y}_{\cdot\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \hline\hline
\end{tabular}
\caption{Evaluation of sources of error in a full factorial version of the experiment, with $r$ replicates of each parameter combination and $P$ participant evaluations of each plot(data/aesthetic combination).}
\end{table}

We have a couple of options:
\begin{itemize}
\item keep the full factorial experiment, use one (at most two) replicates, and use higher level factorial effects to beef up any error variance terms. 
\item Do a full factorial experiment for $K=3$ and use a subset of the factorial experiment for $K=5$ (either using a subset of cases for $\sigma_T$ and $\sigma_C$, or a subset of combinations of the two cases/fractional factorial.)
\end{itemize}

\comment{
The fractional factorial option will be a pain to explain when we write things up; it will be simpler to explain using a subset of cases. Given that we don't particularly care about the third-order effects (and possibly not even the second-order effects) for the parameters, I'm inclined to say that the single-replicate option is the easiest way to go (and lets us keep the simple SSQ in the table, which is a huge bonus in my opinion). Even if we just use the third-order interaction effect as error, we still have 12 degrees of freedom; that should be plenty - we'd only need F=\Sexpr{round(qf(.95, 12, 12), 2)} to get a significant result for even the $(\sigma_T\sigma_C)$ test. 

HH: We might not care about interpreting the two-way interactions, but unfortunately they will be there (see comment at the back). So I would suggest to go with a full factorial design in $\sigma_C, \sigma_T$, and $K$, with three replications each (we need the replicates, also explained in the back). This gives us 18 parameter settings, and $18\cdot 3 = 54$ data sets. In case you still want to consider the effect of the number of datapoints $N$, we could switch from fully factorial to fractional factorial and replace the three-way interaction of $\sigma_C, \sigma_T$, and $K$ by the settings of $N$. That way we will keep the 18 settings. }

\begin{table}[h]
\caption{ANOVA table - only one replicate. Evaluation of sources of error in a full factorial version of the experiment, with one replicate of each parameter combination and $P$ participant evaluations of each plot(data/aesthetic combination).}
\centering\footnotesize
\begin{tabular}{lllll}\hline\hline\\
Level & Factor & Source & DF & Sum of Squares \\\hline
\multirow{5}{*}{Dataset}
      & $K$    & $\alpha$  & 1 & 
      $\sum_i (4)(5)(10P)(\overline{y}_{i\cdot\cdot\cdot\cdot}-
                          \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & $\sigma^2_T$ & $\beta$  & 3 & 
      $\sum_j (2)(5)(10P)(\overline{y}_{\cdot j\cdot\cdot\cdot}-
                          \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & $\sigma^2_C$ & $\gamma$  & 4 & 
      $\sum_i (2)(4)(10P)(\overline{y}_{\cdot\cdot k\cdot\cdot}-
                          \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & & Resid. & 22 & difference \\
      \cline{2-5}
      & Total & & 39 & 
      $\sum_{ijk} (10P)(\overline{y}_{ijk\cdot\cdot}-
                        \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \hline\\\hline
\multirow{7}{*}{Plot}
      & Dataset & blocks & 39 & 
      $\sum_{ijk} (10P)(\overline{y}_{ijk\cdot\cdot}-
                        \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Aes. & $\delta$ & 9 & 
      $\sum_{m} (2)(4)(5)(P)(\overline{y}_{\cdot\cdot\cdot m\cdot}-                        
                             \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Aes x $K$ & $(\alpha\delta)$ & 9 & 
      $\sum_{im} (4)(5)(P)(\overline{y}_{i \cdot\cdot m\cdot}- 
                           \overline{y}_{i \cdot\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & Aes x $\sigma_T$ & $(\beta\delta)$ & 27 & 
      $\sum_{jm} (2)(5)(P)(\overline{y}_{\cdot j\cdot m\cdot}- 
                           \overline{y}_{\cdot j\cdot\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      & Aes x $\sigma_C$ & $(\gamma\delta)$ & 36 & 
      $\sum_{km} (2)(4)(P)(\overline{y}_{\cdot\cdot k m\cdot}- 
                           \overline{y}_{\cdot\cdot k\cdot\cdot} - 
                           \overline{y}_{\cdot\cdot\cdot m\cdot} + 
                           \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Resid & & 9(31) & difference \\
      \cline{2-5}
      & Total & & 399 & 
      $\sum_{ijkm} (P)(\overline{y}_{ijkm\cdot}-
                       \overline{y}_{ijk\cdot\cdot})^2$ \\
      \hline\\\hline
\multirow{4}{*}{Trial}
      & Picture & Sub-blocks & 399 & 
      $\sum_{ijkm} (P)(\overline{y}_{ijkm\cdot}-
                       \overline{y}_{ijk\cdot\cdot})^2$ \\
      \cline{2-5}
      & Participants & $\tau$ & $P-1$ & 
      $\sum_{n} (2)(4)(5)(10) (\overline{y}_{\cdot\cdot\cdot\cdot n} -
                            \overline{y}_{\cdot\cdot\cdot\cdot\cdot})^2$ \\
      \cline{2-5}
      & Resid & & $399(P-1)$ & difference \\
      \cline{2-5}
      & Total & & $400P-1$ & 
      $\sum_{ijkmn} (y_{ijkmn} - 
                     \overline{y}_{\cdot\cdot\cdot\cdot})^2$ \\
      \hline\hline
\end{tabular}
\end{table}

\clearpage
\subsection{Sample Pictures}
The following plots use $\sigma_T=.4$ and $\sigma_C=.3$. \\
<<samplepics, eval=T, echo=F, fig.width=6, fig.height=6, out.width=".45\\linewidth", fig.show='hold', fig.keep='all', fig.align='center', warning=FALSE, message=FALSE>>=
source("../../Code/MixtureLineups.R")
source("../../Code/theme_lineup.R")

# # Define colors and shapes
# colors <-  c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", 
#              "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf")
# shapes <- c(1,0,3,4,8,5,2,6,-0x25C1, -0x25B7)
# 
# colortm <- read.csv("../../Data/color-perceptual-kernel.csv")
# # colortm[3,4] <- 0
# # colortm[4,3] <- 0
# colortm[8,] <- 0
# colortm[,8] <- 0
# 
# shapetm <- read.csv("../../Data/shape-perceptual-kernel.csv")
# # shapetm[9:10,] <- 0
# # shapetm[, 9:10] <- 0
# shapetm[9,] <- 0
# shapetm[,9] <- 0
# shapetm[10,] <- 0
# shapetm[,10] <- 0
# 
# # Lineup Design
# data.parms <- expand.grid(N=45,
#                           K=3,
#                           sd=.4,
#                           q=.3)
# 
# plot.parms <- expand.grid(
#   color = c(0,1),
#   shape = c(0,1),
#   reg = c(0,1),
#   err = c(0,1),
#   ell = c(0,1)
# )[c(
#   1, # control
#   2, 3, # color, shape
#   4, 18, # color + shape, color + ellipse
#   20, # color + shape + ellipse
#   5, 13, # trend, trend + error
#   6, # color + trend
#   30 # color + ellipse + trend + error
#   ),]
# 
# get.aes <- function(r){
#   c("Color", "Shape")[which(as.logical(r[1:2]))]
# }
# 
# get.stats <- function(r){
#   c("Reg. Line", "Error Bands", "Ellipses")[which(as.logical(r[3:5]))]
# }
# 
# data <- ldply(1:nrow(data.parms), function(i) {data.frame(set=i, gen.data(as.list(data.parms[i,])))})
# data.stats <- ddply(data, .(set, .sample), 
#                     function(df){
#                       r2 <- summary(lm(y~x, data=df))
#                       tmp <- summary(aov(lm(y~x+factor(group) + 0, data=df)))
#                       res <- tmp[[1]]$`Mean Sq`
#                       data.frame(.sample=unique(df$.sample), 
#                                  LineRSq = r2$r.squared, 
#                                  Fgroup = round(res[2]/res[3], 2), 
#                                  lineplot=unique(df$target2), 
#                                  groupplot=unique(df$target1))
#                       } )
# answers <- ddply(data.stats, .(set), summarize, lineplot=unique(lineplot), groupplot=unique(groupplot))
# 
# save(colors, shapes, colortm, shapetm, data, answers, data.parms, plot.parms, get.aes, get.stats, file="figure/lineupex/data.Rdata")
load("figure/lineupex/data.Rdata")
d_ply(data, .(set), function(df){
  i <- unique(df$set)
  for(j in 1:nrow(plot.parms)){
    p = gen.plot(df, get.aes(plot.parms[j,]), get.stats(plot.parms[j,]))
    ggsave(plot = p, 
           filename = sprintf("figure/lineupex/set-%d-plot-%d.pdf", i, j), 
           width=6, height=6, units="in", dpi=150)
    print(p)
  }
})
@
% \includegraphics[width=.49\linewidth]{figure/lineupex/set-1-plot-1.pdf}\hfill
% \includegraphics[width=.49\linewidth]{figure/lineupex/set-1-plot-2.pdf}\\
% \includegraphics[width=.49\linewidth]{figure/lineupex/set-1-plot-3.pdf}\hfill
% \includegraphics[width=.49\linewidth]{figure/lineupex/set-1-plot-4.pdf}\\
% \includegraphics[width=.49\linewidth]{figure/lineupex/set-1-plot-5.pdf}\hfill
% \includegraphics[width=.49\linewidth]{figure/lineupex/set-1-plot-6.pdf}\\
% \includegraphics[width=.49\linewidth]{figure/lineupex/set-1-plot-7.pdf}\hfill
% \includegraphics[width=.49\linewidth]{figure/lineupex/set-1-plot-8.pdf}\\
% \includegraphics[width=.49\linewidth]{figure/lineupex/set-1-plot-9.pdf}\hfill
% \includegraphics[width=.49\linewidth]{figure/lineupex/set-1-plot-10.pdf}\\


\bibliographystyle{asa}
\bibliography{references}

\section*{Simulation Studies of Parameter Space}
Using 1000 simulations for each of the 98 combinations of parameters ($K=\{3,5\}$, $\sigma_C=\{.1, .15, .2, .25, .3, .35, .4\}$, $\sigma_T=\{.2, .25, .3, .35, .4, .45, .5\}$), we explored the effect of parameter value on the distribution of summary statistics describing the line strength ($R^2$) and cluster strength (short description here) for null and target plots. The plot below shows the 25th and 75th percentiles of the distribution of these summary statistics for each set of parameter values. These plots guided our evaluation of ``easy", ``medium" and ``hard" parameter values for line and cluster tasks. 

\comment{What we also see from these plots, is that we do have a $\sigma_C \sigma_T$ interaction: the distinction between target and null on a fixed setting of clustering becomes increasingly difficult as the standard deviation for the linear trend is increased, and vice versa. We might also have a three-way interaction between $\sigma_C, \sigma_T$, and $K$: the size of the blue intervals (bottom figure) changes in size between different levels of $K$, it changes for different levels of $\sigma_C$ and $\sigma_T$. I am not sure whether that is an actual three-way interaction or just all three two-way interactions, but it doesn't matter, at this point we are just talking about potentially saving one parameter. What is clear, is that we need to block by parameter setting. We do so, by blocking on each dataset. Each dataset is non-deterministic, though, because we have a random process generating from different parameter settings, not a deterministic run setting as in an engineering setting. We therefore need repetitions of the data generation to be able to separate the variabiltiy coming from within the parameter setting from the additional variability introduced by the subjects' evaluations of the lineups. }
<<simulationparameters,echo=F,include=T, fig.width=10, fig.height=6.5, out.width='.8\\linewidth'>>=
load("../../Data/SimulationDatasetCriteria.Rdata")

dataset.criteria$ParameterSet <- with(dataset.criteria, sprintf("sdT%.2f-sdC%.2f", sd.trend, sd.cluster))
dataset.criteria$ParameterSet[dataset.criteria$type=="cluster"] <- with(dataset.criteria[dataset.criteria$type=="cluster",], sprintf("sdC%.2f-sdT%.2f", sd.cluster, sd.trend))

  
#qplot(data=dataset.criteria, x=LB, xend=UB, y=ParameterSet, yend=ParameterSet, color=dist, geom="segment") + geom_point(aes(x=LB, y=ParameterSet, color=dist))  + geom_point(aes(x=UB, y=ParameterSet, color=dist)) + facet_grid(type~K, scales="free", labeller="label_both")  + theme_bw() + 
#  scale_colour_brewer(palette="Set1") + theme(legend.position="bottom") + xlab("Interquartile intervals of Max (18) null distribution (blue) and target distribution (red) of quality measures.")

dataset.criteria$lsc <- paste("sigma[C]: ", round(dataset.criteria$sd.cluster, 2))
dataset.criteria$lst <- paste("sigma[T]: ", round(dataset.criteria$sd.trend, 2))
dataset.criteria$lK <- paste("K: ", dataset.criteria$K)
qplot(data=subset(dataset.criteria, type=="line"), x=LB, xend=UB, y=sd.cluster, yend=sd.cluster, color=dist, geom="segment") +
  geom_point(aes(x=LB, y=sd.cluster, color=dist))  + 
  geom_point(aes(x=UB, y=sd.cluster, color=dist)) + 
  facet_grid(lst~lK, scales="free", labeller="label_parsed")  + theme_bw() + 
  scale_colour_brewer("distribution",palette="Set1") + theme(legend.position="bottom") + xlab("Interquartile intervals of Max (18) null distribution (blue) and target distribution (red) of linearity measured in R squared.") + ylab(expression("Cluster variability":sigma[C]))

qplot(data=subset(dataset.criteria, type=="cluster"), x=LB, xend=UB, y=sd.trend, yend=sd.trend, color=dist, geom="segment") +
  geom_point(aes(x=LB, y=sd.trend, color=dist))  + 
  geom_point(aes(x=UB, y=sd.trend, color=dist)) + 
  facet_grid(lsc~lK, scales="free", labeller="label_parsed")  + theme_bw() + 
  scale_colour_brewer("distribution", palette="Set1") + theme(legend.position="bottom") + xlab("Interquartile intervals of Max (18) null distribution (blue) and target distribution (red) of amount of clustering.") + ylab(expression( "Variability along the trend":sigma[T]))


@
\end{document}