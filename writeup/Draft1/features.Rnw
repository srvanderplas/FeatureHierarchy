\documentclass[10pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{color}
\usepackage[dvipsnames,svgnames]{xcolor}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage[colorinlistoftodos]{todonotes}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\done}[2][inline]{\todo[color=SpringGreen, #1]{#2}}  % for todos that have been seen and dealt with
\newcommand{\meh}[2][inline]{\todo[color=White, #1]{#2}}   % for todos that may no longer be relevant 
\newcommand{\comment}[2][inline]{\todo[color=SkyBlue, #1]{#2}} % for comments that may not be "to-do"s
%\newcommand{\mcomment}[1]{\todo[color=SkyBlue]{#1}} % for margin comments
\newcommand{\newtext}[1]{\todo[inline, color=White]{ \color{OliveGreen}{#1}}} % new text - not necessarily something to be done
\newcommand{\move}[1]{\todo[inline, color=Lime]{#1}} % new to do item
%
%---------------------------------------------------



\title{Group beats Trend!? \\Testing feature hierarchy in statistical graphics}
\author{Susan VanderPlas, Heike Hofmann\thanks{Department of Statistics and Statistical Laboratory, Iowa State University}}
\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle
\begin{abstract}abstract goes here
\end{abstract}

\section{Introduction and background}
Intro to lineups \citep{Buja:2009hp, mahbub:2013, wickham:2010, Hofmann:2012ts}

The change to lineups we make is to introduce a second target to each lineup. We then keep track of how many observers choose any one of the two targets (to assess the difficulty of a lineup), and additionally we  record how often observers choose one target over the other one. This is information that we can use to evaluate how strong the signal of one target is compared to the other one. 

A further extension of this testing framework are the use of color (in a qualitative color scheme), the use of shapes, and additional density lines - we anticipate that all of these features are going to emphasize the clustering component. 
On the other hand, regression lines should emphasize any linear trends in the data.

\section{Design Choices}
Perceptual kernels \citep{heer:2014}
\section{Generating Model}
We are working with two  models $M_C$ and $M_T$ to generate data for the target plots. The null plots are showing data generate from a mixture model $M_0$. Both models generate data in the same range of values. We made also sure that data from the clustering model $M_C$ shares the same correlation 
with the null data, while data from model $M_T$ exhibits a similar amount of clustering as the null data. 

We compute the correlation coefficient for all of the plots to assess the amount of linearity in each panel. As a measure of clustering, we can use the $F$ statistic of between versus within group variation.

\subsection{Cluster Model $M_C$}
\begin{enumerate}
  \item Generate cluster centers along a line, then generate points around the cluster center. \\
  Algorithm: \\
  Parameters $N$ points, $K$ clusters, $q$ cluster cohesion
  \begin{enumerate}
    \item Generate cluster centers $(c^x_{i}, c^y_{i}), i=1, ..., K$:
      \begin{enumerate}
        \item Generate vectors $c^{x}$ and $c^y$ as permutations of $\{1, ..., K\}$,
        \item such that the correlation between cluster centers \text{Cor}$(c^{x}, c^{y})$ falls into a range of [.25, .9].
        \comment{We might have to go up with the correlation a bit. I'm still worried that people will pick the cluster plot from the trend line lineup because of the lowest slope. }
      \end{enumerate}
      \item Center and standard-normalize cluster centers $(c^x, c^y)$:  
      \[
        \tilde{c}^x_{i} = \frac{c^x_{i} - \bar{c}}{s_c} \ \ \text{ and } \ \ \tilde{c}^y_{i} = \frac{c^y_{i} - \bar{c}}{s_c},
      \]
      where $\overline{c} = K(K+1)/2$ and $s_c^2 = \frac{K(K+1)(2K+1)}{6} - \frac{K^2(K+1)^2}{4}$ for all $i = 1, ..., K$.
%       \begin{enumerate}
%         \item $c_{xi} = (c_{xi} - \overline{c}_x)/sd_{c_{x}}$
%         \item $c_{yi} = (c_{yi} - \overline{c}_y)/sd_{c_{y}}$
%       \end{enumerate}
    \item Determine group size $g_i$ for groups $i = 1, ..., K$ as a random draw $g_i \sim \text{Multinomial}(K, p)$ where $p = p_1/\sum_{i=1}^K p_{1i}$ for  $p_{1i} \sim N(\frac{1}{K}, \frac{1}{2 K^2})$. 
    \item Generate points around cluster centers: 
      \begin{enumerate}
        \item $x^\ast_i = c^x_{g_i} + e$, $e_i \sim N(0, q)$
        \item $y^\ast_i = c^y_{g_i} + e$, $e_i \sim N(0, q)$
      \end{enumerate}
      It may be reasonable to draw $q$ from a distribution of some sort.
    \comment{Let's not worry about getting $q$ from a random distribution, but let's rename it somehow, so it reflects the within cluster deviation a bit better ... $\sigma_C$?}
  \end{enumerate}
% \begin{enumerate}
%   \item Generate cluster centers along a line, then generate points around the cluster center. \\
%   Algorithm: \\
%   Parameters $N$ points, $K$ clusters, $q$ cluster cohesion, $s$ cluster std. dev.
%   \begin{enumerate}
%     \item Generate cluster centers $(c^x_i, c^y_i), i=1, ..., K$:
%       \begin{enumerate}
%         \item $c^x_i = (i-1)+e_x$, $e_x \sim Unif(-0.2, 0.2)$
%         \item $c^z \sim Unif(-q*K, q*K)$\\
%         $c^y_i = (c^z_i-\overline{c^z})/\sigma_{c^z} * q * K$
%       \end{enumerate}
%     \item Determine groups: $g \sim Multinomial(K, p)$ where $p = p_1/\sum_1^K p_{1i}$ where  $p_{1i} \sim N(\frac{1}{K}, \frac{1}{2 K^2})$ 
%     \item Generate points around cluster centers: 
%       \begin{enumerate}
%         \item $x^\ast_i = c^x_{g_i} + e$, $e_i \sim N(0, s)$
%         \item $y^\ast_i = c^y_{g_i} + e$, $e_i \sim N(0, s)$
%       \end{enumerate}
%     \item Scale points
%       \begin{enumerate}
%         \item $x_i = (x^\ast_i - \overline{x^\ast})/sd_{x^\ast}$
%         \item $y_i = (y^\ast_i - \overline{y^\ast})/sd_{y^\ast}$
%       \end{enumerate}
%   \end{enumerate}
%   Advantages:
%     \begin{enumerate}
%       \item Easy to manipulate underlying trend
%       \item Cluster variance/cohesion can be easily manipulated
%     \end{enumerate}
%   Disadvantages:
%     \begin{enumerate}
%       \item Works poorly for more than 3 groups
%       \item Difficult to easily specify cluster distribution along regression line while guaranteeing same underlying regression line
%       \item Cluster variance (away from line) doesn't show up well - not enough clusters to ensure similar overall variance compared to null plots
%     \end{enumerate}
%   \item Generate points in $K$ dimensions using random noise, then use LDA to get $K$ clusters\\
%     Advantages:
%     \begin{enumerate}
%       \item Clusters are separated in space
%       \item Variance is fairly easy to manipulate
%     \end{enumerate}
%   Disadvantages:
%     \begin{enumerate}
%       \item Difficult to translate to linear regression because of cluster distribution
%       \item Clusters are fairly easily identifiable
%     \end{enumerate}
\end{enumerate}
\subsection{Regression Model $M_T$}

This model has the parameter $\sigma_T$ to reflect the amount of scatter around the trend line. 

\subsection{Null Model $M_0$}
The generative model for null data  is created as a mixture model $M_0$ that draws $n_c \sim B_{N, \lambda}$ observations from the cluster model, and $n_T = N - n_c$ from the regression model $M_T$.


\section{Experimental Setup}
I would consider the values $\sigma_C = 0.3, .35, .4, .45$ for $K = 3$ clusters to be interesting. 
The actual values of $\sigma_C$ don't make much sense - because they are only valid within the scaled data values. We might need to re-express the values of $\sigma_C$ in terms of a percentage of the data  or a percentage of the overall variability.

For $K=5$ the parameters for $q$ (now $\sigma_C$) and the standard deviation $\sigma_T$ need to be smaller - we could start at 0.2 and 0.75, respectively.

\subsection*{Design choices}
\begin{enumerate}
\item Plain: two targets with data from one of each of the two generative models are included in a set of eighteen panels of null data.
\item Colour/Shape: points in each of the panels are coloured/marked based on the results of a hierarchical clustering . 
\item Trend line: a line of the least square fit is drawn through the points.
\item Colour \& Shape
\item Colour \& trend line: this emphasises both the clustering and the regression - it is not clear, which signal will be stronger.
\item Colour \& Ellipsoids: around the groups of the same color, ellipsoids are drawn to reflect the 95\% density estimate.
\end{enumerate}

\bibliographystyle{asa}
\bibliography{references}

\end{document}