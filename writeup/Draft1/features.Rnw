\documentclass[10pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{color}
\usepackage{multirow}
\usepackage[dvipsnames,svgnames]{xcolor}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage[colorinlistoftodos]{todonotes}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\done}[2][inline]{\todo[color=SpringGreen, #1]{#2}}  % for todos that have been seen and dealt with
\newcommand{\meh}[2][inline]{\todo[color=White, #1]{#2}}   % for todos that may no longer be relevant 
\newcommand{\comment}[2][inline]{\todo[color=SkyBlue, #1]{#2}} % for comments that may not be "to-do"s
%\newcommand{\mcomment}[1]{\todo[color=SkyBlue]{#1}} % for margin comments
\newcommand{\newtext}[1]{\todo[inline, color=White]{ \color{OliveGreen}{#1}}} % new text - not necessarily something to be done
\newcommand{\move}[1]{\todo[inline, color=Lime]{#1}} % new to do item
%
%---------------------------------------------------



\title{Group beats Trend!? \\Testing feature hierarchy in statistical graphics}
\author{Susan VanderPlas, Heike Hofmann\thanks{Department of Statistics and Statistical Laboratory, Iowa State University}}
\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle
\begin{abstract}abstract goes here
\end{abstract}

\section{Introduction and background}
Intro to lineups \citep{Buja:2009hp, mahbub:2013, wickham:2010, Hofmann:2012ts}

The change to lineups we make is to introduce a second target to each lineup. We then keep track of how many observers choose any one of the two targets (to assess the difficulty of a lineup), and additionally we  record how often observers choose one target over the other one. This is information that we can use to evaluate how strong the signal of one target is compared to the other one. 

A further extension of this testing framework are the use of color (in a qualitative color scheme), the use of shapes, and additional density lines - we anticipate that all of these features are going to emphasize the clustering component. 
On the other hand, regression lines should emphasize any linear trends in the data.

\section{Design Choices}
Perceptual kernels \citep{heer:2014}
\section{Generating Model}
We are working with two  models $M_C$ and $M_T$ to generate data for the target plots. The null plots are showing data generate from a mixture model $M_0$. Both models generate data in the same range of values. We made also sure that data from the clustering model $M_C$ shares the same correlation 
with the null data, while data from model $M_T$ exhibits a similar amount of clustering as the null data. 

We compute the correlation coefficient for all of the plots to assess the amount of linearity in each panel. As a measure of clustering, we can use the $F$ statistic of between versus within group variation.

\subsection{Cluster Model $M_C$}
We begin by generating cluster centers along a line, then we generate points around the cluster center. \\
  Algorithm: \\
  Parameters $N$ points, $K$ clusters, $\sigma_C$ cluster standard deviation
  \begin{enumerate}
    \item Generate cluster centers $(c^x_{i}, c^y_{i})$ for each of the $K$ clusters, $i=1, ..., K$:
      \begin{enumerate}
        \item Generate vectors $c^{x}$ and $c^y$ as permutations of $\{1, ..., K\}$,
        \item such that the correlation between cluster centers \text{Cor}$(c^{x}, c^{y})$ falls into a range of [.25, .9].
        \comment{We might have to go up with the correlation a bit. I'm still worried that people will pick the cluster plot from the trend line lineup because of the lowest slope. }
      \end{enumerate}
      \item Center and standard-normalize cluster centers $(c^x, c^y)$:  
      \[
        \tilde{c}^x_{i} = \frac{c^x_{i} - \bar{c}}{s_c} \ \ \text{ and } \ \ \tilde{c}^y_{i} = \frac{c^y_{i} - \bar{c}}{s_c},
      \]
      where $\overline{c} = K(K+1)/2$ and $s_c^2 = \frac{K(K+1)(2K+1)}{6} - \frac{K^2(K+1)^2}{4}$ for all $i = 1, ..., K$.
    \item Determine group size $g_i$ for clusters $i = 1, ..., K$ as a random draw $g_i \sim \text{Multinomial}(K, p)$ where $p = p_1/\sum_{i=1}^K p_{1i}$ for  $p_{1i} \sim N(\frac{1}{K}, \frac{1}{2 K^2})$. 
    \item Generate points around cluster centers: 
      \begin{enumerate}
        \item $x^\ast_i = c^x_{g_i} + e$, $e_i \sim N(0, \sigma^2_C)$
        \item $y^\ast_i = c^y_{g_i} + e$, $e_i \sim N(0, \sigma^2_C)$
      \end{enumerate}
  \end{enumerate}
  
  
\subsection{Regression Model $M_T$}

This model has the parameter $\sigma_T$ to reflect the amount of scatter around the trend line. 

  Algorithm: \\
  Parameters $N$ points, $\sigma_T$ standard deviation around the line, slope $a$ (1 by default) 
  \begin{enumerate}
    \item Generate $x_i$, $i=1, ..., N$, a sequence of evenly spaced points from [-1, 1]
    \item Jitter $x_i$: $x_i = x_i + \eta_i$, $\eta_i \sim \text{Unif}(-z, z)$, $z = 1/5*2/(N-1)$
    \item Generate $y_i$: $y_i = a * x_i + e_i$, $e_i \sim N(0, \sigma^2_T)$
  \end{enumerate}

\comment{Would the pictures change dramatically, if you used $x \sim U[-1,1]$ to start out with? that would be easier to explain.}
\subsection{Null Model $M_0$}
The generative model for null data  is created as a mixture model $M_0$ that draws $n_c \sim B_{N, \lambda}$ observations from the cluster model, and $n_T = N - n_c$ from the regression model $M_T$.

Under the null model, $M_T$ slope may be between $(.2, .8)$


\section{Experimental Setup}
\subsection{Design}
Factors:
\begin{table}[h]
\begin{center}
\begin{tabular}{lll}
Parameter & Description & Choices\\\hline
$N$ & \# Points & 8, 12, 16 per cluster\\
$K$ & \# Clusters & 3, 4, 5\\
$\sigma_T$ & Scatter around trend line & \\
$\sigma_C$ & Scatter around cluster centers & \\\hline
\end{tabular}
\end{center}
\caption{Data Generation Options}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{lll}
Emphasis & Aesthetics \\\hline
Control & -- \\
\multirow{2}{*}{Group} & Color, Shape, Ellipse \\
& Color + Shape, Color + Ellipse\\
\multirow{2}{*}{Trend} & Line, Error band \\
& Line + Error band\\
Conflict & Color + Trend Line, Color + Trend Line + Error band\\\hline
\end{tabular}
\end{center}
\caption{Plot Generation Options}
\end{table}


I would consider the values $\sigma_C = 0.3, .35, .4, .45$ for $K = 3$ clusters to be interesting. 
The actual values of $\sigma_C$ don't make much sense - because they are only valid within the scaled data values. We might need to re-express the values of $\sigma_C$ in terms of a percentage of the data  or a percentage of the overall variability.

For $K=5$ the parameters for $q$ (now $\sigma_C$) and the standard deviation $\sigma_T$ need to be smaller - we could start at 0.2 and 0.75, respectively.

\subsection*{Design choices}
\begin{enumerate}
\item Plain: two targets with data from one of each of the two generative models are included in a set of eighteen panels of null data.
\item Colour/Shape: points in each of the panels are coloured/marked based on the results of a hierarchical clustering . 
\item Trend line: a line of the least square fit is drawn through the points.
\item Colour \& Shape
\item Colour \& trend line: this emphasises both the clustering and the regression - it is not clear, which signal will be stronger.
\item Colour \& Ellipsoids: around the groups of the same color, ellipsoids are drawn to reflect the 95\% density estimate.
\end{enumerate}

\bibliographystyle{asa}
\bibliography{references}

\end{document}